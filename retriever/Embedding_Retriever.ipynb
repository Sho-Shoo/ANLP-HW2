{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvdxTeX0cIBe"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AUqyYz-WWcR-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710540874684,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    }
   },
   "outputs": [],
   "source": [
    "# %pip install llama-index-vector-stores-chroma -q\n",
    "# !pip install llama-index -q\n",
    "# %pip install llama-index-embeddings-huggingface -q\n",
    "# %pip install llama-index-llms-huggingface -q\n",
    "# %pip install llama-index-readers-file -q\n",
    "# !pip install \"transformers[torch]\" \"huggingface_hub[inference]\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1630,
     "status": "ok",
     "timestamp": 1710540876312,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     },
     "user_tz": 240
    },
    "id": "7YUaD5L-WhG4",
    "outputId": "4014f9d5-f761-4399-ffb9-3d4e307be515"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/ANLP/ANLP-HW2\n",
      "aggregated_documents.csv\t chroma_meta\t\t   faculty_info\t\tparser\n",
      "ANLP_hw2_pretrained_model.ipynb  data\t\t\t   faculty_info.csv\trag_embed.ipynb\n",
      "categorized_doc.xlsx\t\t eval_course_dataset.pkl   knowledge_source_pd\tREADME.md\n",
      "chroma_database\t\t\t eval_faculty_dataset.pkl  language_model\trequirements.txt\n",
      "chroma_db\t\t\t eval_paper_dataset.pkl    LICENSE\t\tretriever\n",
      "chroma.log\t\t\t evaluation_metric\t   openai_key.txt\tsetup.sh\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/ANLP/ANLP-HW2/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdF3O2NEHovH"
   },
   "source": [
    "# Huggingface Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "4253c3ac876646969800ae4e1ec976fa",
      "e3532100425d461fb75c3fbedc339036",
      "2956f89e27ff4ba6b5d18c9a3febb560",
      "102ce3c501dc4cec910009de2c16243d",
      "9f2a82adc23f45e5955abada54fa7cfe",
      "96fe94c5eb224d10966840a12ade0e71",
      "cc145f9181214a249d3e16382156ee0f",
      "bd731680eca541b3a49bd70a140ae1d6",
      "d1d6491dfc49465099601dadc41f3d7d",
      "e4165bda3ac64f898a9f6974e43d0543",
      "fde4e55f78f84f9aa0e6b114e5d57328",
      "297ef908769c41a1929cc53ac7afc1da",
      "e9b78e4b781f480c9422dbd980b3963c",
      "b5a6d213f6ca42848a1cf6120084d7e0",
      "7541fb3f914a4437861a3e50f9e3af5b",
      "63f3f9004f0c4c569872df4f831efa38",
      "2f766405722045e49baaa6661709f6ba",
      "0bb24fbf7a324a769d518804dc2be14c",
      "91a52f22895b48b5a88cc3929dd1a0dc",
      "1a0c6b30d6404d95bb8638f1987ddd1d",
      "11caf8eded9349388f1a69769bd528ce",
      "f8f1bb2c719f475d8eef088526a66f44",
      "36724d06fd07404b96f13ed432ab3464",
      "39c2ee7d7d7d414c9146f61b861418c5",
      "c91d4f24c0b24d6d8ca5130b37644a0c",
      "4ee462bd55634cec9f59a3bb5d386c40",
      "b39d440b40be4a70986cb530be78cb27",
      "d729b69953414075a75d22f83fe5b5f1",
      "3e5dde11fe9a4900b86df0823a390452",
      "7253b89f5c1840b0b0ef781072e8e45e",
      "c13f228adaad44c08c0a5e6ab9dd31cf",
      "11ff42a12fa04f52a46d768385cc81d3"
     ]
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1710374721001,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     },
     "user_tz": 240
    },
    "id": "jdaxbvKgHryN",
    "outputId": "5d915667-eca5-4cda-97a5-d8389dcaec39"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4253c3ac876646969800ae4e1ec976fa"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()#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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLFMwElVekMo"
   },
   "source": [
    "# Local database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40096,
     "status": "ok",
     "timestamp": 1710383629481,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     },
     "user_tz": 240
    },
    "id": "_Qz11wnQcrvE",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "outputId": "8a970990-242d-458c-f7b5-9a9aa975159e"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               title  \\\n",
       "0  https:||lti.cs.cmu.edu|sites|default|files|MCD...   \n",
       "1  https:||www.cmu.edu|commencement|schedule|inde...   \n",
       "2  https:||enr-apps.as.cmu.edu|assets|SOC|sched_l...   \n",
       "\n",
       "                                                text  \n",
       "0  1 Welcome Welcome to the Language Technologies...  \n",
       "1  Carnegie Mellon University Search Search this ...  \n",
       "2  schedule title: Summer Two 2024 Schedule | cou...  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-3ee85a93-993c-4b7e-a0f9-05a0defe1984\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https:||lti.cs.cmu.edu|sites|default|files|MCD...</td>\n",
       "      <td>1 Welcome Welcome to the Language Technologies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https:||www.cmu.edu|commencement|schedule|inde...</td>\n",
       "      <td>Carnegie Mellon University Search Search this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https:||enr-apps.as.cmu.edu|assets|SOC|sched_l...</td>\n",
       "      <td>schedule title: Summer Two 2024 Schedule | cou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ee85a93-993c-4b7e-a0f9-05a0defe1984')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-3ee85a93-993c-4b7e-a0f9-05a0defe1984 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-3ee85a93-993c-4b7e-a0f9-05a0defe1984');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-dbaae0ac-6f36-49f9-9de2-41bc0d94cc37\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dbaae0ac-6f36-49f9-9de2-41bc0d94cc37')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-dbaae0ac-6f36-49f9-9de2-41bc0d94cc37 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 61,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 61,\n        \"samples\": [\n          \"https:||lti.cs.cmu.edu|sites|default|files|MCDS%20Handbook%2023-24%20AY.txt\",\n          \"https:||api.semanticscholar.org|graph|v1|paper|search.txt\",\n          \"https:||lti.cs.cmu.edu|people|faculty|xing-eric.html.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 61,\n        \"samples\": [\n          \"1 Welcome Welcome to the Language Technologies Institute, Master of Computational Data Science Program. While this handbook is specific to your academic experience in the department, there are several other resources and offices graduate students are encouraged to consult during their tenure at Carnegie Mellon University. Information about The Word, the student handbook, the Office of Graduate and Postdoctoral Affairs, the Office of the Dean of Students, and others are included in Appendix A of this handbook. 1.1 The MCDS Degree The MCDS Degree The Master of Computational Data Science (MCDS) degree is a professional Master of Science degree offered by the Language Technologies Institute (LTI), a department in the School of Computer Science at Carnegie Mellon University. The MCDS degree offers students with a Bachelor's degree the opportunity to improve their training with advanced study in Computer Science and Machine Learning. We cater to students with basic analytic skills and a strong aptitude for mathematics, programming, and logical reasoning. An undergraduate degree in Computer Science is not required. Most students will complete the program in three semesters; students coming from other disciplines and students focus on developing applied research skills in preparation for further graduate study or research-oriented employment may require an additional fourth semester. The MCDS Program offers a core curriculum and several concentrations; students entering the program enroll in core courses in their first semester and select further courses to satisfy at least one concentration (see Section 3.3.6). Students construct their own course of study, in consultation with their academic advisor, in order to satisfy broad guidelines. Thus, a student may tailor their coursework in a given concentration to follow a particular area of emphasis. The MCDS program is typically a 16-month program consisting of courses, seminars, a required Capstone Project and a required summer internship or practical training. While some MCDS graduates continue on to PhD programs in the LTI or other leading universities, most graduates go on to jobs in corporate research and development laboratories. The program consists entirely of coursework and a Capstone Project, and no Master\\u2019s Thesis is required. All Capstone projects are structured as research activities and may lead to a publication. There is no Doctoral program in Computational Data Science. Because of the highly selective nature of the MCDS program and quality of the MCDS curriculum, performing well in the program will give a boost to a PhD application. MS graduates are welcome to apply to CMU PhD programs but will not receive preferential treatment. There are significant differences between CMU's different departments and degree programs in philosophical approach, procedures, policies and regulations. Each department issues a handbook that informs graduate students of their program requirements and procedures and ensures that students have written access to the standard information outlined below. This handbook describes the policies, procedures, and requirements for the Master of Computational Data Science (MCDS) degree. All policies not explicitly described in this document conform to School of Computer Science (SCS) policies and university policies described in The Word, Carnegie Mellon University Student Handbook and at the University Policies website. 1.2 Vision Carnegie Mellon University will have a transformative impact on society through continual innovation in education, research, creativity, and entrepreneurship. 1.3 Mission To create a transformative educational experience for students focused on deep disciplinary knowledge; problem solving; leadership, communication, and interpersonal skills; and personal health and well- being. To cultivate a transformative university community committed to (a) attracting and retaining diverse, world-class talent; (b) creating a collaborative environment open to the free exchange of ideas, where<sep    -4525 Phone: (412) 268-9870 Fax: (412) 268-7287 Fax: (412) 268-7287 Robert Frederking Mona Diab, LTI Director Graduate Program Chair Language Technologies Institute Language Technologies Institute School of Computer Science School of Computer Science Carnegie Mellon University Carnegie Mellon University Gates-Hillman Center 5723 Gates-Hillman Center 6515 5000 Forbes Avenue, Pgh, PA 15213 5000 Forbes Avenue, Pgh, PA 15213 Phone: (412) 268-3669 Phone: (412) 268-6656 The Language Technologies Institute is located primarily on the 5 th and 6 th floors of the Gates Hillman Complex (GHC) on Carnegie Mellon\\u2019s Pittsburgh campus: Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Gates Hillman Complex 5402, LTI Pittsburgh, PA 15241-3891 412-268-6591 (phone) 412-268-6298 (fax) http://www.lti.cs.cmu.edu/ 1.5 University Policies and Expectations Each member of the Carnegie Mellon community must be familiar with university policies and guidelines. In addition to this departmental graduate student handbook, the following resources are available to assist you in understanding community expectations: The Word/Student http://www.cmu.edu/student- Handbook: affairs/theword/index.html Academic Integrity https://www.cmu.edu/policies/student-and-student- Website: life/academic-integrity.html University Policies http://www.cmu.edu/policies/ Website: Office of Graduate and http://www.cmu.edu/graduate/policies/index.html Post-Doc Affairs: Please see Appendix A for additional information about university resources. 1.6 Carnegie Mellon University Statement of Assurance Carnegie Mellon University does not discriminate in admission, employment or administration of its programs or activities on the basis of race, color, national origin, sex, handicap or disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status or genetic information. Furthermore, Carnegie Mellon University does not discriminate and is required not to discriminate in violation of federal, state or local laws or executive orders. Inquiries concerning the application of and compliance with this statement should be directed to the university ombudsman, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, telephone 412-268-1018. Obtain general information about Carnegie Mellon University by calling 412-268-2000. Carnegie Mellon University publishes an annual campus security and fire safety report. describing the university's security, alcohol and drug, sexual assault and fire safety policies, and containing statistics about the number and type of crimes committed on the campus, and the number and cause of fires in campus residence facilities during the preceding three years. You can obtain a copy by contacting the Carnegie Mellon Police Department at 412-268-2323. The annual security and fire safety report also is available online at www.cmu.edu/police/annualreports. Information regarding the application of Title IX, including to admission and employment decisions, the sexual misconduct grievance procedures and process, including how to file a report or a complaint of sex discrimination, how to file a report of sexual harassment, and how the university responds to such reports is available at www.cmu.edu/title-ix. The Title IX coordinator may be reached at 412-268-7125 or tix@cmu.edu. 1.7 The Carnegie Mellon Code Students at Carnegie Mellon, because they are members of an academic community dedicated to the achievement of excellence, are expected to meet the highest standards of personal, ethical and moral conduct possible. These standards require personal integrity, a commitment to honesty without compromise, as well as truth without equivocation and a willingness to place the good of the community above the good of the self. Obligations once undertaken must be met, commitments kept. As members of the Carnegie Mellon community, individuals are expected to uphold the standards of the community in addition to holding others accountable for said standards. It is rare that the life of a student in an academic community can be so private that it will not affect the community as a whole or that the above standards do not apply. The discovery, advancement and communication of knowledge are not possible without a commitment to these standards. Creativity cannot exist without acknowledgment of the creativity of others. New knowledge cannot be developed without credit for prior knowledge. Without the ability to trust that these principles will be observed, an academic community cannot exist. The commitment of its faculty, staff and students to these standards contributes to the high respect in which the Carnegie Mellon degree is held. Students must not destroy that respect by their failure to meet these standards. Students who cannot meet them should voluntarily withdraw from the university. The Carnegie Mellon Code can also be found on-line at: https://www.cmu.edu/student-affairs/theword/. 2 The Language Technologies Institute 2.1 Main Office The Gates Hillman Complex: Mailboxes, printers, copiers, and other departmental resources are in GHC 5404. 2.2 Photocopies and Printers The use of a printer/copier requires a CS user id (see the \\u2018Computers\\u2019 section). The School of Computer Science provides several black-and-white and color printers for use by students. SCS Computing Facilities maintains a list of printers: http://www.cs.cmu.edu/~help/printing/. 2.3 Office Space for MS Students Full-time students in the LTI\\u2019s MS degree programs on the Pittsburgh campus have access to a shared working space to create a sense of community and provide space for working when on campus. 2.4 Computers for MS Students Students are expected to provide their own laptop computers that can be used to access university resources and complete course assignments. Laptops running Windows, MacOS, and Linux software are all acceptable. MS students will be given a CS user id. A CS user id is required to use the LTI computer cluster, department printers, and other SCS services. The School of Computer Science has a Help Center located at 4203 GHC. They can be contacted at help@cs.cmu.edu, extension 8-4231 from a campus phone, or 412-268-4231 from an outside line. MS students will be given access to the LTI\\u2019s computer cluster on an as-needed basis, to be used for course assignments, directed study projects, and/or the capstone project. The LTI cluster provides storage and computation for projects involving large datasets and/or lengthy computation. 3 MCDS Degree Completion and Certification This section describes the various rules and regulations that determine the attainment of a MCDS degree by the student. 3.1 CMU Degree Completion and Statute of Limitations Carnegie Mellon graduate students are expected to complete their degree requirements within the standard length of time for their program of study as outlined in the relevant Graduate Student Handbook. Standard program lengths for graduate students vary significantly \\u2013 ranging from two semesters for some full-time master\\u2019s programs to several or more years for doctoral programs. Upon completion of the graduate program degree requirements, the degree will be certified by the student\\u2019s academic program in the semester in which the student completes the requirements. Early Completion Graduate students who consider the completion of all degree requirements in less than the standard length of time for their program of study may consult with their degree- granting program or department to determine if early degree certification is allowed and under what circumstances. Extended or Longer-than-Standard Completion Longer-than-standard degree completion may occur due to academic interruptions in making progress toward the degree as defined by the academic program, interruptions of full-time study or progress towards the degree due to serious, documented medical issues, or other unusual or unforeseen circumstances. Master\\u2019s students who require longer than the standard time to complete their degree requirements are expected to remain in close contact with their graduate program and will be certified at the end of the semester in which they have completed their degree requirements. Policy on Master\\u2019s Student Statute of Limitations www.cmu.edu/policies/student-and-student-life/masters-students-statute-of- limitations.html See the above link regarding guidelines and restrictions which place an upper limit on the maximum length of time allowable for master\\u2019s degree completion and certification. Additional Guidance for Students Program of study. Students seeking guidance about their program of study and degree requirements should consult with their academic advisor and/or appropriate associate dean. Financial aid and student account Students are expected to make normal progress toward their degree to graduate within the standard timeframe for their program of study. Under U.S. Federal Title IV regulations, student eligibility for federal financial aid is contingent upon enrollment in and successful completion of courses that are counted as credit toward their current degree program. To receive the maximum amount of federal financial aid for which they may be eligible, students must enroll each semester in at least 36 units that count toward their current degree level. (See separate guidance regarding integrated degree completion.) Students should consult with their designated college liaison in The HUB regarding billing and financial aid, particularly for early completion, longer-than- standard completion, or integrated undergraduate and master\\u2019s degree programs. International students Immigration status for students in F-1 and J-1 non-immigrant status is tied to making normal progress toward completing degree requirements. Therefore, F-1 and J-1 students who are considering completing their degree requirements early, anticipating longer-than-standard completion, or moving from an undergraduate to a graduate student classification (integrated undergraduate-graduate study) should consult with their designated advisor in the Office of International Education (OIE) to ensure compliance with immigration regulations. 3.2 Full-time Status All MCDS students are expected to enroll full-time (at present, there is no option to pursue the degree as a part-time student). To be considered a full-time student, a student must be registered for, and complete, a minimum of 36 units in every Fall and Spring semester. All international students are required by US Federal law to maintain full-time status. Students can have no more than one (1) remote course counting toward the 36 units used to satisfy full-time enrollment. Failure to maintain full-time status will result in loss of a student visa (and, therefore, \\u201cpermit of stay\\u201d). All students having a Stafford Loan are required to maintain full-time status. 3.3 MCDS Degree Enrollment Process and Related Information 3.3.1 Duration of the degree program The MCDS degree must be completed within five (5) years from the time that the student matriculates into the program. https://www.cmu.edu/policies/student-and-student-life/masters-students-statute-of- limitations.html As outlined in the Master\\u2019s Students Statute of Limitations (see link above), students will complete all requirements for the master\\u2019s degree within a maximum of seven years from original matriculation as a master\\u2019s student, or less if required by a more restrictive department, school or college policy. Once this time-to-degree limit has lapsed, the person may resume work towards a master\\u2019s degree only if newly admitted to a currently offered master\\u2019s degree program under criteria determined by that program. Under extraordinary circumstances, such as leave of absence, military or public service, family or parental leave, or temporary disability, a school or college may, upon the relevant department's recommendation and with the written approval of the dean (or designate), defer the lapse for a period commensurate with the duration of that interruption. Students who are pursuing a master\\u2019s degree as part-time students for all semesters of their program, as approved by their program, may also appeal to their program or department for extension of the time to degree limit. 3.3.2 Residency requirements There is no formal residence requirement. However, most courses in the program are taught on campus without an option for distance education. Students enrolled in in- person course sections (IPR) are expected to attend all class meetings in person. 3.3.3 Degree Certification: Course requirements and related policies/protocols \\uf0b7 In order to receive the MCDS degree, students must have a Quality Point Average (QPA) of 3.0. Completion of the degree is based on completing courses in the core curriculum, the MCDS seminar, electives and capstone project. \\uf0b7 The capstone project consists of students working at CMU on a research project, or on an industry-sponsored project. \\uf0b7 The student must complete 144 eligible units of study. This includes eight core and concentration courses, two 12-unit seminar courses and two 12-unit capstone courses. All students complete a common MCDS core in their first two semesters, consisting of five courses. All students must also complete at least one concentration, consisting of three courses in one of three areas: Analytics, Systems, or Human-Centered Data Science. The remaining elective course can be any course with number 600 or above chosen from the SCS course catalog. Any additional non-prerequisite units taken beyond the 144 units are also considered electives. \\uf0b7 To maintain full-time enrollment status, a student must enroll in a minimum of 36 course units per semester. A student may not take more than 60 units per semester, without permission from their academic advisor. Students must maintain full-time enrollment status (minimum of 36 units) in their final semester. 3.3.4 Prerequisite Core Course All MCDS students are expected to pass the 11-637 Foundations of Computational Data Science course by the end of their first semester. Each student must pass 11-637 with a grade of \\u201cB\\u201d or better. 3.3.5 Plan of study The degree consists of two timing options based on the length of time the student spends working on the degree. The student chooses their timing at the start of the degree program (for visa requirements). Changes in timing are possible with the approval of the Director of the degree program and successful visa extension application with CMU\\u2019s Office of International Education. Note that all degree options consist of the same amount of coursework: \\uf0b7 Professional Preparation Track \\u2013 a 16-month degree consisting of study for Fall and Spring semesters, a summer internship, and Fall semester of study. Each semester consists of a minimum of 48 units of study. This timing is typical for most students. The student graduates in December. \\uf0b7 Research Preparation Track \\u2013 a 20-month degree consisting of study for Fall and Spring semesters, a summer internship, and a second year of Fall and Spring study. Each semester consists of a minimum of 36 units of study. This timing is designed for students interested in extending their time at CMU for developing applied research skills in preparation for further graduate study or research- oriented employment. Note that the per-semester course load is lower, but the total cost is higher since four semesters of tuition are paid. This timing is also recommended for students interested in pursuing a PhD after graduation. The student graduates in May. 3.3.6.1 MCDS Curriculum All MCDS students must complete 144 units of graduate study which satisfy the following curriculum: \\uf0b7 11-637 - Foundations of Computational Data Science \\uf0b7 Four (4) additional MCDS Core Courses (10-601 Introduction to Machine Learning; 05-839 Interactive Data Science; 15-619 Cloud Computing; 11-631 Data Science Seminar; 48 units) \\uf0b7 Three courses (3) from one area of concentration curriculum (36 units) \\uf0b7 Three (3) MCDS Capstone courses (11-634, 11-635 and 11-632) (36 units) \\uf0b7 One (1) Elective: any graduate level course 600 and above in the School of Computer Science (12 units) 3.3.6.2 Common MCDS Core Courses All MCDS students are required to complete four common core courses in their first two semesters: \\uf0b7 10-601 - Machine Learning \\uf0b7 15-619 - Cloud Computing \\uf0b7 05-839 - Interactive Data Science \\uf0b7 11-631 - Data Science Seminar 3.3.6.3 Areas of Concentration In addition to the common MCDS core, all students must complete at least one area of concentration, which consists of three courses in Analytics, Systems, or Human- Centered Data Science. Students consult with their academic advisor and choose one or more areas of concentration during their first semester, in preparation for enrolling in Spring classes. \\uf0b7 Analytics concentration: o One (1) Machine Learning course o One (1) Software Systems course o One (1) big data course \\uf0b7 Systems concentration: o 15-513 Introduction to Computer Systems (elective, prerequisite for many advanced Systems courses) o Three (3) systems project courses \\uf0b7 Human-Centered Data Science concentration: o One (1) Methods course o Two (2) HCI courses A detailed list of courses satisfying each concentration is contained in the MCDS Program FAQ. 3.3.6.4 MCDS Capstone Courses All MCDS students complete three Capstone courses: \\uf0b7 11-634 - Capstone Planning Seminar (12 units) \\uf0b7 11-635 - Capstone Research (12 units) \\uf0b7 11-632 - Data Science Capstone (12 units) MCDS Program Learning Outcomes \\uf0b7 Design, implement and evaluate the use of analytic algorithms on sample datasets. \\uf0b7 Explain how a machine-learning model is developed for and evaluated on real world datasets. \\uf0b7 Design and execute experimental data collection and present resulting analyses using appropriate user experience (UX) techniques including interactive data visualizations. \\uf0b7 Apply and customize analytics, systems and human-centered data science techniques to application-specific data science requirements and objectives. \\uf0b7 Identify tradeoffs among data science techniques (analytics, systems and/or human-centered) and contrast design alternatives, within the context of specific data science application domains. \\uf0b7 Survey, interpret and comparatively criticize state of the art research talks and papers, with emphasis on constructive improvements. \\uf0b7 Organize, execute, report on, and present a real world data science project in collaboration with other researchers/programmers. Depending on the concentration, additional learning outcomes are emphasized: Analytics. Students electing to complete the Analytics concentration will also learn to: \\uf0b7 Design, implement and evaluate a software system and machine-learning model on real world datasets at real world scale. \\uf0b7 Analyze and document data science requirements in different application domains and survey as well as critique state of the art solutions for those requirements. Systems. Students electing to complete the Systems concentration will also learn to: \\uf0b7 Implement and evaluate complex, scalable data science systems, with emphasis on providing experimental evidence for design decisions. \\uf0b7 Anticipate and avert structural and/or implementation problems with systems design, especially with scaling and tail distributions. HCDS. Students electing to complete the Human-Centered Data Science (HCDS) concentration will also learn to: \\uf0b7 Design, implement and evaluate a user experience prototype to allow for clear understanding of data science solutions. \\uf0b7 Apply social and behavioral research methods to data science problems to understand the human aspects of data collection and analysis. Throughout their coursework, students will take introductory courses on all those topics, practice them in advanced courses and seminars and demonstrate all learned skills in their Capstone project and internship. Students are encouraged to choose elective courses in the curriculum according to their professional goals and mastery of the main subjects. 3.3.10 Capstone project The capstone project consists of students working in a team or individually on a project. The capstone project integrates the educational experience of the student. A capstone project is typically a CMU research project, or an industry sponsored project; occasionally students define capstone topics through communication with the faculty. Students interested in defining their own capstone topics should discuss with the MCDS faculty as early as possible. The capstone project is a great opportunity for a student (or student team) to \\u201cshow off\\u201d their unique skills and accomplishments. Capstone projects have been instrumental in the hiring decisions for several employers. 3.3.11 Elective courses Electives can be any graduate level course (numbered 600 or above) in the School of Computer Science. Students use their elective courses to enhance study in an area of interest or to explore new areas of interest. 3.3.12 Undergraduate courses Undergraduate courses are taken to address an area of weakness in the student\\u2019s prior preparation. Undergraduate courses (numbered less than or equal to 599) may be taken pass/fail or for credit but do not count toward the 144 units of eligible study: The course and course grade will appear on the student\\u2019s transcript, and the course grade will factor as part of the student\\u2019s QPA. 3.3.13 Independent study course Independent study courses allow students to cover study of a particular area of interest, and are used when no formal course is available in a given subject area. Students who are interested in continuing to a Ph.D. degree often enroll in Independent Study, since it offers the opportunity to perform research directly with a faculty member. Independent study courses are considered electives. Each independent study course must be advised and approved by at least one faculty member. Agreement to supervise an independent study course is purely voluntary on the part of the faculty member. It is the duty of the student, therefore, to negotiate the terms and conditions of the independent study with the pertinent faculty members of CMU who will be supervising the study. These individuals are referred to as \\u201cindependent study supervisors.\\u201d Once the student finds someone who agrees to supervise such a course, he/she must: 1. Students wishing to take an independent study must request approval from their academic advisor and complete proposal before the first day of classes in a given semester: 2. Enter into an agreement with the independent study supervisor that includes course expectations, including deliverables. 3. Secure the \\u201cIndependent Study Contract Form\\u201d from the MCDS administrator. 4. Complete the form, provide a brief description of the work to be done, including deliverables and how they will be graded. 5. Secure signatures of both the student and the supervisor. Return the form to the MCDS administrator in order to obtain approval for the independent study from the Director. Independent study contracts must be submitted no later than on the last day of the first week of classes in a given semester. 3.3.14 Double counting courses No course may be used to complete two MCDS degree requirements, nor may a course satisfy requirements in two degree programs. 3.3.15 Courses outside of the School of Computer Science Elective courses in other Schools at Carnegie Mellon may be taken with prior permission of the Director. 3.3.16 Grades All courses offered by the SCS CMU are graded on the 4.3 grading standard http://www.cmu.edu/policies/documents/Grades.html. MCDS students must maintain a 3.0 overall average each semester to remain in good standing. A student must obtain a B- or better grade in all courses, which count towards core requirements. If a student receives a C- or better, that course may count as an elective towards the degree requirements. All courses must receive a letter grade; courses taken pass/fail do not count towards the MCDS degree. Enrollment Services is the only University office that can provide an official letter of enrollment, official transcript and enrollment verification. Enrollment verification can be requested online through The HUB at: https://www.cmu.edu/hub/registrar/student- records/verifications/ 3.3.17 Student Review, Academic Probation and Academic Actions The MCDS program conducts an academic progress review at the conclusion of each semester in order to monitor individual student progress towards graduation regarding the fulfillment of curricular requirements, course grades, and academic integrity. Should a student\\u2019s effort fall below the acceptable level of academic performance and/or fail to meet standards and policies established by Carnegie Mellon University, the student may be dismissed from the program. Infractions After each academic progress review, each student will receive a letter indicating the result of the review and their standing in the program. If applicable, the letter will also note the following infractions by the student in the given semester: \\uf0b7 Cumulative QPA is below 3.0, resulting in the student being put on Academic Probation (see below) \\uf0b7 Cumulative QPA is below 2.6, resulting in academic probation or possible dismissal (see below) \\uf0b7 Academic Integrity Violation (AIV) deemed an infraction by the MCDS committee (see 3.7.2) Minimum QPA and Academic Probation Students must maintain a cumulative QPA of 3.0 to remain in good standing with the program. Should a student\\u2019s overall QPA drop below 3.0 during any given semester, he/she will be placed on academic probation for the following semester. In probation cases, the student will be required to \\uf0b7 enroll in courses as advised by the academic advisor, \\uf0b7 improve his/her grades to no less than an overall 3.0 QPA in the following semester, and \\uf0b7 meet any other goal set by the advisor during that period (e.g., fulfilling a core course requirement). If a student\\u2019s cumulative QPA drops below 2.6, the student will be considered at risk of being unable to complete the program and will be considered for dismissal. He/she will be required to meet the program director to discuss his/her situation. Only if, after that meeting, the MCDS program committee ascertains that the student is likely to complete the remaining program requirements in the allotted time, the student will be allowed to continue his/her studies in MCDS, and dismissed otherwise. If the student is allowed to continue their studies, they will be placed on academic probation for the following semester and is subject to the requirements above. Dismissal A student may be dismissed from the program for any of the following cases: \\uf0b7 If the student has been put on academic probation and failed to meet the remedial requirements set by the advisor in the following semester, or committed an Academic Integrity Violation deemed an infraction by the MCDS committee while on academic probation \\uf0b7 If the student has a cumulative QPA of 2.6 or lower and the MCDS program committee does not ascertain that the student is likely to complete the remaining program requirements in the allotted time \\uf0b7 If the student has committed two Academic Integrity Violation deemed infractions by the MCDS committee (see 3.7.2) \\uf0b7 If the student has committed an Academic Integrity Violation infraction where the violation is deemed to be sufficiently egregious as determined by the MCDS program committee \\uf0b7 If the student has been found to infringe a University Policy, where such infringement is deemed grounds for dismissal Students who realize that one of these situations may apply to them are strongly encouraged to meet with the academic advisor to discuss a plan to mitigate the situation. Students who find they are struggling in the program will have the best chances of success if they communicate early and often with the academic advisor. 3.3.18 Incomplete grades Carnegie Mellon University students are expected to complete a course during the academic semester in which the course was taken. However, if the instructor agrees, a grade of \\u201cI\\u201d (incomplete) may be given when a student has been unable to complete the work of a course. However, the work completed up to that date must be of passing quality and the grade of incomplete provides no undue advantage to that student over other students. By awarding an \\u201cI\\u201d grade, an instructor must specify the requirements for the completion of the work and designate a default letter grade in the event that the student fails to complete the remaining work. Students must complete the required course work by no later than the end of the following academic semester or sooner if required by the instructor. The instructor must record the permanent course grade by the last day of the examination period of the following semester, or the Registrar will automatically assign the default grade. If further work has not been completed after one semester and a default grade is rendered, the default grade will become the grade of record. 3.3.19 Change of grades and missing grades If a grade has been assigned in error, it can be changed to a different permanent grade. The procedure for changing a grade is as follows: \\uf0b7 Discuss the matter with the course instructor; provide evidence that the grade issued was not the grade earned. \\uf0b7 If the instructor agrees, the student should contact the program administrator to process a Change of Grade Form in order to correct the grade that was issued in error. Generally, the instructor is the final authority for a course grade. \\uf0b7 If a grade has not been assigned, please notify the course instructor for the completion of a Missing Grade Form. 3.3.20 Qualifying examinations and procedures (or equivalent) None required. 3.3.21 Thesis/dissertation None required. 3.3.22 On transfer to another program If the requirements for the MCDS degree have not been completed when a student leaves to pursue another academic program, the degree will not be awarded. Completion of the MCDS degree does not guarantee admission into any doctoral degree program at Carnegie Mellon University. The courses that will be completed as part of the MCDS may serve to enhance one\\u2019s application to these programs but will in no way insure admittance. 3.3.23 Intellectual property policy The MCDS degree program adheres to Carnegie Mellon University policy on intellectual property: http://www.cmu.edu/policies/documents/IntellProp.html 3.3.24 Teaching requirements None required. However, students are encouraged to apply for teaching assistant positions in courses where they have excelled. 3.3.25 Language proficiency requirements None required. However, non-native English speakers are encouraged to take advantage of the various support functions provided by the Intercultural Communication Center (ICC) and the Global Communication Center (GCC). 3.3.26 Academic Integrity and Policies on Plagiarism and Cheating The university considers any form of cheating or plagiarism to be a serious violation of student ethics. The student is required to understand and rigorously follow only the permitted forms of collaboration as defined by the instructor in every class. The work you submit must be your own, unless you have clearly attributed it to others. You must not use the work of others without proper citation. And, you must not use resources, including other persons, except as authorized by the course or project for which you are submitting the work. Such conduct might be accepted or commonplace elsewhere, but it is not here. Be careful. Be warned. Failure to abide by these rules, even just once, can result in your permanent separation from the University without refund of money paid. Note that the policy requires the student to be informed and understand the academic integrity rules for every assignment or exam in a course. The MCDS program strives to produce graduates with the highest standards of academic integrity. Academic Integrity Violations are taken very seriously and the MCDS program has a zero tolerance policy for multiple Academic Integrity Violations. A single violation is grounds for dismissal from the graduate program if deemed sufficiently egregious as determined by the MCDS program committee. If a student commits a second violation, the expected penalty is dismissal from the graduate program (see also academic progress review at section 3.3.17). Please review the University Policy on Academic Integrity: https://www.cmu.edu/policies/student-and-student-life/academic-integrity.html The policy includes the University expectations around academic integrity and provides definitions of cheating, plagiarism, and unauthorized assistance. A review of the University\\u2019s Academic Disciplinary Actions procedures https://www.cmu.edu/student-affairs/theword/academic-discipline/index.html is also recommended. These procedures outline the process for investigating, reporting, and adjudicating violations of the University Policy on Academic Integrity. The procedures also outline the appeal process. 3.3.27 Process for Appealing Final Grades https://www.cmu.edu/student-affairs/theword/academic/appeal-of-grades-and-academic-actions.html Final grades will be changed only in exceptional circumstances and only with the approval of the instructor and the department, unit or program. Grading is a matter of sound discretion of the instructor and final grades are rarely changed without the consent of the instructor who assigned the grade. The following circumstances are the unusual exceptions that may warrant a grade appeal: (a) the final grade assigned for a course is based on manifest error (e.g. a clear error such as arithmetic error in computing a grade or failure to grade one of the answers on an exam), or (b) the faculty or staff member who assigned the grade did so in violation of a University policy. 3.3.27 Teaching Assistants The MCDS degree does not have a teaching requirement. However, some students may wish to be a teaching assistant. MCDS students may petition for approval to TA up to one course per semester provided that they are in good academic standing (Overall QPA 3.0 or above). As required by the English Fluency in Higher Education Act of 1990, graduate students are required to have a certain level of fluency in English before they can instruct in Pennsylvania. Through this Act, all institutions of higher education in the state are required to evaluate and certify the English fluency of all instructional personnel, including teaching assistants and interns. The full university policy can be reviewed at: http://www.cmu.edu/policies/faculty/evaluation-certification-english-fluency- instructors.html. In addition to administering the International Teaching Assistant (ITA) Test (mandatory screening test for any non-native speaker of English), Language Support in the Student Academic Success Center helps teaching assistants who are non-native English speakers develop fluency and cultural understanding to teach successfully at Carnegie Mellon. Visit the website for additional information: https://www.cmu.edu/student-success/ 3.3.28 Internship Requirement and Search for Full Time Employment An internship is required for the degree program. In some cases, when a student has prior work experience, the Director of the degree program may waive this requirement. Students start searching for internships generally in the Fall and Spring semesters. Once the student returns from an internship in the Fall, they should immediately begin the search for full time employment. Extensive resources are available at http://www.cmu.edu/career/ including a resume submission system, a list of employers, on campus interviews and mock interviews, and many other resources. The Technical Opportunities Conference (TOC) http://engineering.cmu.edu/companies/toc/ occurs every September. This conference is one of the main recruiting events each year. All international students are required to apply for Curricular Practical Training (CPT). CPT is employment that is an integral part of an established curriculum and is directly related to the student\\u2019s major area of study. Please visit the Office of International Education (OIE) link below to learn more about the CPT process: http://www.cmu.edu/oie/forstu/jobs.html 3.4 Enrollment Verification Enrollment Services is the only University office that can provide an official letter of enrollment, official transcript and enrollment verification. Enrollment verification can be requested online through The HUB via this link: http://www.cmu.edu/hub/transcripts/verifications/enrollment.html 3.5 University Policies on Grades and Grading 3.6.1 University Policy on Grades This policy offers details concerning university grading principles for students taking courses and covers the specifics of assigning and changing grades, grading options, drop/withdrawals and course repeats. It also defines the undergraduate and graduate grading standards. You can review the university grading policies here: https://www.cmu.edu/policies/student-and-student-life/grading.html 3.6.2 University Policy on Grades for Transfer Courses Carnegie Mellon University offers students the opportunity to take courses for credit through a cross-registration program (see Pittsburgh Council on Higher Education (PCHE) and Cross-registration below) and through the receipt of transfer credit from other accredited institutions. The Carnegie Mellon University transcript will include information on such courses as follows: Carnegie Mellon courses and courses taken through the university\\u2019s cross-registration program will have grades recorded on the transcript and be factored into the QPA. All other courses will be recorded on this transcript indicating where the course was taken, but without grades. Such courses will not be taken into account for academic actions, honors or QPA calculations. (Note: suspended students may take courses elsewhere; however, they may receive transfer credit only if their college\\u2019s and department\\u2019s policies allow this.) You can review the university policy here: https://www.cmu.edu/policies/student-and-student-life/transfer-credit-evaluation-and- assignment.html 3.6 Academic Integrity 3.7.1 Expectations Regarding Proper Conduct In the midst of self-exploration, the high demands of a challenging academic environment can create situations where some students have difficulty exercising good judgment. Academic challenges can provide many opportunities for high standards to evolve if students actively reflect on these challenges and if the community supports discussions to aid in this process. It is the responsibility of the entire community to establish and maintain the integrity of our university. Carnegie Mellon University educates its students to become professionals who will serve society with integrity. The university also creates and disseminates new knowledge and expressions of knowledge in ways that benefit society. Carnegie Mellon strives to serve the changing needs of society through the three primary goals outlined in its mission statement: to create and disseminate knowledge and art through research and artistic expression, teaching and learning and transfer to society, to serve students by teaching them leadership and problem-solving skills, and the values of quality, ethical behavior, responsibility to society and commitments to work, to pursue the advantages provided by a diverse community, open to the exchange of ideas, where discovery and artistic creativity can flourish. In any presentation, creative, artistic or research, it is the ethical responsibility of each student to identify the conceptual sources of the work submitted. Failure to do so is dishonest and is the basis for a charge of cheating or plagiarism, which is subject to disciplinary action. Please review the University Policy on Academic Integrity (https://www.cmu.edu/policies/student-and-student-life/academic-integrity.html). The policy includes the University expectations around academic integrity and provides definitions of cheating, plagiarism, and unauthorized assistance. A review of the University\\u2019s Academic Disciplinary Actions procedures (https://www.cmu.edu/student-affairs/theword/academic-discipline/index.html) is also recommended. These procedures outline the process for investigating, reporting, and adjudicating violations of the University Policy on Academic Integrity. The procedures also outline the appeal process. 3.7.2 Protocol for Academic Integrity Violations The university has a very clear and specific protocol for responding to alleged violations of academic integrity. Carnegie Mellon's Academic Disciplinary Actions Overview for Graduate Students describes procedures and the appeal process for disciplinary actions against graduate students in cases of alleged academic integrity violation. For more information on disciplinary actions please see: https://www.cmu.edu/student- affairs/theword/acad_standards/creative/disciplinary.html Further documentation on how to respond to an allegation of a violation as a graduate student: https://www.cmu.edu/student-affairs/ocsi/academic-integrity/grads.html Important Note: MCDS implements the above policy\\u2019s option of \\u201cconven[ing] a disciplinary hearing according to the procedures of the department/program\\u201d. We have adopted the following hearing procedure and \\u201ctwo strikes\\u201d rule: \\uf0b7 If an instructor determines that an academic integrity violation has occurred, both the instructor and students are given the opportunity to explain the situation to the MCDS program committee. A written hearing by email suffices for this purpose. \\uf0b7 The program committee then reviews all information and decides whether the violation is deemed an infraction (see 3.3.17), and which secondary actions are to be taken on the program level. \\uf0b7 Two-Strike-Rule: MCDS may dismiss students upon a first AIV infraction. A second infraction will always lead to the offending student being dismissed from the program, with no exceptions. The MCDS program reserves the right to withdraw a degree even though it has been granted should there be discovery that the work upon which it was based or the academic records in support of it had been falsified. In such a case, the degree will be withdrawn promptly upon discovery of the falsification. The complete reference to this university policy is available at: https://www.cmu.edu/policies/student-and-student- life/withdrawal-of-a-degree.html 4 Academic Policies 4.1 MCDS Academic Policies 4.1.1 Duration of Study MCDS students enrolled for full-time studies are normally expected to complete the degree in three semesters (16 months). This includes a summer internship. 4.1.2 Double-Dipping A Masters student who uses courses taken as part of another degree program (at Carnegie Mellon or elsewhere) toward their program requirements cannot use those same courses toward any other M.S. degree offered by the School of Computer Science without prior approval. 4.1.3 Pass/Fail and Audit Grades Pass/fail and audit grades are not permitted for courses used to satisfy a degree requirement. Graduate students who are required to take additional undergraduate courses to build up the core foundations of computer science may not elect the pass/fail or audit option for these courses. 4.1.4 Transfer Credit An equivalent graduate course previously completed at Carnegie Mellon, or another institution, may be permitted to satisfy one of the MCDS course requirements, with permission from the Director. Students must petition for transfer credit by providing the Program Director with the prior course syllabus and other details that may be required by the Director in order to make a decision. See the section on \\u201cDefinition of transfer credit versus course exemption\\u201d. NOTE: In all cases, credit may only be transferred from another degree program for courses deemed \\u201cfree electives\\u201d - i.e., courses which were not used to satisfy a core requirement or total units requirement in a prior degree program. All MS students are required to take a minimum of 96 units of coursework at CMU. 4.1.5 External Internships and Job Interviewing MCDS students are expected to attain an external internship during the summer. International students must coordinate carefully with the University, due to visa restrictions. International students are required to consult with the Office of International Education for eligibility before seeking an internship/co-op or signing an offer contract. We caution all students to be aware of potential intellectual property (IP) implications with internships, and to review any IP agreements with their academic advisor before signing them. It is possible to lose ownership of your own inventions if they occur during an external internship. While it is necessary for students to travel off-campus for job interviews, it is not acceptable for a student to miss a course requirement or a capstone project commitment due to interview travel. Students should work proactively with prospective employers to arrange interview travel in a way that minimizes the impact on their final semester course work. 4.1.6 Transferring into the MCDS Program Direct transfers into the MCDS program are not permitted. Students who are currently enrolled at Carnegie Mellon who wish to transfer into the MCDS program must do so by applying to the MCDS program via the normal admissions process. As specified in Sec. 4.1.4 in this document, some transfer credit and/or exemption from MCDS requirements may be possible on a case-by-case basis. 4.1.7 Transferring Out of the MCDS Program The MCDS program does not prevent students from transferring to another degree program. Each degree program has its own rules about whether and when transfers into the program are permitted. A student that is interested in transferring out of the MCDS degree program should consult the handbook and Program Director of the desired degree program to learn whether transfers are permitted, and if so, how and when to request such a transfer. 4.1.8 Statute of Limitations https://www.cmu.edu/policies/student-and-student-life/masters-students-statute-of- limitations.html As outlined in the Master\\u2019s Students Statute of Limitations (link above), students will complete all requirements for the master\\u2019s degree within a maximum of seven years from original matriculation as a master\\u2019s student, or less if required by a more restrictive department, school or college policy. Once this time-to-degree limit has lapsed, the person may resume work towards a master\\u2019s degree only if newly admitted to a currently offered master\\u2019s degree program under criteria determined by that program. Under extraordinary circumstances, such as leave of absence, military or public service, family or parental leave, or temporary disability, a school or college may, upon the relevant department's recommendation and with the written approval of the dean (or designate), defer the lapse for a period commensurate with the duration of that interruption. Students who are pursuing a master\\u2019s degree as part-time students for all semesters of their program, as approved by their program, may also appeal to their program or department for extension of the time to degree limit. 4.2 LTI Academic Policies 4.2.1 \\u201cGrandfather\\u201d policy A student can graduate under the policies in effect at the time that the student entered the program; or, at the student's choice, the student can graduate under policies that are adopted after the student entered the program. In unusual cases, the Director may approve exceptions to the program requirements. 4.2.2 Course Drop/Add/withdrawal procedures Students taking undergraduate and Master\\u2019s level courses must follow the procedures and deadlines for adding, dropping, or withdrawing from courses as identified on the academic calendar. Information can be found at: https://www.cmu.edu/hub/registrar/course-changes/index.html. There is a separate calendar for masters level courses. 4.2.3 Courses with restricted enrollment MCDS students have priority for the program core courses. The MCDS program administrators cannot intercede with other departments to secure seats for its students in other courses as all departments reserve seats for their accepted students. Usually all remaining open seats are assigned on a first- come, first-served basis. Students can, on occasion, contact the assigned course instructor in order to plead his/her case for admission to the course. Admission may be granted at the discretion of the instructor. The policy of the department offering the course(s) is always followed. 4.2.4 Definition of transfer credit versus course exemption The LTI may grant transfer credit or issue an exemption for equivalent graduate courses previously completed at another institution. This decision rests with the Director of the particular program. If a student is exempt from a required course due to prior courses or experience, the student can replace that course with an open elective. The student does not receive credit for the external course but can take any course that could normally count toward the degree in its place. If a student receives credit for prior coursework completed at CMU or elsewhere, the student receives that many units of credit, and the total amount of required coursework is reduced by that amount. 4.2.5 External Employment/Consulting Since the MCDS program places heavy demands on student time, external employment and/or consulting are strongly discouraged. Exceptional students who wish to consult should discuss this with their advisor. International students must also have approval in advance from the Office of International Education (OIE) for any outside employment. 4.2.6 Leave of Absence A student in good standing may be granted a LOA of at most 1 year, upon written request to the Program Director and with consent of the student's advisor. It is the responsibility of the student on LOA to contact the program administrator to apply for a return to the program. 4.2.7 Withdrawal from Program Students may voluntarily withdraw from the MCDS program. If a student decides to withdraw, or is considering a withdrawal, she/he should contact the program administrator to schedule an advising meeting as soon as possible. The university\\u2019s general withdrawal policy can be found here: https://www.cmu.edu/hub/registrar/leaves-and-withdrawals/ 4.2.8 Satisfactory Progress If a student does not make satisfactory progress each semester toward completing the degree, the LTI may remove the student from the program. See section on \\u201cEnd of Semester Evaluation\\u201d. In particular, students in the three-semester program who fail one of their first-semester MCDS core required courses are strongly encouraged to consider switching to the four-semester program. 4.2.9 Winter and Summer Breaks Students supported by research projects or working in an on-campus internship are expected to remain on campus working during breaks in classes. A two-week vacation is typically allowed in the summer for the students who are working on campus (not pursuing an external internship). Supported students should arrange their winter break time with their supervisor. 4.3 CMU Academic Policies 4.3.1 Assistance for Individuals with Disabilities http://www.cmu.edu/education-office/disability-resources/ The Office of Disability Resources at Carnegie Mellon University has a continued mission to provide physical, digital, and programmatic access to ensure that students with disabilities have equal access to their educational experience. We work to ensure that qualified individuals receive reasonable accommodations as guaranteed by the Americans with Disabilities Act (ADA) and Section 504 of the Rehabilitation Act of 1973. Students who would like to receive accommodations can begin the process through Disability Resources' secure online portal (https://rainier.accessiblelearning.com/cmu/) or email access@andrew.cmu.edu to begin the interactive accommodation process. Students with physical, sensory, cognitive, or emotional disabilities are encouraged to self-identify with the Office of Disability Resources and request needed accommodations. Any questions about the process can be directed to access@andrew.cmu.edu, or call (412) 268-6121. 4.3.2 Summary of Graduate Student Appeal and Grievance Procedures Graduate students will find the Summary of Graduate Student Appeal and Grievance Procedures on the Graduate Education Resource webpage. This document summarizes processes available to graduate students who seek review of academic and non-academic issues. Generally, graduate students are expected to seek informal resolution of all concerns within the applicable department, unit or program before invoking formal processes. When an informal resolution cannot be reached, however, a graduate student who seeks further review of the matter is to follow the formal procedures outlined here. These appeal and grievance procedures shall apply to students in all graduate programs of the University. Students should refer to the department specific information in this handbook for department and college information about the administration and academic policies of the program. You can review a summary of the university\\u2019s graduate student\\u2019s appeal and grievance procedures here: https://www.cmu.edu/graduate/policies/appeal-grievance-procedures.html 4.3.3 Safeguarding Educational Equity: Sexual Harassment and Sexual Assault The University prohibits sex-based discrimination, sexual harassment, sexual assault, dating/ domestic violence and stalking. The University also prohibits retaliation against individuals who bring forward such concerns or allegations in good faith. The University\\u2019s Sexual Misconduct Policy is available at https://www.cmu.edu/policies/administrative-and-governance/sexual- misconduct/index.html. The University\\u2019s Policy Against Retaliation is available at: https://www.cmu.edu/policies/administrative-and-governance/whistleblower.html If you have been impacted by any of these issues, you are encouraged to make contact with any of the following resources: \\uf0b7 Office of Title IX Initiatives: http://www.cmu.edu/title-ix/, 412-268-7125, tix@cmu.edu \\uf0b7 University Police: https://www.cmu.edu/police/, 412-268-2323 \\uf0b7 Additional resources and information can be found at: https://www.cmu.edu/title-ix/resources-and-information/index.html 4.3.4 Consensual Intimate Relationship Policy Regarding Undergraduate Students https://www.cmu.edu/policies/student-and-student-life/consensual-relationships.html This policy addresses the circumstances in which romantic, sexual or amorous relationships/interactions with undergraduate students, even if consensual, are inappropriate and prohibited. The purpose of this policy is to assure healthy professional relationships. This policy is not intended to discourage consensual intimate relationships unless there is a conflicting professional relationship in which one party has authority over the other as in the policy. 4.3.5 Gestational and Parental Accommodations https://www.cmu.edu/graduate/programs-services/maternity-accommodation- protocol.html Students whose anticipated delivery date is during the course of the semester may consider taking time away from their coursework and/or research responsibilities. Any student who gives birth to a child while engaged in coursework or research is eligible to take either a short-term absence or formal leave of absence. Students are encouraged to consult with relevant university faculty and staff as soon as possible as they begin making plans regarding time away. \\uf0b7 Students must contact the Office of the Dean of Student Affairs to register for Maternity Accommodations. Students will complete an information form and meet with a member of the Dean\\u2019s Office staff to determine resources and procedures appropriate for the individual student. o Planning for the student\\u2019s discussion with appropriate academic contact(s) (advisor, associate dean, etc.) may be reviewed during this meeting. \\uf0b7 Students in course work should consider options for the semester of the anticipated birth such as working with their course instructors to receive incomplete grades, electing to drop to part-time status or taking a full semester leave of absence. \\uf0b7 Students engaged in research must work with their faculty to develop plans for the research for the time they are away and for resuming upon return. \\uf0b7 Master's students who receive an academic stipend funded by Carnegie Mellon are eligible to continue to receive stipend funding for up to six (6) weeks during a Short-Term Maternity Accommodation or a Formal Leave of Absence. Continued academic stipend funding may be extended by two (2) weeks, for a total of eight (8) weeks, if an absence longer than six weeks is medically necessary. To receive this support students must be registered with the Office of the Dean of Students. 4.3.6 Change of Address MCDS students are responsible for notifying MCDS and the HUB of all address changes in a timely manner. Students will be held responsible for any failure to receive official college notices due to not having a correct address on file; F-1 students may jeopardize their status if address information is not kept current. Students can change their address using SIO, which is available via the HUB website: http://www.cmu.edu/hub/index.html 5 Financial Issues 5.1 Tuition 5.1.1 Tuition payments To understand your invoice, payment options, etc., visit: http://www.cmu.edu/hub. The MCDS program sets tuition each year for all students in the program. The student must handle tuition problems by contacting The Hub. The MCDS Program Administrators cannot solve tuition problems. 5.1.2 Sponsored Students A sponsored student is one who has another party (such as an embassy or company) who has agreed to pay the student\\u2019s tuition. Please read the \\u201cSponsored Students\\u201d section. 5.1.3 Employer Reimbursement Process If you have an employer reimbursement plan, write your employer\\u2019s name and address on the bill (or provide CMU with a letter of support) and pay at least one-third of the tuition charge when returning the bill. You must pay previous semester balances before you can enroll for the next semester. 5.1.4 Carnegie Mellon employee reimbursement procedure Contact the Benefits Office for specific information on tuition benefits. You must complete a Tuition Remission Form each semester in order to receive these benefits. To receive a Tuition Remission Form, visit the Human Resources website at: http://www.cmu.edu/hr. 5.1.5 Financial aid, tuition waivers, Scholarships The MCDS degree program does not provide financial aid to graduate students, nor does it provide tuition waivers or scholarships. For complete financial aid information see: https://www.cmu.edu/sfs/financial-aid/index.html . The Financial Aid Office of Carnegie Mellon will provide assistance in completing the necessary paperwork to apply for Stafford loans. Graduate students should consult the graduate student financial aid information found on The HUB website: https://www.cmu.edu/sfs/financial-aid/graduate/index.html . Students will find the Graduate Financial Aid Guide, information about funding options and how to apply for financial aid and other helpful links. Graduate students who find themselves in need of immediate funds for emergency situations should contact the Office of the Dean of Student Affairs (see Appendix A), www.cmu.edu/student-affairs/index.html, to inquire about an Emergency Student Loan. U.S. citizens and permanent residents may complete the Free Application for Federal Student Aid (FAFSA) online at https://fafsa.ed.gov Students may obtain information regarding their loans through the William D. Ford Direct Loan Program, including deferment forms and payment information at http://www.dlssonline.com/index.asp Information about the federal student aid programs may be found at www.studentaid.ed.gov 5.1.6 External fellowships The MCDS program will accept students with external fellowships. 5.1.7 Grade Reports Grade reports are mailed to individual students by the university at the conclusion of each semester. See the official calendar for mailing dates. 5.1.8 Late Graduation On occasion, a student and/or his/her employer may request that the student attend Carnegie Mellon for an additional semester in order to complete a program that will be beneficial to both the student and the employer. Under such circumstances late graduation can be arranged. Student is to discuss his/ her situation with the Director. 5.1.9 Employment Eligibility Verification If you are receiving a stipend, are a TA, or are planning to have a position with CMU then Employment Eligibility Verification is required. Form I-9 must be completed within 3 business days of beginning work for any type of compensation (stipend or employment). Additional details are highlighted below. To ensure compliance with federal law, Carnegie Mellon University maintains the Employment Eligibility Verification (I-9) Policy covering the university\\u2019s I-9 and E-Verify requirements: \\uf0b7 Every individual receiving a stipend from CMU or employed by CMU must comply with the I-9 Policy by completing the Form I-9 within three business days following the first day of stipend start date/employment. \\uf0b7 Individuals who expect to work on a federally funded project are further responsible for submitting an E-Verify Processing Request Form to the Office of Human Resources if required. \\uf0b7 For more information, please see CMU\\u2019s Guidance for Completing the Form I-9 and E-Verify Requirements at CM or visit the Human Resources Service website to learn more about Form I-9 and E-Verify and to schedule an appointment to complete the Form I-9. 5.2 Conference Travel Funding Students funded by a research project may receive travel funding according to policies set by the individual projects. Students who have no project funding may be provided with partial funding, with a larger amount available for travel to present a refereed conference paper. There is an LTI form that must be filled out in advance. An additional conference travel funding opportunity is provided by GSA and the Provost\\u2019s Office for students, student work groups or groups to attend a conference, whether as a participant or as a presenter. The process is managed by the Graduate Education Office. Students can find more information about the application process and deadlines at: https://www.cmu.edu/graduate/professional-development/index.html 5.3 Expenses The program will reimburse any expenses incurred on behalf of the MCDS program if approved by the Director. The student must apply for approval of expenses before they are incurred. Verification of purchase and/or expenses along with receipts is to be presented to the program administrator for reimbursement. Reimbursement requests must be filed within three months of the calendar date when it was incurred. If the reimbursement request concerns pre-approved conference or workshop travel expenses of an MCDS students, then it must be filed within three months of the calendar date of the event\\u2019s last day. Reimbursement requests received after this period will not be processed. The University does not reimburse for taxes. 5.4 Health Insurance https://www.cmu.edu/health-services/student-insurance/plans.html Carnegie Mellon has a Student Health Insurance policy requiring full-time, degree- seeking students to carry adequate medical insurance. Students must either purchase the plan offered by the University or an application for a waiver can be made if the student is \\u201cenrolled as the dependent, partner/spouse or principal in an employer or government-sponsored insurance plan\\u201d. It is the responsibility of each student to make arrangements with Student Health Services to either pay for their insurance at the beginning of the semester or elect a payment plan over the course of the academic year. More information is available at the Student Health Services Web site https://www.cmu.edu/health-services/student-insurance/plans.html. 5.5 Emergency Loans Graduate students who find themselves in need of immediate funds for emergency situations should contact the Office of the Dean of Students (see Appendix A), www.cmu.edu/student-affairs/index.html, to inquire about the types of emergency funding available to enrolled students. 6 Additional University Resources 6.1 The HUB Student Services Center The HUB is located in Warner Hall, Lower Level. The HUB staff delivers comprehensive service and counsel to students and families regarding financial aid, billing and payment, registration and academic records. The Assistant Directors in The HUB serve as contacts for specific colleges and assist enrolled students with key aspects of the enrollment process. Student can find their assigned HUB Assistant Director on their Student Information Online (SIO) Resource page. Questions that need specialized, in- depth attention can be directed to the student's assigned Assistant Director. For general questions and information, students may email The HUB or call 412-268-8186. thehub@andrew.cmu.edu and http://www.cmu.edu/hub/ 6.2 Student Information Online (SIO) Student Information Online (SIO) is a secure site where students can find important, personalized information, including E-Bills and student account information, financial aid status and eligibility, grades and QPA, and course schedules. Students can update their and their spouse's or domestic partner's contact information, sign up for E-Check & E-Refund, authorize their spouses, domestic partners or other individual to receive a copy of their E-Bill, request verifications, view their housing and meal plan assignments, and much more. Students can log on to SIO by going to www.cmu.edu/hub/sio and entering their Andrew User ID and password. On SIO, students will designate an emergency contact address of a relative or family friend to be contacted in the case of an emergency. If students do not want their name and address published in the campus directory, they must notify the HUB in writing. 6.3 ID Cards Graduate students can obtain their ID card from The HUB once they have been entered into SIO for the semester. These cards identify their holders as members of the campus community. Student cards are deactivated upon the cardholder\\u2019s separation from the university. Affiliate ID Cards are available for spouses and partners of graduate students that allow them to access Carnegie Mellon\\u2019s campus. These cards are available through The HUB to spouses and partners of graduate students who are enrolled for the current academic year in a full-time graduate degree program. The card is valid for one year. For information about domestic partner registration, visit the Office of the Dean of Student Affairs webpage: http://www.studentaffairs.cmu.edu/dean/domestic_partner/. For more information about student and affiliate ID cards (spouse, domestic partners and dependent children), please visit: http://www.cmu.edu/idplus/idcards/cardtypes.html. 6.4 Transcripts Information about and instructions for ordering transcripts are available at: www.cmu.edu/hub/transcripts.html. Transcript questions may be directed to esg- transcripts@andrew.cmu.edu. 6.5 Pittsburgh Council on Higher Education (PCHE) and Cross-registration Carnegie Mellon University offers students the opportunity to take courses for credit through a cross-registration program (see Pittsburgh Council on Higher Education (PCHE) and Cross-registration below) and through the receipt of transfer credit from other accredited institutions. The Carnegie Mellon University transcript will include information on such courses as follows: Carnegie Mellon courses and courses taken through the university's cross-registration program will have grades recorded on the transcript and be factored into the QPA. All other courses will be recorded on this transcript indicating where the course was taken, but without grades. Such courses will not be taken into account for academic actions, honors or QPA calculations. NOTE: Suspended students may take courses elsewhere; however, they may receive transfer credit only if their college's and department's policies allow this. You can review the PCHE cross-registration guidelines here: https://www.cmu.edu/policies/student-and-student-life/cross-college-university- registration.html 6.6 Student Privacy Rights and FERPA This university policy notifies students of their rights under the federal Family Educational Rights and Privacy Act (FERPA). According to FERPA, students have the right to: \\uf0b7 Inspect and review their education records; \\uf0b7 Request an amendment to their education record if they believe they are inaccurate or misleading; \\uf0b7 Request a hearing if their request for an amendment is not resolved to their satisfaction; \\uf0b7 Consent to disclosure of personally identifiable information from their education records, except to the extent that FERPA authorizes disclosure without consent; \\uf0b7 File a complaint with the U.S. Department of Education Family Policy Compliance Office if they believe their rights under FERPA have been violated. For questions about Student Privacy Rights, FERPA or filing a complaint, contact John Papinchak, University Registrar, jp7p@andrew.cmu.edu, in Enrollment Services. You can review the university\\u2019s policy on privacy rights here: https://www.cmu.edu/policies/student-and-student-life/privacy-rights- students.html 6.7 Academic Calendar The Academic Calendar can be found at https://www.cmu.edu/hub/calendar/index.html and provides information on all deadlines including registration dates, class start dates, add/drop deadlines, exam dates and more. 6.8 Professional Development The Career and Professional Development Center (CPDC) at Carnegie Mellon is a centralized career center staffed by a team of seasoned and highly educated professionals who orchestrate the career exploration, experiential learning, and career networking needs of students and alumni. CMU's career and professional development model is grounded in discipline-specific career development, experiential learning, and employer relations shaped by strong connections with the university's seven academic colleges. The center's success is founded upon a solid understanding of career and professional development theory, integration of technology, and an unwavering commitment to providing personalized attention towards meeting the unique individual needs of students, alumni, and employers. The CDPC is located on the Lower Level of the University Center, 412-268-2064 The Office of the Assistant Vice Provost for Graduate Education (AVPGE) offers a robust schedule of professional development opportunities. Some are geared towards a specific population (master\\u2019s students, PhD students at the beginning of their program, graduate students seeking tenure track positions, etc.) and others are open to all graduate students (time management, balancing, staying healthy). A full schedule of programs can be found at: http://www.cmu.edu/graduate/. http://www.studentaffairs.cmu.edu/career/index.html 6.9 University Libraries There are three main libraries at Carnegie Mellon: Hunt Library, Mellon Institute Library and Engineering & Science Library with the combined mission of providing access and help to graduate students in finding the information needed, teaching graduate students to evaluate available information and use reliable sources. The libraries\\u2019 digital resources and services, including off-campus/ wireless access to databases and e- journals, offer online access. There are also two neighboring libraries open to Carnegie Mellon graduate students: Carnegie Library of Pittsburgh and University of Pittsburgh Libraries. Visit the University Libraries website for information about all mentioned library locations and hours, on-line resources and FAQ\\u2019s. More information can be found at: http://www.library.cmu.edu/ 6.10 Computing Services Computing Services is located in Cyert Hall 285. Computing Services develops, maintains and supports the computing infrastructure for Carnegie Mellon students, faculty members and staff members. This includes the campus wired and wireless networks, public computer labs or \\u201cclusters,\\u201d cable television and telephone services, computing related documentation and support through the Help Center. In addition, Computing Services provides standard classroom technologies for over 100 lecture halls, classrooms and seminar rooms across campus. The website contains additional information regarding The Help Center hours, location and contact information, computing cluster hours and location, the Carnegie Mellon web portal, computing security and policies and guidelines. Students can email the Help Center at advisor@andrew.cmu.edu with questions and for assistance. More information http://www.cmu.edu/computing/ 6.11 Family and Dependents Resources The Graduate Student Assembly website maintains a resource page for spouses, domestic partners and families of graduate students, including The Student Parent Association, new mother rooms, and links to resources around campus and the Pittsburgh area. Affiliate ID Cards are available for spouses and domestic partners of graduate students that allow them to access Carnegie Mellon\\u2019s campus. These cards are available through The HUB to spouses and partners of graduate students who are enrolled for the current academic year in a full- time graduate degree program. The card is valid for one year. More information can be found at: http://www.cmu.edu/stugov/gsa/resources/family.html For more information about student and affiliate ID cards, please visit: http://www.cmu.edu/idplus/idcards/cardtypes.html. 6.12 Domestic Partner Registration Carnegie Mellon extends certain benefits to domestic partners of students. Eligible students may elect benefits for their domestic partners through the registration process orchestrated by the Office of the Dean of Student Affairs, located on the 3rd floor of Warner Hall. See the web site for information regarding the benefits available for domestic partners, eligibility for domestic partner benefits, registration instructions and forms. More information can be found at: http://www.studentaffairs.cmu.edu/dean/domestic_partner/index.html 6.13 Housing The University does not currently offer housing to graduate students. The Office of Housing and Dining Services does provide community housing information on a very limited basis to assist graduate students who are seeking housing in the communities surrounding the university, including information on the legal aspects of renting an apartment, moving checklists and the off-campus housing database. More information can be found at: www.cmu.edu/housing/community-housing/index.html 6.14 Dining Dining services and operations are offered through the Office of Housing and Dining Services. The office operates dining locations open around campus in academic buildings, Hunt Library and the University Center. These locations offer flexible hours with options from the early morning through late night. The Dining Service website contains information about dining locations, hours of operation, graduate student dining plans forms, nutritional information, and weekly menus for dining locations. More information can be found at: http://www.cmu.edu/dining/ 6.15 Parking and Transportation Graduate students will find information about parking and availability, parking policies, transportation options and Port Authority Transit usage with a valid university ID on the Parking and Transportation Services site. The Parking and Transportation Services office is located in the lower level of the University Center, LL#8. There is limited parking on campus and the varying permit rates can be found on the website. All parking areas of campus are either by permit, metered or by the hour in the garage. Parking and Transportation Services will ticket any car parked in a permit area without a permit or at an expired meter. The city monitors the metered parking along Margaret Morrison, Frew and Tech Streets and will ticket at expired meters as well. More information can be found at: http://www.cmu.edu/parking/ The University offers shuttle and escort services operated through University Police. The Shuttle Service operates several routes within Oakland, Squirrel Hill and Shadyside areas, as well as to university sites located outside of the main campus. The Escort Service offers vehicle routes within a radius of campus between 6:30 pm-6 am daily. Information regarding up-to-date shuttle and escort schedules, pick-up/drop-off locations, routes and usage policies can be found at: www.cmu.edu/police/shuttleandescort/. SafeWalk provides another option to campus community members walking across and around campus during late-night hours. SafeWalk is a student volunteer organization that provides campus escorts for all members of the Carnegie Mellon community. SafeWalk operates nightly during the regular academic year (except certain holidays and break periods) from 10pm until 2am. Students, faculty and staff may request an escort by calling 412-268-SAFE (8-7233 from a campus phone), by approaching an escort team, or by stopping by the SafeWalk dispatch area in the University Center, Lower Level near the Post Office Package Pick- Up window between 10pm-2am. SafeWalk will escort to locations approximately one mile from campus. Additional SafeWalk information can be found at: www.studentaffairs.cmu.edu/safewalk. 6.16 Copying, Printing and Mailing Services Carnegie Mellon offers community members easy access to FedEx, copy centers, printing and mailing services, and postal services. More information regarding these services, locations and contact information can be found at the provided link. More information can be found at: http://www.cmu.edu/student- affairs/theword/campus_resources/copyprintmail.html 6.17 University Center The University Center is a centerpiece of the campus that provides a space for special events, physical fitness, student organizations and various activities, as well as accommodating retail and dining services. As the campus crossroads, the University Center functions as a place for students to interact, get involved and enjoy new experiences. Visit the University Center website for information about campus eateries, ATMs and PNC Bank, fitness rooms and schedules, retail stores, scheduling University Center space, the public prayer room, student organizations and the Wright-Rogal Chapel. The University Center Information Desk is the location if you want to know about upcoming campus events or have questions about Carnegie Mellon in general, call the Information Desk at 412-268-2107. The Information Desk not only provides information about campus events, but also sells postage stamps, makes copies, sends faxes, distributes campus maps, manages a lost & found, and has information brochures about Pittsburgh and the campus. More information can be found at: http://www.cmu.edu/university-center 6.18 Athletic/Fitness Facilities For the participant or the spectator, Carnegie Mellon offers intercollegiate athletics, intramural sports, physical education classes and club sports. The Athletics Department also offers aerobics classes in the University Center and Skibo Gym as well as occasional workshops and instruction related to fitness and health. The Athletics Office is located in the Skibo Gymnasium. Skibo Gym facilities include courts for basketball, volleyball, badminton, as well as weight- training and aerobic equipment. The University Center\\u2019s recreational facilities include an eight- lane pool, racquetball and squash courts, aerobics room, fitness center and gym for basketball and volleyball. All users must present a current Carnegie Mellon Card to use these facilities. More information can be found at: http://www.cmu.edu/athletics 6.19 CMU Alert CMU Alert sends voice and/or text messages to registered phones in the event of a campus emergency that threatens public safety or during tests of the system in the Spring and Fall semesters. Students can register for CMU Alert through the web site. More information can be found at: http://www.cmu.edu/alert 6.20 Accidents on CMU property Please report all accidents to Charity Anderson (caanders@andrew.cmu.edu) and the MCDS program administrator. You will be asked to complete an accident report. By reporting accidents, the student helps minimize future harm! Serious accidents and accidents taking place elsewhere on campus should be reported to Campus Police, x8-6232 (non-emergency), x8-2323 (emergency). 6.21 Consumer Information Carnegie Mellon University suggests that all current and prospective students be informed consumers. Please see this link for detailed consumer information: https://www.cmu.edu/hub/consumer- information/. Appendix A 2023-2024 Highlighted University Resources for Graduate Students Table of Contents Key Resources for Graduate Student Support 1 Office of Graduate and Postdoctoral Affairs 1 Office of the Dean of Students 1 The Division of Student Affairs 2 Center for Student Diversity & Inclusion 2 Assistance for Individuals with Disabilities 3 Eberly Center for Teaching Excellence & Educational Innovation 3 Graduate Student Assembly 4 Office of International Education (OIE) 4 Veterans and Military Community 5 Carnegie Mellon Ethics Hotline 5 Policy Against Retaliation 5 Key Offices for Academic & Research Support 6 Computing and Information Resources 6 Student Academic Success Center 6 University Libraries 6 Research at CMU 7 Office of Research Integrity & Compliance 7 Key Offices for Health, Wellness & Safety 7 Counseling & Psychological Services 7 Health Services 8 Campus Wellness 8 Religious and Spiritual Life Initiatives (RSLI) 8 University Police 9 Shuttle and Escort Services 9 The WORD 10 7 Key Resources for Graduate Student Support 8 Office of Graduate and Postdoctoral Affairs https://www.cmu.edu/graduate graded@cmu.edu The Office of Graduate and Postdoctoral Affairs provides university-wide support for all graduate students and academic programs, with a focus on supporting graduate student success at Carnegie Mellon. Examples of resources offered through the Office of Graduate and Postdoctoral Affairs include, but are not limited to: \\uf0b7 Website with university resources, contact information for CMU programs and services, possible financial assistance and potential funding opportunities, and various procedural and policy information \\uf0b7 Newsletter to all graduate students with information on activities, resources, and opportunities \\uf0b7 Professional development seminars and workshops, and various programming and events for the graduate student community The Office of Graduate and Postdoctoral Affairs also works directly with the colleges and departments on issues related to graduate students and serve as a resource for developing policy and procedures. The Office of Graduate and Postdoctoral Affairs partners with many other offices and organizations, such as the Graduate Student Assembly, to support the holistic graduate student educational experience. 9 Office of the Dean of Students https://www.cmu.edu/student-affairs/dean/ The Office of the Dean of Students provides central leadership of the meta curricular experience at Carnegie Mellon including the coordination of student support. Graduate students will find the enrollment information for Domestic Partner Registration and Parental Accommodations in the Office of the Dean of Students or on their website. This Office also manages the Student Emergency Support Funding process. There are currently three forms of support funding for enrolled students: emergency student loans, student parental loans, and the Tartan Emergency Support Fund. Inquiring students will be provided with additional information about the various types of funding during a consultation meeting with a member of the Dean of Students team. Tuition costs are not eligible for Student Emergency Support funding. College Liaisons and the Student Support Resources team serve as additional resources for graduate students. College Liaisons are senior members of the Division of Student Affairs who work with departments and colleges addressing student concerns across a wide range of issues. College Liaisons are identified on the Important Contacts list in Student Information Online (SIO). The Student Support Resources team offers an additional level of support for students who are navigating a wide range of life events. Student Support Resources staff members work in partnership with campus and community resources to provide coordination of care and support appropriate to each student\\u2019s situation. 10 The Division of Student Affairs The Division of Student Affairs includes (not an exhaustive list): \\uf0b7 Athletics, Physical Education and Recreation \\uf0b7 Career and Professional Development Center (CPDC) \\uf0b7 Center for Student Diversity and Inclusion \\uf0b7 Cohon University Center \\uf0b7 Counseling & Psychological Services (CaPS) \\uf0b7 Dining Services \\uf0b7 Office of Community Standards and Integrity (OCSI) \\uf0b7 Office of Student Leadership, Involvement, and Civic Engagement (SLICE) \\uf0b7 University Health Services (UHS) \\uf0b7 Wellness Initiatives 11 Center for Student Diversity & Inclusion https://www.cmu.edu/student-diversity/ Diversity and inclusion have a singular place among the values of Carnegie Mellon University. The Center for Student Diversity & Inclusion actively cultivates a strong, diverse and inclusive community capable of living out these values and advancing research, creativity, learning and development that changes the world. The Center offers resources to enhance an inclusive and transformative student experience in dimensions such as access, success, campus climate and intergroup dialogue. Additionally, the Center supports and connects historically underrepresented students and those who are first in their family to attend college in a setting where students\\u2019 differences and talents are appreciated and reinforced, both at the graduate and undergraduate level. Initiatives coordinated by the Center include, but are not limited to: \\uf0b7 First generation/first in family to attend college programs \\uf0b7 LGBTQ+ Initiatives \\uf0b7 Race and ethnically focused programs, including Inter-University Graduate Students of Color Series (SOC) and PhD SOC Network \\uf0b7 Women\\u2019s empowerment programs, including Graduate Women\\u2019s Gatherings (GWGs) 12 Assistance for Individuals with Disabilities https://www.cmu.edu/disability-resources/ The Office of Disability Resources at Carnegie Mellon University has a continued mission to provide physical, digital, and programmatic access to ensure that students with disabilities have equal access to their educational experience. The Office works to ensure that qualified individuals receive reasonable accommodations as guaranteed by the Americans with Disabilities Act (ADA) and Section 504 of the Rehabilitation Act of 1973. Students who would like to receive accommodations can begin the process through Disability Resources' secure online portal or email access@andrew.cmu.edu to begin the interactive accommodation Process. Students with physical, sensory, cognitive, or emotional disabilities are encouraged to self-identify with the Office of Disability Resources and request needed accommodations. Any questions about the process can be directed to access@andrew.cmu.edu, or call (412) 268- 6121. 13 Eberly Center for Teaching Excellence & Educational Innovation https://www.cmu.edu/teaching/ The Eberly Center offers a wide variety of confidential, consultation services and professional development programs to support graduate students as teaching assistants or instructors of record during their time at Carnegie Mellon University and as future faculty members at other institutions. Regardless of one's current or future teaching context and duties, Eberly\\u2019s goal is to disseminate evidence-based teaching strategies in ways that are accessible and actionable. Programs and services include campus-wide Graduate Student Instructor Orientation events and our Future Faculty Program, both of which are designed to help participants be effective and efficient in their teaching roles. The Eberly Center also assists departments in creating and conducting customized programs to meet the specific needs of their graduate student instructors. Specific information about Eberly Center support for graduate students is found at: https://www.cmu.edu/teaching/graduatestudentsupport/ 14 Graduate Student Assembly https://www.cmu.edu/stugov/gsa/ The Graduate Student Assembly (GSA) is the branch of Carnegie Mellon Student Government that represents and advocates for the diverse interests of all graduate students at CMU. GSA is composed of representatives from the different graduate programs and departments who want to improve the graduate student experience at the different levels of the university. GSA is funded by the Student Activities Fee from all graduate students. GSA passes legislation, allocates student activities funding, advocates for legislative action locally and in Washington D.C. on behalf of graduate student issues and needs, and otherwise acts on behalf of all graduate student interests. GSA\\u2019s recent accomplishments are a testament to their making a difference, and steps to implementing the vision laid out by the strategic plan. https://www.cmu.edu/stugov/gsa/about-the-gsa/strategic-plan.html GSA offers an expanding suite of social programming on and off-campus to bring graduate students from different departments together and build a sense of community. GSA is the host of the Graduate Student Lounge on the 3rd floor of the Cohon University Center. GSA also maintains a website of graduate student resources on and off-campus. GSA continues to rely on student feedback to improve the graduate student experience at CMU. Feel free to contact them at gsa@cmu.edu to get involved, stop by their office in the Cohon University Center Room 304 or become a representative for your department. 15 Office of International Education (OIE) https://www.cmu.edu/oie/ Carnegie Mellon hosts international graduate and undergraduate students who come from more than 90 countries. The Office of International Education (OIE) is the liaison to the University for all non-immigrant students and scholars, as well the repository for study abroad opportunities. OIE provides many services including: advising on personal, immigration, study abroad, academic, and social and acculturation issues; presenting programs of interest such as international career workshops, tax workshops, and cross-cultural and immigration workshops; international education and statistics on international students in the United States; posting pertinent information to students through email and the OIE website and conducting orientation and pre- departure programs. 16 Veterans and Military Community https://www.cmu.edu/veterans/ Military veterans are a vital part of the Carnegie Mellon University community. Graduate students can find information on applying for veteran education benefits, campus services, veteran\\u2019s groups at CMU, and non-educational resources through the Veterans and Military Community website. There are also links and connections to veteran resource in the Pittsburgh community. The ROTC and Veteran Affairs Coordinator can be reached at urovaedbenefits@andrew.cmu.edu or 412-268-8747. 17 Carnegie Mellon Ethics Hotline https://www.cmu.edu/hr/resources/ethics-hotline.html The health, safety and well-being of the university community are top priorities at Carnegie Mellon University. CMU provides a hotline that all members of the university community should use to confidentially report suspected unethical activity, violations of university policy, or violations of law. Students, faculty and staff can anonymously file a report by calling 1-844- 587-0793 or visiting https://cmu.ethicspoint.com/. All submissions are reported to appropriate university personnel and handled discreetly. The hotline is NOT an emergency service. For emergencies, call University Police at 412-268-2323. 18 Policy Against Retaliation It is the policy of Carnegie Mellon University to protect from retaliation any individual who makes a good faith report of a suspected violation of any applicable law or regulation, university Policy or procedure, any contractual obligation of the university, and any report made pursuant to the Carnegie Mellon University Code of Business Ethics and Conduct. Additional details regarding the Policy Against Retaliation are available at: https://www.cmu.edu/policies/administrative-and-governance/whistleblower.html 19 Key Offices for Academic & Research Support 20 Computing and Information Resources https://www.cmu.edu/computing/ Computing Services maintains and supports computing resources for the campus community, including the campus wired and wireless networks, printing, computer labs, file storage, email and software catalog. As members of this community, we are all responsible for the security of these shared resources. Be sure to review the Safe Computing (https://www.cmu.edu/computing/safe/) section and the University Computing Policy (https://www.cmu.edu/policies/information-technology/computing.html) Visit the Computing Services website (https://www.cmu.edu/computing/) to learn more. For assistance the Computing Services Help Center is available at 412-268-4357 (HELP) or ithelp@cmu.edu. 21 Student Academic Success Center https://www.cmu.edu/student-success/ The Student Academic Success Center\\u2019s (SASC) work to support success focuses on creating spaces for students to engage in their coursework and approach to learning through many group and individual program options. SASC supports student success by providing academic coaching, subject-specific tutoring, effective communication strategies, accommodations for students with disabilities, and language support for multilingual learners. SASC engages with faculty and staff to improve the coordination and professional development for academic advisors. Visit the SASC website for more information about services offered in areas such as communication and language support; language and cross- cultural support; and learning support. 22 University Libraries https://www.library.cmu.edu/ The University Libraries offers a wide range of information, resources, and services supporting graduate students in coursework, research, teaching, and publishing. The library licenses and purchases books, journals, media, and other needed materials in various formats. Library liaisons, consultants, and information specialists provide in-depth and professional assistance and advice in all-things information, including: \\uf0b7 Locating and obtaining specific resources \\uf0b7 Providing specialized research support \\uf0b7 Advanced training in the use and management of data Sign up for workshops and hands-on topic-specific sessions such as data visualization with Tableau, cleaning data with OpenRefine, and getting started with Zotero. Weekly drop-in hours for Digital Humanities and for Research Data Research Management are scheduled during the academic year. Start at the library home page to find the books, journals, and databases you need; to identify and reach out to the library liaison in your field; to sign up for scheduled workshops; and to connect with consultants in scholarly publishing, research data management, and digital humanities. 23 Research at CMU https://www.cmu.edu/research/ The primary purpose of research at the university is the advancement of knowledge in all fields in which the university is active. Research is regarded as one of the university\\u2019s major contributions to society and as an essential element in education, particularly at the graduate level and in faculty development. Research activities are governed by several university policies. Guidance and more general information are found by visiting the Research at Carnegie Mellon website. 24 Office of Research Integrity & Compliance https://www.cmu.edu/research-compliance/ The Office of Research Integrity & Compliance (ORIC) is designed to support research at Carnegie Mellon University. The staff work with researchers to ensure research is conducted with integrity and in accordance with federal and Pennsylvania regulation. ORIC assists researchers with human subject research, conflicts of interest, responsible conduct of research, export controls, and institutional animal care & use. ORIC also provides consultation, advice, and review of allegations of research misconduct. 25 Key Offices for Health, Wellness & Safety 26 Counseling & Psychological Services https://www.cmu.edu/counseling/ Counseling & Psychological Services (CaPS) affords the opportunity for students to talk privately about academic and personal concerns in a safe, confidential setting. An initial consultation at CaPS can help clarify the nature of the concern, provide immediate support, and explore further options if needed. These may include a referral for counseling within CaPS, to another resource at Carnegie Mellon, or to another resource within the larger Pittsburgh community. CaPS also provides workshops and group sessions on mental health related topics specifically for graduate students on campus. CaPS services are provided at no cost. Appointments can be made in person, or by telephone at 412-268-2922. 27 Health Services https://www.cmu.edu/HealthServices/ University Health Services (UHS) is staffed by physicians, advanced practice clinicians and registered nurses who provide general medical care, allergy injections, first aid, gynecological care, and contraception as well as on-site pharmaceuticals. The CMU Student Insurance Plan covers most visit fees to see the physicians and advanced practice clinicians & nurse visits. Fees for prescription medications, laboratory tests, diagnostic procedures and referral to the emergency room or specialists are the student\\u2019s responsibility and students should review the UHS website and their insurance plan for detailed information about the university health insurance requirement and fees. UHS also has a registered dietician and health promotion specialists on staff to assist students in addressing nutrition, drug and alcohol and other healthy lifestyle issues. In addition to providing direct health care, UHS administers the Student Health Insurance Program. The Student Health Insurance plan offers a high level of coverage in a wide network of health care providers and hospitals. Appointments can be made by visiting UHS\\u2019s website, walk-in, or by telephone, 412-268-2157. 28 Campus Wellness https://www.cmu.edu/wellness/ At Carnegie Mellon, we believe our individual and collective well-being is rooted in healthy connections to each other and to campus resources. The university provides a wide variety of wellness, mindfulness and connectedness initiatives and resources designed to help students thrive inside and outside the classroom. 29 Religious and Spiritual Life Initiatives (RSLI) https://www.cmu.edu/wellbeing/resources/religious-spiritual/index.html Carnegie Mellon is committed to the holistic growth of our students, including creating opportunities for spiritual and religious practice and exploration. RSLI has relationships with local houses of worship from various traditions and many of these groups are members of CMU\\u2019s Council of Religious Advisors. They also offer programs and initiatives that cross traditional religious boundaries in order to increase knowledge of and appreciation for the full diversity of the worldview traditions. RSLI staff are available to support students across the spectrum of religious and spiritual practice and would be more than happy to help you make a connection into a community of faith during your time at CMU. 30 University Police https://www.cmu.edu/police/ x2323 The University Police Department is located at 4551 Filmore Street . The department\\u2019s services include police patrols and call response, criminal investigations, fixed officer and foot officer patrols, event security, and crime prevention and education programming as well as bicycle and laptop registration. Visit the department\\u2019s website for additional information about the staff, emergency phone locations, crime prevention, lost and found, fingerprint services, and annual statistic reports. Carnegie Mellon University publishes an annual campus security and fire safety report describing the university\\u2019s security, alcohol and drug, sexual assault, and fire safety policies. The report also contains statistics about the number and type of crimes committed on the campus and the number and cause of fires in campus residence facilities during the preceding three years. Graduate students can obtain a copy by contacting the University Police Department at x2323. The annual security and fire safety report is also available online at: https://www.cmu.edu/police/annualreports/ 31 Shuttle and Escort Services https://www.cmu.edu/parking/transport/ Parking and Transportation coordinates the Shuttle Service and Escort Service provided for CMU students, faculty, and community. The Shuttle & Escort website has full information about these services, stops, routes, tracking and schedules. 32 The WORD https://www.cmu.edu/student-affairs/theword/ The WORD is Carnegie Mellon University\\u2019s online student handbook and serves as the foundation for the department (and sometimes college) handbook. The WORD contains university-wide academic policy information and resources, community policies and resources, and describes the university level procedures used to review possible violations of these standards. It is designed to provide all students with the tools, guidance, and insights to help you achieve your full potential as a member of the Carnegie Mellon community. Graduate students are encouraged to bookmark this site and refer to it often. University policies can also be found in full text at: https://www.cmu.edu/policies/. \",\n          \"Author (LTI's Professor): Yonantan Bisk; Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs; Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang; Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'} Author (LTI's Professor): Yonantan Bisk; Title: HomeRobot: Open-Vocabulary Mobile Manipulation; Authors: Sriram Yenamandra, A. Ramachandran, Karmesh Yadav, Austin S. Wang, Mukul Khanna, Th\\u00e9ophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander Clegg, John Turner, Z. Kira, M. Savva, Angel X. Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton; Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.; Year: 2023; Venue: Conference on Robot Learning; Citations: 19; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The HomeRobot OVMM benchmark is introduced, where an agent navigates household environments to grasp novel objects and place them on target receptacles, and baselines achieve a 20% success rate in the real world; the experiments identify ways future research work improve performance.'} Author (LTI's Professor): Yonantan Bisk; Title: EXCALIBUR: Encouraging and Evaluating Embodied Exploration; Authors: Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Weihs; Abstract: Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: \\u201cis the small heavy red bowl made from glass?\\u201d or \\u201cis there a silver spoon heavier than the egg?\\u201d. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures and represents the next frontier for embodied AI research.'} Author (LTI's Professor): Yonantan Bisk; Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents; Authors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye; Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.; Year: 2023; Venue: arXiv.org; Citations: 16; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A framework to use the knowledge in LLMs to simplify the control problem, rather than solving it is proposed, which leads to a significant 15% improvement over SOTA for generalization to human goal specifications.'} Author (LTI's Professor): Yonantan Bisk; Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models; Authors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\\u00e3o Silv\\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, L. Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui; Abstract: \\u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Yonantan Bisk; Title: Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis; Authors: Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Varma Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao, Yu Quan Chong, Chen Wang, Katia P. Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Z. Kira, Fei Xia, Yonatan Bisk; Abstract: Building general-purpose robots that can operate seamlessly, in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. Unfortunately, however, most existing robotic systems have been constrained - having been designed for specific tasks, trained on specific datasets, and deployed within specific environments. These systems usually require extensively-labeled data, rely on task-specific models, have numerous generalization issues when deployed in real-world scenarios, and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing an overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository of resources, including papers reviewed in this survey as well as related projects and repositories for developing foundation models for robotics.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable is provided and a taxonomy is established to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics.'} Author (LTI's Professor): Yonantan Bisk; Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models; Authors: Open X-Embodiment Collaboration, A. Padalkar, Acorn Pooley, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Animesh Garg, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, B. Sch\\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Joao Silv'erio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, L. Chen, Lerrel Pinto, Fei-Fei Li, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Max Spero, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart'in-Mart'in, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui; Abstract: Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website $\\\\href{https://robotics-transformer-x.github.io}{\\\\text{robotics-transformer-x.github.io}}$.; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper provides datasets in standardized data formats and models to make it possible to explore the possibility of generalist X-robot policy in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies.'} Author (LTI's Professor): Yonantan Bisk; Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception; Authors: Gyan Tatiya, Jonathan M Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov; Abstract: A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multimodal Object property learning with Self-Attention and Interactive Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from multimodal foundation models and aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems.'} Author (LTI's Professor): Yonantan Bisk; Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation; Authors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk; Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A new task OUTDOOR is introduced, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain are introduced.'} Author (LTI's Professor): Yonantan Bisk; Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models; Authors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\\u00e3o Silv\\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, L. Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui; Abstract: \\u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Yonantan Bisk; Title: SLAP: Spatial-Language Attention Policies; Authors: Priyam Parashar, Vidhi Jain, Xiaohan Zhang, Jay Vakil, Sam Powers, Yonatan Bisk, Chris Paxton; Abstract: Despite great strides in language-guided manipulation, existing work has been constrained to table-top settings. Table-tops allow for perfect and consistent camera angles, properties are that do not hold in mobile manipulation. Task plans that involve moving around the environment must be robust to egocentric views and changes in the plane and angle of grasp. A further challenge is ensuring this is all true while still being able to learn skills efficiently from limited data. We propose Spatial-Language Attention Policies (SLAP) as a solution. SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows an 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations). We see a 4x improvement over baseline in mobile manipulation setting. In addition, we show how SLAPs robustness allows us to execute Task Plans from open-vocabulary instructions using a large language model for multi-step mobile manipulation. For videos, see the website: https://robotslap.github.io; Year: 2023; Venue: Conference on Robot Learning; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The proposed Spatial-Language Attention Policies (SLAP) uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy, which shows an 80% success rate in the real world across eight tasks with a single model, and a 4x improvement over baseline in mobile manipulation setting.'} Author (LTI's Professor): Yonantan Bisk; Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment; Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell; Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\\\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'} Author (LTI's Professor): Yonantan Bisk; Title: SPRING: Studying the Paper and Reasoning to Play Games; Authors: Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Tom M. Mitchell, Yuan-Fang Li; Abstract: Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context\\\"reasoning\\\"induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.; Year: 2023; Venue: ; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories, and show the potential of games as a test bed for LLMs.'} Author (LTI's Professor): Yonantan Bisk; Title: WebArena: A Realistic Web Environment for Building Autonomous Agents; Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig; Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.; Year: 2023; Venue: arXiv.org; Citations: 73; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'} Author (LTI's Professor): Yonantan Bisk; Title: Computational Language Acquisition with Theory of Mind; Authors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig; Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.; Year: 2023; Venue: International Conference on Learning Representations; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"It is found that training speakers with a highly weighted ToM listener component leads to performance gains in the authors' image referential game setting, and suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.\\\"} Author (LTI's Professor): Yonantan Bisk; Title: SPRING: Studying Papers and Reasoning to play Games; Authors: Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Russ Salakhutdinov, A. Azaria, Tom M. Mitchell, Yuanzhi Li; Abstract: None; Year: 2023; Venue: Neural Information Processing Systems; Citations: 2; TLDR: None Author (LTI's Professor): Yonantan Bisk; Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models; Authors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\\u00e3o Silv\\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, K. Majd, Krishan Rana, K. Srinivasan, L. Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, T. Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui; Abstract: \\u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models,; Year: 2023; Venue: arXiv.org; Citations: 49; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A consolidation of pretrained models in domains from NLP to Computer Vision, where large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications.'} Author (LTI's Professor): Yonantan Bisk; Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents; Authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap; Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.; Year: 2023; Venue: arXiv.org; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': 'SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence, is presented and it is found that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.'} Author (LTI's Professor): Jamie Callan; Title: Conversational Search with Random Walks over Entity Graphs; Authors: Gustavo Gon\\u00e7alves, Jo\\u00e3o Magalh\\u00e3es, Jamie Callan; Abstract: The entities that emerge during a conversation can be used to model topics, but not all entities are equally useful for this task. Modeling the conversation with entity graphs and predicting each entity's centrality in the conversation provides additional information that improves the retrieval of answer passages for the current question. Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.; Year: 2023; Venue: International Conference on the Theory of Information Retrieval; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.'} Author (LTI's Professor): Jamie Callan; Title: KALE: Using a K-Sparse Projector for Lexical Expansion; Authors: Lu\\u00eds Borges, Bruno Martins, Jamie Callan; Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.; Year: 2023; Venue: International Conference on the Theory of Information Retrieval; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'KALE is a new lightweight method that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary, which can replace the original lexical vocabulary with gains in accuracy and efficiency.'} Author (LTI's Professor): Jamie Callan; Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms; Authors: Zhen Fan, Luyu Gao, Jamie Callan; Abstract: Lexical exact-match systems perform text retrieval efficiently with sparse matching signals and fast retrieval through inverted lists, but naturally suffer from the mismatch between lexical surface form and implicit term semantics. This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF). Each CSF pairs a lexical surface form with a context source, and is represented by a lexical form weight and a contextualized semantic vector representation. This framework is able to perform sparse lexicon-based retrieval by learning to represent each query and document as a \\\"bag-of-CSFs\\\", simultaneously addressing two key factors in sparse retrieval: vocabulary expansion of surface form and semantic representation of term meaning. At retrieval time, it efficiently matches CSFs through exact-match of learned surface forms, and effectively scores each CSF pair via contextual semantic representations, leading to joint improvement in both term match and term scoring. Multiple experiments show that this approach successfully resolves the main mismatch issues in lexical exact-match retrieval and outperforms state-of-the-art lexical exact-match systems, reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact-match-based system.; Year: 2023; Venue: International Conference on the Theory of Information Retrieval; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF), reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact- match-based system.'} Author (LTI's Professor): Jamie Callan; Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval; Authors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan; Abstract: None; Year: 2023; Venue: European Conference on Information Retrieval; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Jamie Callan; Title: Active Retrieval Augmented Generation; Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig; Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 50; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'} Author (LTI's Professor): Jamie Callan; Title: Multi-Objective Improvement of Android Applications; Authors: Jamie Callan, J. Petke; Abstract: Non-functional properties, such as runtime or memory use, are important to mobile app users and developers, as they affect user experience. Previous work on automated improvement of non-functional properties in mobile apps failed to address the inherent trade-offs between such properties. We propose a practical approach and the first open-source tool, GIDroid (2023), for multi-objective automated improvement of Android apps. In particular, we use Genetic improvement, a search-based technique that navigates the space of software variants to find improved software. We use a simulation-based testing framework to greatly improve the speed of search. GIDroid contains three state-of-the-art multi-objective algorithms, and two new mutation operators, which cache the results of method calls. Genetic improvement relies on testing to validate patches. Previous work showed that tests in open-source Android applications are scarce. We thus wrote tests for 21 versions of 7 Android apps, creating a new benchmark for performance improvements. We used GIDroid to improve versions of mobile apps where developers had previously found improvements to runtime, memory, and bandwidth use. Our technique automatically re-discovers 64% of existing improvements. We then applied our approach to current versions of software in which there were no known improvements. We were able to improve execution time by up to 35%, and memory use by up to 33% in these apps.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a practical approach and the first open-source tool, GIDroid, for multi-objective automated improvement of Android apps, and uses Genetic improvement, a search-based technique that navigates the space of software variants to find improved software.'} Author (LTI's Professor): Justine Cassell; Title: When to generate hedges in peer-tutoring interactions; Authors: Alafate Abulimiti, C. Clavel, Justine Cassell; Abstract: This paper explores the application of machine learning techniques to predict where hedging occurs in peer-tutoring interactions. The study uses a naturalistic face-to-face dataset annotated for natural language turns, conversational strategies, tutoring strategies, and nonverbal behaviors. These elements are processed into a vector representation of the previous turns, which serves as input to several machine learning models, including MLP and LSTM. The results show that embedding layers, capturing the semantic information of the previous turns, significantly improves the model\\u2019s performance. Additionally, the study provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviors, in predicting hedges by using Shapley values for feature explanation. We discover that the eye gaze of both the tutor and the tutee has a significant impact on hedge prediction. We further validate this observation through a follow-up ablation study.; Year: 2023; Venue: SIGDIAL Conferences; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The results show that embedding layers, capturing the semantic information of the previous turns, significantly improves the model\\u2019s performance and provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviors, in predicting hedges by using Shapley values for feature explanation.'} Author (LTI's Professor): Justine Cassell; Title: How About Kind of Generating Hedges using End-to-End Neural Models?; Authors: Alafate Abulimiti, C. Clavel, Justine Cassell; Abstract: Hedging is a strategy for softening the impact of a statement in conversation. In reducing the strength of an expression, it may help to avoid embarrassment (more technically, \\u201cface threat\\u201d) to one\\u2019s listener. For this reason, it is often found in contexts of instruction, such as tutoring. In this work, we develop a model of hedge generation based on i) fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by ii) reranking to select the candidate that best matches the expected hedging strategy within a candidate pool using a hedge classifier. We apply this method to a natural peer-tutoring corpus containing a significant number of disfluencies, repetitions, and repairs. The results show that generation in this noisy environment is feasible with reranking. By conducting an error analysis for both approaches, we reveal the challenges faced by systems attempting to accomplish both social and task-oriented goals in conversation.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work develops a model of hedge generation based on fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by reranking to select the candidate that best matches the expected hedging strategy within a candidate pool using a hedge classifier.'} Author (LTI's Professor): Justine Cassell; Title: Beyond Single-Mindedness: A Figure-Ground Reversal for the Cognitive Sciences; Authors: Mark Dingemanse, Andreas Liesenfeld, Marlou Rasenberg, Saul Albert, F. Ameka, Abeba Birhane, Dimitris Bolis, Justine Cassell, Rebecca Clift, E. Cuffari, H. Jaegher, C. Novaes, N. Enfield, Riccardo Fusaroli, E. Gregoromichelaki, E. Hutchins, Ivana Konvalinka, D. Milton, J. R\\u0105czaszek-Leonardi, V. Reddy, F. Rossano, David Schlangen, J. Seibt, E. Stokoe, L. Suchman, C. Vesper, T. Wheatley, Martina Wiltschko; Abstract: A fundamental fact about human minds is that they are never truly alone: all minds are steeped in situated interaction. That social interaction matters is recognized by any experimentalist who seeks to exclude its influence by studying individuals in isolation. On this view, interaction complicates cognition. Here, we explore the more radical stance that interaction co-constitutes cognition: that we benefit from looking beyond single minds toward cognition as a process involving interacting minds. All around the cognitive sciences, there are approaches that put interaction center stage. Their diverse and pluralistic origins may obscure the fact that collectively, they harbor insights and methods that can respecify foundational assumptions and fuel novel interdisciplinary work. What might the cognitive sciences gain from stronger interactional foundations? This represents, we believe, one of the key questions for the future. Writing as a transdisciplinary collective assembled from across the classic cognitive science hexagon and beyond, we highlight the opportunity for a figure-ground reversal that puts interaction at the heart of cognition. The interactive stance is a way of seeing that deserves to be a key part of the conceptual toolkit of cognitive scientists.; Year: 2023; Venue: Cognitive Sciences; Citations: 18; TLDR: None Author (LTI's Professor): Justine Cassell; Title: \\\"You might think about slightly revising the title\\u201d: Identifying Hedges in Peer-tutoring Interactions; Authors: Yann Raphalen, C. Clavel, Justine Cassell; Abstract: Hedges have an important role in the management of rapport. In peer-tutoring, they are notably used by tutors in dyads experiencing low rapport to tone down the impact of instructions and negative feedback.Pursuing the objective of building a tutoring agent that manages rapport with teenagers in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of a such a hybrid model approach.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A model explainability tool is employed to explore the features that characterize hedges in peer-tutoring conversations, and some novel features, and the benefits of a such a hybrid model approach are identified.'} Author (LTI's Professor): Mona Diab; Title: Author Correction: Arabic natural language processing for Qur\\u2019anic research: a systematic review; Authors: M. Bashir, Aqil M. Azmi, H. Nawaz, W. Zaghouani, Mona T. Diab, Ala I. Al-Fuqaha, Junaid Qadir; Abstract: None; Year: 2023; Venue: Artificial Intelligence Review; Citations: 0; TLDR: None Author (LTI's Professor): Mona Diab; Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology; Authors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues; Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.; Year: 2023; Venue: International Workshop on Spoken Language Translation; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.'} Author (LTI's Professor): Fernando Diaz; Title: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision; Authors: Fernando Diaz; Abstract: Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work addresses the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements.'} Author (LTI's Professor): Fernando Diaz; Title: Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery; Authors: Rebecca Salganik, Fernando Diaz, G. Farnadi; Abstract: As online music platforms grow, music recommender systems play a vital role in helping users navigate and discover content within their vast musical databases. At odds with this larger goal, is the presence of popularity bias, which causes algorithmic systems to favor mainstream content over, potentially more relevant, but niche items. In this work we explore the intrinsic relationship between music discovery and popularity bias. To mitigate this issue we propose a domain-aware, individual fairness-based approach which addresses popularity bias in graph neural network (GNNs) based recommender systems. Our approach uses individual fairness to reflect a ground truth listening experience, i.e., if two songs sound similar, this similarity should be reflected in their representations. In doing so, we facilitate meaningful music discovery that is robust to popularity bias and grounded in the music domain. We apply our BOOST methodology to two discovery based tasks, performing recommendations at both the playlist level and user level. Then, we ground our evaluation in the cold start setting, showing that our approach outperforms existing fairness benchmarks in both performance and recommendation of lesser-known content. Finally, our analysis explains why our proposed methodology is a novel and promising approach to mitigating popularity bias and improving the discovery of new and niche content in music recommender systems.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a domain-aware, individual fairness-based approach which addresses popularity bias in graph neural network (GNNs) based recommender systems and applies the BOOST methodology to two discovery based tasks, performing recommendations at both the playlist level and user level.'} Author (LTI's Professor): Fernando Diaz; Title: Overview of the TREC 2021 Fair Ranking Track; Authors: Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, Sebastian Kohlmeier; Abstract: The TREC Fair Ranking Track aims to provide a platform for participants to develop and evaluate novel retrieval algorithms that can provide a fair exposure to a mixture of demographics or attributes, such as ethnicity, that are represented by relevant documents in response to a search query. For example, particular demographics or attributes can be represented by the documents' topical content or authors. The 2021 Fair Ranking Track adopted a resource allocation task. The task focused on supporting Wikipedia editors who are looking to improve the encyclopedia's coverage of topics under the purview of a WikiProject. WikiProject coordinators and/or Wikipedia editors search for Wikipedia documents that are in need of editing to improve the quality of the article. The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia. The under-representation of particular protected characteristics in Wikipedia can result in systematic biases that can have a negative human, social, and economic impact, particularly for disadvantaged or protected societal groups.; Year: 2023; Venue: Text Retrieval Conference; Citations: 23; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia.'} Author (LTI's Professor): Fernando Diaz; Title: Recall, Robustness, and Lexicographic Evaluation; Authors: Fernando Diaz, Bhaskar Mitra; Abstract: Although originally developed to evaluate sets of items, recall is often used to evaluate rankings of items, including those produced by recommender, retrieval, and other machine learning systems. The application of recall without a formal evaluative motivation has led to criticism of recall as a vague or inappropriate measure. In light of this debate, we reflect on the measurement of recall in rankings from a formal perspective. Our analysis is composed of three tenets: recall, robustness, and lexicographic evaluation. First, we formally define `recall-orientation' as the sensitivity of a metric to a user interested in finding every relevant item. Second, we analyze recall-orientation from the perspective of robustness with respect to possible content consumers and providers, connecting recall to recent conversations about fair ranking. Finally, we extend this conceptual and theoretical treatment of recall by developing a practical preference-based evaluation method based on lexicographic comparison. Through extensive empirical analysis across three recommendation tasks and 17 information retrieval tasks, we establish that our new evaluation method, lexirecall, has convergent validity (i.e., it is correlated with existing recall metrics) and exhibits substantially higher sensitivity in terms of discriminative power and stability in the presence of missing labels. Our conceptual, theoretical, and empirical analysis substantially deepens our understanding of recall and motivates its adoption through connections to robustness and fairness.; Year: 2023; Venue: ; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Through extensive empirical analysis across three recommendation tasks and 17 information retrieval tasks, it is established that the new evaluation method, lexirecall, has convergent validity and exhibits substantially higher sensitivity in terms of discriminative power and stability in the presence of missing labels.'} Author (LTI's Professor): Scott Fahlman; Title: Score: A Rule Engine for the Scone Knowledge Base System; Authors: Jeffrey Chen, S. Fahlman; Abstract: We present Score, a rule engine designed and implemented for the Scone knowledge base system. Scone is a knowledge base system designed for storing and manipulating rich representations of general knowledge in symbolic form. It represents knowledge in the form of nodes and links in a network structure, and it can perform basic inference about the relationships between different elements efficiently. On its own, Scone acts as a sort of\\\"smart memory\\\"that can interface with other software systems. One area of improvement for Scone is how useful it can be in supplying knowledge to an intelligent agent that can use the knowledge to perform actions and update the knowledge base with its observations. We augment the Scone system with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone's knowledge base, potentially improving the capabilities of any planning systems built on top of Scone. Production rule systems consist of\\\"if-then\\\"production rules that try to match their predicates to existing knowledge and fire their actions when their predicates are satisfied. We propose two kinds of production rules, if-added and if-needed rules, that differ in how they are checked and fired to cover multiple use cases. We then implement methods to efficiently check and fire these rules in a large knowledge base. The new rule engine is not meant to be a complex stand-alone planner, so we discuss how it fits into the context of Scone and future work on planning systems.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"The Scone system is augmented with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone's knowledge base, potentially improving the capabilities of any planning systems built on top of Scone.\\\"} Author (LTI's Professor): Daniel Fried; Title: Pragmatic Inference with a CLIP Listener for Contrastive Captioning; Authors: Jiefu Ou, Benno Krojer, Daniel Fried; Abstract: We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off-the-shelf CLIP model to parameterize the listener. Compared with captioner-only pragmatic models, our method benefits from rich vision language alignment representations from CLIP when reasoning over distractors. Like previous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to discriminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which allows us to automatically optimize the captions for informativity - outperforming past methods for discriminative captioning by 11% to 15% accuracy in human evaluations; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images by leveraging an off-the-shelf CLIP model to parameterize the listener.'} Author (LTI's Professor): Daniel Fried; Title: SantaCoder: don't reach for the stars!; Authors: Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Mu\\u00f1oz Ferrandis, Niklas Muennighoff, Mayank Mishra, A. Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, J. Poirier, Hailey Schoelkopf, S. Troshin, Dmitry Abulkhanov, M. Romero, M. Lappert, F. Toni, Bernardo Garc'ia del R'io, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, I. Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, D. Lansky, Huu Nguyen, Danish Contractor, Luisa Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, S. Hughes, Daniel Fried, Arjun Guha, H. D. Vries, Leandro von Werra; Abstract: The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.; Year: 2023; Venue: arXiv.org; Citations: 85; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The current state of the Personally Identifiable Information (PII) redaction pipeline is outlined, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data are outlined.'} Author (LTI's Professor): Daniel Fried; Title: Grounding Language Models to Images for Multimodal Generation; Authors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried; Abstract: We propose an ef\\ufb01cient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and \\ufb01netune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.; Year: 2023; Venue: arXiv.org; Citations: 61; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Daniel Fried; Title: StarCoder: may the source be with you!; Authors: Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, J. Lamy-Poirier, Jo\\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, J. Stillerman, S. Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, N. Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, M. Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, A. Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\\u00f1oz Ferrandis, Sean M. Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, H. D. Vries; Abstract: The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.; Year: 2023; Venue: arXiv.org; Citations: 231; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work performs the most comprehensive evaluation of Code LLMs to date and shows that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model.'} Author (LTI's Professor): Daniel Fried; Title: Grounding Language Models to Images for Multimodal Inputs and Outputs; Authors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried; Abstract: We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.; Year: 2023; Venue: International Conference on Machine Learning; Citations: 33; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Daniel Fried; Title: Generating Images with Multimodal Language Models; Authors: Jing Yu Koh, Daniel Fried, R. Salakhutdinov; Abstract: We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 69; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces, and exhibits a wider range of capabilities compared to prior multimodal language models.'} Author (LTI's Professor): Daniel Fried; Title: WebArena: A Realistic Web Environment for Building Autonomous Agents; Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig; Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.; Year: 2023; Venue: arXiv.org; Citations: 73; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'} Author (LTI's Professor): Alexander Hauptmann; Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data; Authors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann; Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.; Year: 2023; Venue: Defense + Commercial Sensing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition and shows that this model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.'} Author (LTI's Professor): Alexander Hauptmann; Title: Towards Open-Domain Twitter User Profile Inference; Authors: Haoyang Wen, Zhenxin Xiao, E. Hovy, Alexander Hauptmann; Abstract: ,; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Daphne Ippolito; Title: The State of Intent Detection in the Era of Large Autoregressive Language Models; Authors: Tom B. Brown, Benjamin Mann, Nick Ryder, Jared D Subbiah, Prafulla Kaplan, A. Dhariwal, P. Neelakantan, Girish Shyam, Amanda Sastry, Sandhini Askell, Ariel Agarwal, Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott teusz Litwin, Benjamin Gray, Jack Chess, Christopher Clark, Sam Berner, Alec McCandlish, Ilya Radford, Sutskever Dario, Amodei, Matthew Henderson, Ivan Vulic. 2020, Ef-310, Aakanksha Chowdhery, Sharan Narang, J. Devlin, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek, Parker Rao, Yi Barnes, Noam Tay, Vin-316 Shazeer, Emily odkumar Prabhakaran, Nan Reif, Ben Du, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, H. Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, L. Fedus, Denny Zhou, Daphne Ippolito, D. Luan, Hyeontaek Lim, Barret Zoph, A. Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Oleksandr Polozov, K. Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta; Abstract: In-context learning (ICL) using large pre-001 trained autoregressive language models (LLMs, 002 e.g. GPT-3) has demonstrated effective clas-003 sification performance at a variety of natural 004 language tasks. Using LLMs for intent detec-005 tion is challenging due to the large label space 006 and limited context window, such that it is diffi-007 cult to fit a sufficient number of examples in the 008 prompt to allow the use of in-context learning. 009 In this paper, dense retrieval is used to bypass 010 this limitation, giving the model only a par-011 tial view of the full label space. We show that 012 retriever-augmented large language models are 013 an effective way to tackle intent detection, by-014 passing context window limitations effectively 015 through the retrieval mechanism. Comparing 016 the LLaMA and OPT model families at differ-017 ent scales, we set new state of the art perfor-018 mance in the few-shot setting with zero training 019 for two of the three intent classification datasets 020 that we consider, while achieving competitive 021 results on the third one. This work demon-022 strates that the Retriever+ICL framework is a 023 strong zero-training competitor to fine-tuned in-024 tent detection approaches. In addition, a small 025 study on the number of examples provided at 026 different model scales is done, showing that 027 larger models are needed to make effective use 028 of more examples in-prompt. 029; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The Retriever+ICL framework is a strong zero-training competitor to fine-tuned in-024 tent detection approaches, and is set new state of the art in the few-shot setting with zero training.'} Author (LTI's Professor): Daphne Ippolito; Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System; Authors: Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, Yun William Yu; Abstract: Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model\\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).; Year: 2023; Venue: International Conference on Natural Language Generation; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling) are presented, which has implications for detecting generated text.'} Author (LTI's Professor): Daphne Ippolito; Title: A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity; Authors: S. Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David M. Mimno, Daphne Ippolito; Abstract: Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.; Year: 2023; Venue: arXiv.org; Citations: 35; TLDR: {'model': 'tldr@v2.0.0', 'text': 'These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which are hoped to help support more informed data-centric decisions in LM development.'} Author (LTI's Professor): Daphne Ippolito; Title: This paper is included in the Proceedings of the 32nd USENIX Security; Authors: \\u2217. NicholasCarlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, F. Tram\\u00e8r, Borja Balle, Daphne Ippolito, Eric Wallace, Google, DeepMind, Princeton, Uc Berkeley; Abstract: Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted signi\\ufb01cant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-\\ufb01lter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that diffusion models memorize individual images from their training data and emit them at generation time, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.'} Author (LTI's Professor): Daphne Ippolito; Title: Extracting Training Data from Diffusion Models; Authors: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tram\\u00e8r, B. Balle, Daphne Ippolito, Eric Wallace; Abstract: Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.; Year: 2023; Venue: USENIX Security Symposium; Citations: 231; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.'} Author (LTI's Professor): Daphne Ippolito; Title: Report of the 1st Workshop on Generative AI and Law; Authors: A. F. Cooper, Katherine Lee, James Grimmelmann, Daphne Ippolito, Christopher Callison-Burch, Christopher A. Choquette-Choo, Niloofar Mireshghallah, Miles Brundage, David Mimno, M. Choksi, J. Balkin, Nicholas Carlini, Christopher De Sa, Jonathan Frankle, Deep Ganguli, Bryant Gipson, A. Guadamuz, Swee Leng Harris, Abigail Z. Jacobs, Elizabeth Joh, Gautam Kamath, M. Lemley, Cass Matthews, C. McLeavey, Corynne Mcsherry, Milad Nasr, Paul Ohm, Adam Roberts, Tom Rubin, Pamela Samuelson, Ludwig Schubert, Kristen Vaccaro, Luis Villa, Felix Wu, Elana Zeide; Abstract: This report presents the takeaways of the inaugural Workshop on Generative AI and Law (GenLaw), held in July 2023. A cross-disciplinary group of practitioners and scholars from computer science and law convened to discuss the technical, doctrinal, and policy challenges presented by law for Generative AI, and by Generative AI for law, with an emphasis on U.S. law in particular. We begin the report with a high-level statement about why Generative AI is both immensely significant and immensely challenging for law. To meet these challenges, we conclude that there is an essential need for 1) a shared knowledge base that provides a common conceptual language for experts across disciplines; 2) clarification of the distinctive technical capabilities of generative-AI systems, as compared and contrasted to other computer and AI systems; 3) a logical taxonomy of the legal issues these systems raise; and, 4) a concrete research agenda to promote collaboration and knowledge-sharing on emerging issues at the intersection of Generative AI and law. In this report, we synthesize the key takeaways from the GenLaw workshop that begin to address these needs. All of the listed authors contributed to the workshop upon which this report is based, but they and their organizations do not necessarily endorse all of the specific claims in this report.; Year: 2023; Venue: Social Science Research Network; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'There is an essential need for a shared knowledge base that provides a common conceptual language for experts across disciplines to meet the challenges presented by law for Generative AI, and by GenerativeAI for law, with an emphasis on U.S. law in particular.'} Author (LTI's Professor): Daphne Ippolito; Title: What are Adapters Really Efficient At?; Authors: Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Mitesh M. Khapra, Pratyush Kumar, V. Rudra, Murthy Anoop, Kunchukuttan. 2022, Naama-677, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir R. Radev, Al-683 ham, Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Jonas Pfeiffer, Aishwarya Kamath, Andreas R\\u00fcckl\\u00e9, Kyunghyun Cho, Iryna Gurevych, Clifton Poth, Aishwarya, Ivan Kamath, Sebastian Vuli\\u00b4c, Kyunghyun Ruder, Gregor Geigle, Max Glockner, Jonas Beck, Nils Pfeiffer, Reimers Iryna, Victor Sanh, Colin Raffel, Lintang Bach, Zaid Sutawika, Antoine Alyafeai, Arnaud Chaffin, Arun Stiegler, Manan Raja, Dey Saiful, Canwen Bari, Urmish Xu, Thakker, Shanya Sharma, Eliza Szczechla, Taewoon, Gunjan Kim, Nihal Chhablani, Nayak, Debajyoti, Jonathan Datta, Mike Tian-Jian Chang, Han Jiang, Matteo Wang, S. Manica, Zheng Xin Shen, Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Tripathi Neeraj, Jos Rozen, Abheesht Sharma, A. Santilli, Thibault F\\u00e9vry, Jason Alan Fries, Maarten Sap, Hannah Rashkin, Derek Chen, Ronan, Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Adam R. Brown, Adam Santoro, Adri\\u00e0 Gupta, Agnieszka Garriga-Alonso, Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par-765, Allen Nie, Aman Hussain, Amanda Askell, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, A. Santilli, Andreas Stuhlm\\u00fcller, Andrew M. Dai, Andrew La, Andrew Lampinen, Angela Zou, Angelica Jiang, Anh Chen, Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-773, Arul Menezes, Arun Kirubarajan, Asher Mul-774, Ashish lokandov, Austin Sabharwal, Herrick, Avia, A. Efrat, Ayla Erdem, B. Karaka\\u00b8s, Ryan Roberts, B. S. Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan \\u00d6zyurt, Behnam Hedayatnia, Behnam, Benjamin Neyshabur, Benno Inden, Berk Stein, Ek-779 mekci, Bill Yuchen, Blake Lin, Cameron Howald, Cameron Diao, Catherine Dour, Cedrick Stinson, Ar-781 C\\u00e9sar, Chandan Ferri Ram\\u00edrez, Charles Singh, Christopher D. Manning, Christopher Potts, Cindy 785 Ramirez, Clara Rivera, Clemencia Siro, Colin Raf-786, Courtney Ashcraft, Cristina Garbacea, Dan Sileo, Daniel H Garrette, Dan Hendrycks, Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\\u00ed Gonz\\u00e1lez, Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, D. Gilboa, David Dohan, D. Drakard, David Ju-792, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Tam, m\\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-807 L\\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijase-808 vic, Hannah Kim, Harsh Mehta, H. Bogar, Henry Shevlin, Hinrich Sch\\u00fctze, H. Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jae-813 hoon Lee, J. Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Ko-815 co\\u00b4n, Jana Thompson, Jared Kaplan, Jarema Radom, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, K. Markert, Kaustubh D. Dhole, Kevin Gim-827 pel, Kevin Omondi, K. Mathewson, Kristen Chi-828 afullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-829 Donell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras Ochando, Louis-Philippe Morency, Luca Moschella, Maarten \\u00b8Senel, Maarten Bosma, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Jose Maru, Marie Ram\\u00edrez Quintana, Tolkiehn Mario, Martha Giulianelli, Martin Lewis, L. PotthastMatthew, Matthew L. Leavitt, M\\u00e1ty\\u00e1s Schu-840 bert Hagen, Medina Orduna, Melody Baitemirova, Arnaud Melvin, Michael A McElrath, Michael Yee, Michael Co-842 hen, Michael Gu, M. Ivanitskiy, Michael Star-843 ritt, M. Strube, Michele Sw\\u02dbedrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mo Suzgun, Monica Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T. Varma, Nanyun Peng, tish Shirish Keskar, Niveditha Iyer, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio, Moreno Casares, Parth Doshi, Jason Wei, Maarten Bosma, Vincent Y. Zhao, Adams Wei Guu, Brian Yu, Nan Lester, An-921 Du, M. Dai, Quoc V. Le, Finetuned, Adina Williams, Nikita Nangia, Samuel R. Bowman, Thomas Wolf, Lysandre Debut, Clement Chaumond, Anthony Delangue, Pier-339 Moi, Tim ric Cistac, R\\u00e9mi Rault, Morgan Louf, Funtow-900 Joe, Sam Davison, Patrick Shleifer, von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Sylvain Gugger, Mariama Drame, Yinfei Yang, Yuan Zhang, Chris Tar, Hailey Schoelkopf, Niklas Muen-954, Alham Fikri, David Ifeoluwa Adelani, M Saiful Bari, Lintang Sutawika, Zhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo, Jonathan May; Abstract: Adapters have been positioned as a parameter-001 efficient fine-tuning (PEFT) approach, whereby 002 a minimal number of parameters are added to 003 the model and fine-tuned. However, adapters 004 have not been sufficiently analyzed to under-005 stand if PEFT translates to benefits in train-006 ing/deployment efficiency and maintainabil-007 ity/extensibility. Through extensive experi-008 ments on many adapters, tasks, and languages 009 in supervised and cross-lingual zero-shot set-010 tings, we clearly show that for Natural Lan-011 guage Understanding tasks, the parameter ef-012 ficiency in adapters does not translate to effi-013 ciency gains compared to full fine-tuning of 014 models. More precisely, adapters are relatively 015 expensive to train and have slightly higher de-016 ployment latency. Furthermore, the maintain-017 ability /extensibility benefits of adapters can be 018 achieved with simpler approaches like multi-019 task training via full fine-tuning, which also 020 provide relatively faster training times. We, 021 therefore, recommend that for moderately sized 022 models practitioners should rely on full fine-023 tuning or multi-task training rather than using 024 adapters. 025; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is recommended that for moderately sized models practitioners should rely on full fine-023 tuning or multi-task training rather than using adapters, as adapters are relatively expensive to train and have slightly higher de-016 ployment latency.'} Author (LTI's Professor): Daphne Ippolito; Title: Are aligned neural networks adversarially aligned?; Authors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tram\\u00e8r, Ludwig Schmidt; Abstract: Large language models are now tuned to align with the goals of their creators, namely to be\\\"helpful and harmless.\\\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 75; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models, and conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.'} Author (LTI's Professor): Daphne Ippolito; Title: Effective Prompt Extraction from Language Models; Authors: Yiming Zhang, Daphne Ippolito; Abstract: The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction experiments on real systems such as Bing Chat and ChatGPT suggest that system prompts can be revealed by an adversary despite existing defenses in place.; Year: 2023; Venue: ; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper presents a framework for systematically measuring the effectiveness of prompt extraction attacks and determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination.'} Author (LTI's Professor): Daphne Ippolito; Title: Scalable Extraction of Training Data from (Production) Language Models; Authors: Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. F. Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tram\\u00e8r, Katherine Lee; Abstract: This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.; Year: 2023; Venue: arXiv.org; Citations: 47; TLDR: {'model': 'tldr@v2.0.0', 'text': 'In order to attack the aligned ChatGPT, a new divergence attack is developed that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly.'} Author (LTI's Professor): Lori Levin; Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient; Authors: W. Gaetz, C. Dockstader, P. Furlong, S. Amaral, A. Vossough, E. Schwartz, T. Roberts, Lori S. Levin; Abstract: None; Year: 2023; Venue: Brain Research; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Lori Levin; Title: Identifying Health-Related Quality of Life Domains after Upper Extremity Transplantation.; Authors: D. Tulsky, Pamela A. Kisala, Callie E Tyner, J. Slotkin, C. Kaufman, C. Dearth, A. Horan, S. Talbot, J. Shores, K. Azari, C. Cetrulo, G. Brandacher, C. Cooney, David E Victorson, M. Dooley, Lori S. Levin, Cdr Scott M Tintle; Abstract: None; Year: 2023; Venue: Archives of Physical Medicine and Rehabilitation; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This study identified key constructs for use in evaluation of the potentially substantial physical, medical, social, and emotional effects of UET, including physical functioning and medical complications, positive and negative emotional functioning, and social participation, relationships, and independence.'} Author (LTI's Professor): Lori Levin; Title: Assessment of quality of life after upper extremity transplantation: Framework for patient-reported outcome scale domains; Authors: Callie E Tyner, J. Slotkin, Pamela A. Kisala, Lori S. Levin, Scott M. Tintle, D. Tulsky; Abstract: Upper extremity transplantation offers the promise of restored function and regained quality of life (QOL) for individuals who have sustained hand or arm amputation. However, a major challenge for this procedure becoming an accessible treatment option for patients is the lack of standard measures to document benefits to QOL. Patient-reported outcomes (PRO) measures are well-suited for this kind of intervention, where the perspective of the patient is central to defining treatment success. To date, qualitative work with experts, clinicians, and patients has been used to identify the most important domains of QOL for PRO item development. Specifically, our group\\u2019s qualitative work has identified several domains of QOL that are unique to individuals who have received upper extremity transplants, which are distinct from topics covered by existing PRO measures. These include emotional and social aspects of upper extremity transplant, such as Expectations and Perceived Outcomes, Integration and Assimilation of Transplant, Fitting in, and Post-Surgical Challenges and Complications. The broad topic of Satisfaction with Transplant was subdivided into three subtopics: Function, Sensation, and Aesthetics. Satisfaction with Sensation was also identified as a unique domain not evaluated by existing PRO measures. This report operationalizes these eight QOL domains by presenting scoping definitions. This manuscript describes the work that has been completed for domain characterization as an early step toward developing standardized PRO measures to evaluate these important outcomes specific to upper extremity transplantation.; Year: 2023; Venue: Frontiers in Psychology; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Qualitative work with experts, clinicians, and patients has identified several domains of QOL that are unique to individuals who have received upper extremity transplants, which are distinct from topics covered by existing PRO measures.'} Author (LTI's Professor): Lori Levin; Title: What is needed to ensure long-term sustainability for the field of vascularized composite allotransplantation?; Authors: Yoshiko Toyoda, Lori S. Levin; Abstract: The field of vascularized composite allotransplantation (VCA) has demonstrated remarkable advances since its inception with some excellent long-term results in a variety of graft types. However, unlike solid organ transplantation, it has yet to become mainstream. We therefore discuss strategies on ensuring long-term sustainability by addressing continued clinical developments of VCA to improve the risk-to-benefit balance, importance of public support, improved policy and financial support, and need for a bridge to the future of transplant surgery. There has been headway on all fronts and collaboration among the VCA centers for centralization of data and incorporation of patient voices will be essential for continued progress.; Year: 2023; Venue: Current Opinion in Organ Transplantation; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Strategies on ensuring long-term sustainability by addressing continued clinical developments of VCA to improve the risk-to-benefit balance, importance of public support, improved policy and financial support, and need for a bridge to the future of transplant surgery are discussed.'} Author (LTI's Professor): Lei Li; Title: Can Language Models Understand Physical Concepts?; Authors: Lei Li, Jingjing Xu, Qingxiu Dong, Ce Zheng, Qi Liu, Lingpeng Kong, Xu Sun; Abstract: Language models~(LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is not yet clear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85\\\\% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up the parameters of LMs 134x. Our dataset is available at \\\\url{https://github.com/TobiasLee/VEC}; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A distillation method is proposed to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up the parameters of LMs 134x, and indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge.'} Author (LTI's Professor): Lei Li; Title: Silkie: Preference Distillation for Large Visual Language Models; Authors: Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong; Abstract: This paper explores preference distillation for large vision language models (LVLMs), improving their ability to generate helpful and faithful responses anchoring the visual context. We first build a vision-language feedback (VLFeedback) dataset utilizing AI annotation. Specifically, responses are generated by models sampled from 12 LVLMs, conditioned on multi-modal instructions sourced from various datasets. We adopt GPT-4V to assess the generated outputs regarding helpfulness, visual faithfulness, and ethical considerations. Furthermore, the preference supervision is distilled into Qwen-VL-Chat through the direct preference optimization (DPO) method. The resulting model Silkie, achieves 6.9% and 9.5% relative improvement on the MME benchmark regarding the perception and cognition capabilities, respectively. Silkie also demonstrates reduced hallucination by setting a new state-of-the-art score of 3.02 on the MMHal-Bench benchmark. Further analysis shows that DPO with our VLFeedback dataset mainly boosts the fine-grained perception and complex cognition abilities of LVLMs, leading to more comprehensive improvements compared to human-annotated preference datasets.; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper explores preference distillation for large vision language models (LVLMs), improving their ability to generate helpful and faithful responses anchoring the visual context, leading to more comprehensive improvements compared to human-annotated preference datasets.'} Author (LTI's Professor): Lei Li; Title: Large Language Models are not Fair Evaluators; Authors: Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui; Abstract: In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the\\\"win/tie/lose\\\"outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\\\url{https://github.com/i-Eval/FairEval} to facilitate future research.; Year: 2023; Venue: arXiv.org; Citations: 165; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a calibration framework with three simple yet effective strategies that successfully mitigates evaluation bias, resulting in closer alignment with human judgments.'} Author (LTI's Professor): Lei Li; Title: M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning; Authors: Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, Qi Liu; Abstract: Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese. We have open-sourced the dataset to encourage further research.; Year: 2023; Venue: arXiv.org; Citations: 47; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Ying-VLM, a VLM model trained on the M$^3$IT dataset, is developed, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese.'} Author (LTI's Professor): Lei Li; Title: Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning; Authors: Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun; Abstract: In-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mechanism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow computation layers' processing; (2) the consolidated information in label words serves as a reference for LLMs' final predictions. Based on these insights, we introduce an anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL. The promising applications of our findings again validate the uncovered ICL working mechanism and pave the way for future studies.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 35; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL are introduced.'} Author (LTI's Professor): Lei Li; Title: Communication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter; Authors: Yi Liu, Xiaohan Bi, Lei Li, Sishuo Chen, Wenkai Yang, Xu Sun; Abstract: Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as a promising paradigm for institutions with limited language resources. This approach allows multiple institutions to act as clients and train a unified model through model synchronization, rather than collecting sensitive data for centralized training. This significantly reduces the cost of corpus collection and preserves data privacy. However, as pre-trained language models (PLMs) continue to increase in size, the communication cost for transmitting parameters during synchronization has become a training speed bottleneck. In this paper, we propose a communication-efficient Fed-MNMT framework that addresses this issue by keeping PLMs frozen and only transferring lightweight adapter modules between clients. Since different language pairs exhibit substantial discrepancies in data distributions, adapter parameters of clients may conflict with each other. To tackle this, we explore various clustering strategies to group parameters for integration and mitigate the negative effects of conflicting parameters. Experimental results demonstrate that our framework reduces communication cost by over 98% while achieving similar or even better performance compared to competitive baselines. Further analysis reveals that clustering strategies effectively solve the problem of linguistic discrepancy and pruning adapter modules further improves communication efficiency.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a communication-efficient Fed-MNMT framework that addresses the problem of linguistic discrepancy by keeping PLMs frozen and only transferring lightweight adapter modules between clients, and explores various clustering strategies to group parameters for integration and mitigate the negative effects of conflicting parameters.'} Author (LTI's Professor): Lei Li; Title: A Survey for In-context Learning; Authors: Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui; Abstract: With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few training examples. It has been a new trend exploring ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress, challenges, and future work in ICL. We \\ufb01rst present a formal de\\ufb01nition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques of ICL, including training strategies, prompting strategies, and so on. Finally, we present the challenges of ICL and provide potential directions for further research. We hope our work can encourage more research on uncovering how ICL works and improving ICL in future work. 1; Year: 2023; Venue: arXiv.org; Citations: 167; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The progress, challenges, and future work in ICL are summarized and a formal definition of ICL is presented and its correlation to related studies are clarified and potential directions for further research are provided.'} Author (LTI's Professor): Lei Li; Title: ImageNetVC: Zero-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories; Authors: Heming Xia, Qingxiu Dong, Lei Li, Jingjing Xu, Ziwei Qin, Zhifang Sui; Abstract: Recently, Pretrained Language Models (PLMs) have been serving as general-purpose interfaces, posing a significant demand for comprehensive visual knowledge. However, it remains unclear how well current PLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge. To investigate this, we propose I MAGE N ET VC, a fine-grained, human-annotated dataset specifically designed for zero-shot visual common-sense evaluation across 1,000 ImageNet categories. Utilizing I MAGE N ET VC, we delve into the fundamental visual commonsense knowledge of both unimodal PLMs and VaLMs, un-covering the scaling law and the influence of the backbone model on VaLMs. Furthermore, we investigate the factors affecting the visual commonsense knowledge of large-scale models, providing insights into the development of language models enriched with visual commonsense knowledge. Our code and dataset are available at https://github.com/hemingkx/ ImageNetVC .; Year: 2023; Venue: arXiv.org; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes I MAGE N ET VC, a fine-grained, human-annotated dataset specifically designed for zero-shot visual common-sense evaluation across 1,000 ImageNet categories, and delves into the fundamental visual commonsense knowledge of both unimodal PLMs and VaLMs, un-covering the scaling law and the influence of the backbone model on VaL Ms.'} Author (LTI's Professor): Lei Li; Title: Can We Edit Factual Knowledge by In-Context Learning?; Authors: Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, Baobao Chang; Abstract: Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or out-dated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/Zce1112zslx/IKE.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 44; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge.'} Author (LTI's Professor): Teruko Mitamura; Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA; Authors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg; Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.; Year: 2023; Venue: Workshop on Document-grounded Dialogue and Conversational Question Answering; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.'} Author (LTI's Professor): Teruko Mitamura; Title: ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules; Authors: Zhi-Qi Cheng, Qianwen Dai, Siyao Li, Jingdong Sun, T. Mitamura, A. Hauptmann; Abstract: Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy. We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrating its superiority over existing methods. Our proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model. Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.1; Year: 2023; Venue: IEEE International Conference on Computer Vision; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model and offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.'} Author (LTI's Professor): Teruko Mitamura; Title: Hierarchical Event Grounding; Authors: Jiefu Ou, Adithya Pratapa, Rishubh Gupta, T. Mitamura; Abstract: Event grounding aims at linking mention references in text corpora to events from a knowledge base (KB). Previous work on this task focused primarily on linking to a single KB event, thereby overlooking the hierarchical aspects of events. Events in documents are typically described at various levels of spatio-temporal granularity. These hierarchical relations are utilized in downstream tasks of narrative understanding and schema construction. In this work, we present an extension to the event grounding task that requires tackling hierarchical event structures from the KB. Our proposed task involves linking a mention reference to a set of event labels from a subevent hierarchy in the KB. We propose a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss. On an automatically created multilingual dataset from Wikipedia and Wikidata, our experiments demonstrate the effectiveness of the hierarchical loss against retrieve and re-rank baselines. Furthermore, we demonstrate the systems' ability to aid hierarchical discovery among unseen events. Code is available at https://github.com/JefferyO/Hierarchical-Event-Grounding; Year: 2023; Venue: AAAI Conference on Artificial Intelligence; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents an extension to the event grounding task that requires tackling hierarchical event structures from the KB, and proposes a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework; Authors: P. Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard J. Chen, Zihao Deng, Nicholas Allen, R. Auerbach, Faisal Mahmood, R. Salakhutdinov, Louis-Philippe Morency; Abstract: The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task, and introduces two new estimators for these PID statistics that scale to high-dimensional distributions.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Quantifying & Modeling Feature Interactions: An Information Decomposition Framework; Authors: P. Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard J. Chen, Zihao Deng, Faisal Mahmood, R. Salakhutdinov, Louis-Philippe Morency; Abstract: The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different signals. Despite these empirical advances, there remain fundamental research questions: how can we quantify the nature of interactions that exist among input features? Subsequently, how can we capture these interactions using suitable data-driven methods? To answer this question, we propose an information-theoretic approach to quantify the degree of redundancy , uniqueness , and synergy across input features, which we term the PID statistics of a multimodal distribution. Using 2 newly proposed estimators that scale to high-dimensional distributions, we demonstrate their usefulness in quantifying the interactions within multimodal datasets, the nature of interactions captured by multimodal models, and principled approaches for model selection. We conduct extensive experiments on both synthetic datasets where the PID statistics are known and on large-scale multimodal benchmarks where PID estimation was previously impossible. Finally, to demonstrate the real-world applicability of our approach, we present three case studies in pathology, mood prediction, and robotic perception where our framework accurately recommends strong multimodal models for each application.; Year: 2023; Venue: arXiv.org; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy across input features, which is term the PID statistics of a multimodal distribution.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities; Authors: Alex Wilf, Sihyun Shawn Lee, P. Liang, Louis-Philippe Morency; Abstract: Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory\\\"Simulation Theory\\\"to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs' ToM capabilities.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"SimToM is introduced, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking that shows substantial improvement over existing methods and suggests perspective- taking as a promising direction for future research into improving LLMs' ToM capabilities.\\\"} Author (LTI's Professor): Louis-Philippe Morency; Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings; Authors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman; Abstract: None; Year: 2023; Venue: Journal of Vision; Citations: 0; TLDR: None Author (LTI's Professor): Louis-Philippe Morency; Title: Tutorial on Multimodal Machine Learning: Principles, Challenges, and Open Questions; Authors: P. Liang, Louis-Philippe Morency; Abstract: Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents capable of understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in healthcare and robotics, multimodality has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this tutorial is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. Building upon a new edition of our survey paper on multimodal ML and academic courses at CMU, this tutorial will cover three topics: (1) what is multimodal: the principles in learning from heterogeneous, connected, and interacting data, (2) why is it hard: a taxonomy of six core technical challenges faced in multimodal ML but understudied in unimodal ML, and (3) what is next: major directions for future research as identified by our taxonomy.; Year: 2023; Venue: ICMI Companion; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This tutorial is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning by synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Continual Learning for Personalized Co-Speech Gesture Generation; Authors: Chaitanya Ahuja, Pratik Joshi, Ryo Ishii, Louis-Philippe Morency; Abstract: Co-speech gestures are a key channel of human communication, making them important for personalized chat agents to generate. In the past, gesture generation models assumed that data for each speaker is available all at once, and in large amounts. However in practical scenarios, speaker data comes sequentially and in small amounts as the agent personalizes with more speakers, akin to a continual learning paradigm. While more recent works have shown progress in adapting to low-resource data, they catastrophically forget the gesture styles of initial speakers they were trained on. Also, prior generative continual learning works are not multimodal, making this space less studied. In this paper, we explore this new paradigm and propose C-DiffGAN: an approach that continually learns new speaker gesture styles with only a few minutes of per-speaker data, while retaining previously learnt styles. Inspired by prior continual learning works, C-DiffGAN encourages knowledge retention by 1) generating reminiscences of previous low-resource speaker data, then 2) crossmodally aligning to them to mitigate catastrophic forgetting. We quantitatively demonstrate improved performance and reduced forgetting over strong baselines through standard continual learning measures, reinforced by a qualitative user study that shows that our method produces more natural, style-preserving gestures. Code and videos can be found at https://chahuja.com/cdiffgan; Year: 2023; Venue: IEEE International Conference on Computer Vision; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper quantitatively demonstrate improved performance and reduced forgetting over strong baselines through standard continual learning measures, reinforced by a qualitative user study that shows that the proposed C-DiffGAN method produces more natural, style-preserving gestures.'} Author (LTI's Professor): Louis-Philippe Morency; Title: MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning; Authors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov; Abstract: Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'MultiZoo is released, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas that provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation.'} Author (LTI's Professor): Louis-Philippe Morency; Title: MMOE: Mixture of Multimodal Interaction Experts; Authors: Haofei Yu, P. Liang, R. Salakhutdinov, Louis-Philippe Morency; Abstract: Multimodal machine learning, which studies the information and interactions across various input modalities, has made significant advancements in understanding the relationship between images and descriptive text. However, this is just a portion of the potential multimodal interactions seen in the real world and does not include new interactions between conflicting utterances and gestures in predicting sarcasm, for example. Notably, the current methods for capturing shared information often do not extend well to these more nuanced interactions, sometimes performing as low as 50% in binary classification. In this paper, we address this problem via a new approach called MMOE, which stands for a mixture of multimodal interaction experts. Our method automatically classifies data points from unlabeled multimodal datasets by their interaction type and employs specialized models for each specific interaction. Based on our experiments, this approach improves performance on these challenging interactions by more than 10%, leading to an overall increase of 2% for tasks like sarcasm prediction. As a result, interaction quantification provides new insights for dataset analysis and yields simple approaches that obtain state-of-the-art performance.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This method automatically classifies data points from unlabeled multimodal datasets by their interaction type and employs specialized models for each specific interaction, leading to an overall increase of 2% for tasks like sarcasm prediction.'} Author (LTI's Professor): Louis-Philippe Morency; Title: SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations; Authors: Victoria Lin, Louis-Philippe Morency; Abstract: Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents SenteCon, a method for introducing human interpretability in deep language representations that outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth.; Authors: H. Swartz, Lauren M. Bylsma, Jay Fournier, J. Girard, C. Spotts, J. Cohn, Louis-Philippe Morency; Abstract: None; Year: 2023; Venue: Journal of Affective Disorders; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Preliminary evidence supporting the efficacy of both brief IPT and CBT, delivered by either TH or IP, for depression showed that working alliance is preserved in TH, and delivery via TH may improve therapy adherence.'} Author (LTI's Professor): Louis-Philippe Morency; Title: MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things; Authors: Shentong Mo, P. Liang, Russ Salakhutdinov, Louis-Philippe Morency; Abstract: The Internet of Things (IoT), the network integrating billions of smart physical devices embedded with sensors, software, and communication technologies for the purpose of connecting and exchanging data with other devices and systems, is a critical and rapidly expanding component of our modern world. The IoT ecosystem provides a rich source of real-world modalities such as motion, thermal, geolocation, imaging, depth, sensors, video, and audio for prediction tasks involving the pose, gaze, activities, and gestures of humans as well as the touch, contact, pose, 3D of physical objects. Machine learning presents a rich opportunity to automatically process IoT data at scale, enabling efficient inference for impact in understanding human wellbeing, controlling physical devices, and interconnecting smart cities. To develop machine learning technologies for IoT, this paper proposes MultiIoT, the most expansive IoT benchmark to date, encompassing over 1.15 million samples from 12 modalities and 8 tasks. MultiIoT introduces unique challenges involving (1) learning from many sensory modalities, (2) fine-grained interactions across long temporal ranges, and (3) extreme heterogeneity due to unique structure and noise topologies in real-world sensors. We also release a set of strong modeling baselines, spanning modality and task-specific methods to multisensory and multitask models to encourage future research in multisensory representation learning for IoT.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes MultiIoT, the most expansive IoT benchmark to date, encompassing over 1.15 million samples from 12 modalities and 8 tasks, and releases a set of strong modeling baselines, spanning modality and task-specific methods to multisensory and multitask models to encourage future research in mult isensory representation learning for IoT.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Neural Mixed Effects for Nonlinear Personalized Predictions; Authors: T. W\\u00f6rtwein, Nicholas Allen, Lisa B. Sheeber, R. Auerbach, J. Cohn, Louis-Philippe Morency; Abstract: Personalized prediction is a machine learning approach that predicts a person\\u2019s future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a neural network in a scalable manner1. NME combines the efficiency of neural network optimization with nonlinear mixed effects modeling. Empirically, we observe that NME improves performance across six unimodal and multimodal datasets, including a smartphone dataset to predict daily mood and a mother-adolescent dataset to predict affective state sequences where half the mothers experience symptoms of depression. Furthermore, we evaluate NME for two model architectures, including for neural conditional random fields (CRF) to predict affective state sequences where the CRF learns nonlinear person-specific temporal transitions between affective states. Analysis of these person-specific transitions on the mother-adolescent dataset shows interpretable trends related to the mother\\u2019s depression symptoms.; Year: 2023; Venue: International Conference on Multimodal Interaction; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'NME combines the efficiency of neural network optimization with nonlinear mixed effects modeling and improves performance across six unimodal and multimodal datasets, including a smartphone dataset to predict daily mood and a mother-adolescent datasets to predict affective state sequences where half the mothers experience symptoms of depression.'} Author (LTI's Professor): Louis-Philippe Morency; Title: MultiZoo and MultiBench: A Standardized Toolkit for Multimodal Deep Learning; Authors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov; Abstract: None; Year: 2023; Venue: Journal of machine learning research; Citations: 0; TLDR: None Author (LTI's Professor): Louis-Philippe Morency; Title: SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior; Authors: Maneesh Bilalpur, Saurabh Hinduja, Laura Cariola, Lisa B. Sheeber, Nicholas B Allen, Louis-Philippe Morency, Jeffrey F. Cohn; Abstract: Depression strongly impacts parents\\u2019 behavior. Does parents\\u2019 depression strongly affect the behavior of their children as well? To investigate this question, we compared dyadic interactions between 73 depressed and 75 non-depressed mothers and their adolescent child. Families were of low income and 84% were white. Child behavior was measured from audio-video recordings using manual annotation of verbal and nonverbal behavior by expert coders and by multimodal computational measures of facial expression, face and head dynamics, prosody, speech behavior, and linguistics. For both sets of measures, we used Support Vector Machines. For computational measures, we investigated the relative contribution of single versus multiple modalities using a novel approach to SHapley Additive exPlanations (SHAP). Computational measures outperformed manual ratings by human experts. Among individual computational measures, prosody was the most informative. SHAP reduction resulted in a four-fold decrease in the number of features and highest performance (77% accuracy; positive and negative agreements at 75% and 76%, respectively). These findings suggest that maternal depression strongly impacts the behavior of adolescent children; differences are most revealed in prosody; multimodal features together with SHAP reduction are most powerful.; Year: 2023; Venue: International Conference on Multimodal Interaction; Citations: 0; TLDR: None Author (LTI's Professor): Louis-Philippe Morency; Title: What are Adapters Really Efficient At?; Authors: Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Mitesh M. Khapra, Pratyush Kumar, V. Rudra, Murthy Anoop, Kunchukuttan. 2022, Naama-677, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir R. Radev, Al-683 ham, Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Jonas Pfeiffer, Aishwarya Kamath, Andreas R\\u00fcckl\\u00e9, Kyunghyun Cho, Iryna Gurevych, Clifton Poth, Aishwarya, Ivan Kamath, Sebastian Vuli\\u00b4c, Kyunghyun Ruder, Gregor Geigle, Max Glockner, Jonas Beck, Nils Pfeiffer, Reimers Iryna, Victor Sanh, Colin Raffel, Lintang Bach, Zaid Sutawika, Antoine Alyafeai, Arnaud Chaffin, Arun Stiegler, Manan Raja, Dey Saiful, Canwen Bari, Urmish Xu, Thakker, Shanya Sharma, Eliza Szczechla, Taewoon, Gunjan Kim, Nihal Chhablani, Nayak, Debajyoti, Jonathan Datta, Mike Tian-Jian Chang, Han Jiang, Matteo Wang, S. Manica, Zheng Xin Shen, Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Tripathi Neeraj, Jos Rozen, Abheesht Sharma, A. Santilli, Thibault F\\u00e9vry, Jason Alan Fries, Maarten Sap, Hannah Rashkin, Derek Chen, Ronan, Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Adam R. Brown, Adam Santoro, Adri\\u00e0 Gupta, Agnieszka Garriga-Alonso, Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par-765, Allen Nie, Aman Hussain, Amanda Askell, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, A. Santilli, Andreas Stuhlm\\u00fcller, Andrew M. Dai, Andrew La, Andrew Lampinen, Angela Zou, Angelica Jiang, Anh Chen, Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-773, Arul Menezes, Arun Kirubarajan, Asher Mul-774, Ashish lokandov, Austin Sabharwal, Herrick, Avia, A. Efrat, Ayla Erdem, B. Karaka\\u00b8s, Ryan Roberts, B. S. Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan \\u00d6zyurt, Behnam Hedayatnia, Behnam, Benjamin Neyshabur, Benno Inden, Berk Stein, Ek-779 mekci, Bill Yuchen, Blake Lin, Cameron Howald, Cameron Diao, Catherine Dour, Cedrick Stinson, Ar-781 C\\u00e9sar, Chandan Ferri Ram\\u00edrez, Charles Singh, Christopher D. Manning, Christopher Potts, Cindy 785 Ramirez, Clara Rivera, Clemencia Siro, Colin Raf-786, Courtney Ashcraft, Cristina Garbacea, Dan Sileo, Daniel H Garrette, Dan Hendrycks, Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\\u00ed Gonz\\u00e1lez, Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, D. Gilboa, David Dohan, D. Drakard, David Ju-792, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Tam, m\\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-807 L\\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijase-808 vic, Hannah Kim, Harsh Mehta, H. Bogar, Henry Shevlin, Hinrich Sch\\u00fctze, H. Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jae-813 hoon Lee, J. Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Ko-815 co\\u00b4n, Jana Thompson, Jared Kaplan, Jarema Radom, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, K. Markert, Kaustubh D. Dhole, Kevin Gim-827 pel, Kevin Omondi, K. Mathewson, Kristen Chi-828 afullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-829 Donell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras Ochando, Louis-Philippe Morency, Luca Moschella, Maarten \\u00b8Senel, Maarten Bosma, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Jose Maru, Marie Ram\\u00edrez Quintana, Tolkiehn Mario, Martha Giulianelli, Martin Lewis, L. PotthastMatthew, Matthew L. Leavitt, M\\u00e1ty\\u00e1s Schu-840 bert Hagen, Medina Orduna, Melody Baitemirova, Arnaud Melvin, Michael A McElrath, Michael Yee, Michael Co-842 hen, Michael Gu, M. Ivanitskiy, Michael Star-843 ritt, M. Strube, Michele Sw\\u02dbedrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mo Suzgun, Monica Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T. Varma, Nanyun Peng, tish Shirish Keskar, Niveditha Iyer, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio, Moreno Casares, Parth Doshi, Jason Wei, Maarten Bosma, Vincent Y. Zhao, Adams Wei Guu, Brian Yu, Nan Lester, An-921 Du, M. Dai, Quoc V. Le, Finetuned, Adina Williams, Nikita Nangia, Samuel R. Bowman, Thomas Wolf, Lysandre Debut, Clement Chaumond, Anthony Delangue, Pier-339 Moi, Tim ric Cistac, R\\u00e9mi Rault, Morgan Louf, Funtow-900 Joe, Sam Davison, Patrick Shleifer, von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Sylvain Gugger, Mariama Drame, Yinfei Yang, Yuan Zhang, Chris Tar, Hailey Schoelkopf, Niklas Muen-954, Alham Fikri, David Ifeoluwa Adelani, M Saiful Bari, Lintang Sutawika, Zhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo, Jonathan May; Abstract: Adapters have been positioned as a parameter-001 efficient fine-tuning (PEFT) approach, whereby 002 a minimal number of parameters are added to 003 the model and fine-tuned. However, adapters 004 have not been sufficiently analyzed to under-005 stand if PEFT translates to benefits in train-006 ing/deployment efficiency and maintainabil-007 ity/extensibility. Through extensive experi-008 ments on many adapters, tasks, and languages 009 in supervised and cross-lingual zero-shot set-010 tings, we clearly show that for Natural Lan-011 guage Understanding tasks, the parameter ef-012 ficiency in adapters does not translate to effi-013 ciency gains compared to full fine-tuning of 014 models. More precisely, adapters are relatively 015 expensive to train and have slightly higher de-016 ployment latency. Furthermore, the maintain-017 ability /extensibility benefits of adapters can be 018 achieved with simpler approaches like multi-019 task training via full fine-tuning, which also 020 provide relatively faster training times. We, 021 therefore, recommend that for moderately sized 022 models practitioners should rely on full fine-023 tuning or multi-task training rather than using 024 adapters. 025; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is recommended that for moderately sized models practitioners should rely on full fine-023 tuning or multi-task training rather than using adapters, as adapters are relatively expensive to train and have slightly higher de-016 ployment latency.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Text-Transport: Toward Learning Causal Effects of Natural Language; Authors: Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael; Abstract: As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper introduces Text-Transport, a method for estimation of causal effects from natural language under any text distribution that transports causal effects between domains, bypassing the need for strong assumptions in the target domain.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos; Authors: Dong Won Lee, Chaitanya Ahuja, P. Liang, Sanika Natu, Louis-Philippe Morency; Abstract: Many educational videos use slide presentations, a sequence of visual pages that contain text and figures accompanied by spoken language, which are constructed and presented carefully in order to optimally transfer knowledge to students. Previous studies in multimedia and psychology attribute the effectiveness of lecture presentations to their multimodal nature. As a step toward developing vision-language models to aid in student learning as intelligent teacher assistants, we introduce the Lecture Presentations Multimodal (LPM) Dataset as a large-scale benchmark testing the capabilities of vision-and-language models in multimodal understanding of educational videos. Our dataset contains aligned slides and spoken language, for 180+ hours of video and 9000+ slides, with 10 lecturers from various subjects (e.g., computer science, dentistry, biology). We introduce three research tasks, (1) figure-to-text retrieval, (2) text-to-figure retrieval, and (3) generation of slide explanations, which are grounded in multimedia learning and psychology principles to test a vision-language model\\u2019s understanding of multimodal content. We provide manual annotations to help implement these tasks and establish baselines on them. Comparing baselines and human student performances, we find that state-of-the-art vision-language models (zero-shot and fine-tuned) struggle in (1) weak crossmodal alignment between slides and spoken text, (2) learning novel visual mediums, (3) technical language, and (4) long-range sequences. We introduce PolyViLT, a novel multimodal transformer trained with a multi-instance learning loss that is more effective than current approaches for retrieval. We conclude by shedding light on the challenges and opportunities in multimodal understanding of educational presentation videos.; Year: 2023; Venue: IEEE International Conference on Computer Vision; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'PolyViLT is introduced, a novel multimodal transformer trained with a multi-instance learning loss that is more effective than current approaches for retrieval and sheds light on the challenges and opportunities in multimodal understanding of educational presentation videos.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Counterfactual Augmentation for Multimodal Learning Under Presentation Bias; Authors: Victoria Lin, Louis-Philippe Morency, D. Dimitriadis, Srinagesh Sharma; Abstract: In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a presentation bias in the labels that compromises the ability to train new models. In this paper, we propose counterfactual augmentation, a novel causal method for correcting presentation bias using generated counterfactual labels. Our empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods. Model analyses further indicate that the generated counterfactuals align closely with true counterfactuals in an oracle setting.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods, and model analyses indicate that the generatedcounterfactuals align closely with true counterfactUALs in an oracle setting.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Multimodal Feature Selection for Detecting Mothers' Depression in Dyadic Interactions with their Adolescent Offspring; Authors: Maneesh Bilalpur, Saurabh Hinduja, Laura A. Cariola, Lisa B. Sheeber, Nick Alien, L\\u00e1szl\\u00f3 A. Jeni, Louis-Philippe Morency, J. Cohn; Abstract: Depression is the most common psychological disorder, a leading cause of disability world-wide, and a major contributor to inter-generational transmission of psychopathol-ogy within families. To contribute to our understanding of depression within families and to inform modality selection and feature reduction, it is critical to identify interpretable features in developmentally appropriate contexts. Mothers with and without depression were studied. Depression was defined as history of treatment for depression and elevations in current or recent symptoms. We explored two multimodal feature selection strategies in dyadic interaction tasks of mothers with their adolescent children for depression detection. Modalities included face and head dynamics, facial action units, speech-related behavior, and verbal features. The initial feature space was vast and inter-correlated (collinear). To reduce dimension-ality and gain insight into the relative contribution of each modality and feature, we explored feature selection strategies using Variance Inflation Factor (VIF) and Shapley values. On an average collinearity correction through VIF resulted in about 4 times feature reduction across unimodal and multimodal features. Collinearity correction was also found to be an optimal intermediate step prior to Shapley analysis. Shapley feature selection following VIF yielded best performance. The top 15 features obtained through Shapley achieved 78 % accuracy. The most informative features came from all four modalities sampled, which supports the importance of multimodal feature selection.; Year: 2023; Venue: IEEE International Conference on Automatic Face & Gesture Recognition; Citations: 2; TLDR: None Author (LTI's Professor): Louis-Philippe Morency; Title: Difference-Masking: Choosing What to Mask in Continued Pretraining; Authors: Alex Wilf, Syeda Nahida Akter, Leena Mathur, P. Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency; Abstract: The self-supervised objective of masking-and-predicting has led to promising performance gains on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition that deciding what to mask can substantially improve learning outcomes. We investigate this in continued pretraining setting in which pretrained models continue to pretrain on domain-specific data before performing some downstream task. We introduce Difference-Masking, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language-only and multimodal video tasks.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Difference-Masking is introduced, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Expanding the Role of Affective Phenomena in Multimodal Interaction Research; Authors: Leena Mathur, Maja J Matari'c, Louis-Philippe Morency; Abstract: In recent decades, the field of affective computing has made substantial progress in advancing the ability of AI systems to recognize and express affective phenomena, such as affect and emotions, during human-human and human-machine interactions. This paper describes our examination of research at the intersection of multimodal interaction and affective computing, with the objective of observing trends and identifying understudied areas. We examined over 16,000 papers from selected conferences in multimodal interaction, affective computing, and natural language processing: ACM International Conference on Multimodal Interaction, AAAC International Conference on Affective Computing and Intelligent Interaction, Annual Meeting of the Association for Computational Linguistics, and Conference on Empirical Methods in Natural Language Processing. We identified 910 affect-related papers and present our analysis of the role of affective phenomena in these papers. We find that this body of research has primarily focused on enabling machines to recognize or express affect and emotion; there has been limited research on how affect and emotion predictions might, in turn, be used by AI systems to enhance machine understanding of human social behaviors and cognitive states. Based on our analysis, we discuss directions to expand the role of affective phenomena in multimodal interaction research.; Year: 2023; Venue: International Conference on Multimodal Interaction; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An examination of research at the intersection of multimodal interaction and affective computing, with the objective of observing trends and identifying understudied areas, finds that this body of research has primarily focused on enabling machines to recognize or express affect and emotion.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification; Authors: P. Liang, Yun Cheng, R. Salakhutdinov, Louis-Philippe Morency; Abstract: In order to perform multimodal fusion of heterogeneous signals, we need to understand their interactions: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how humans annotate two categorizations of multimodal interactions: (1) partial labels, where different annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator annotates the label given the first modality before asking them to explicitly reason about how their answer changes when given the second. We further propose an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions, uniqueness: the extent to which one modality enables a prediction that the other does not, and synergy: the extent to which both modalities enable one to make a prediction that one would not otherwise make using individual modalities. Through experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate and efficient method for quantifying multimodal interactions.; Year: 2023; Venue: International Conference on Multimodal Interaction; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A comparative study of how humans annotate two categorizations of multimodal interactions is performed and a method to automatically convert annotations of partial and counterfactual labels to information decomposition is proposed, yielding an accurate and efficient method for quantifying multimodals interactions.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Intensive Longitudinal Assessment of Adolescents to Predict Suicidal Thoughts and Behaviors.; Authors: R. Auerbach, Ranqing Lan, H. Galfalvy, Kira L. Alqueza, J. Cohn, Ryann Crowley, Katherine Durham, Karla Joyce, Lauren E. Kahn, Rahil A. Kamath, Louis-Philippe Morency, G. Porta, A. Srinivasan, Jamie Zelazny, D. Brent, Nicholas Allen; Abstract: None; Year: 2023; Venue: Journal of the American Academy of Child and Adolescent Psychiatry; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"Intensive longitudinal assessment through personal smartphones offers a feasible method to assess variability in adolescents' emotional experiences and suicide risk.\\\"} Author (LTI's Professor): Louis-Philippe Morency; Title: Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications; Authors: P. Liang, Chun Kai Ling, Yun Cheng, A. Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, R. Salakhutdinov; Abstract: In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We validate these estimated bounds and show how they accurately track true interactions. Finally, two semi-supervised multimodal applications are explored based on these theoretical results: (1) analyzing the relationship between multimodal performance and estimated interactions, and (2) self-supervised learning that embraces disagreement between modalities beyond agreement as is typically done.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings and validate these estimated bounds and show how they accurately track true interactions.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Representation Learning for Interpersonal and Multimodal Behavior Dynamics: A Multiview Extension of Latent Change Score Models; Authors: A. Vail, J. Girard, Lauren M. Bylsma, Jay Fournier, Holly A. Swartz, Jeffrey F. Cohn, Louis-Philippe Morency; Abstract: Characterizing the dynamics of behavior across multiple modalities and individuals is a vital component of computational behavior analysis. This is especially important in certain applications, such as psychotherapy, where individualized tracking of behavior patterns can provide valuable information about the patient\\u2019s mental state. Conventional methods that rely on aggregate statistics and correlational metrics may not always suffice, as they are often unable to capture causal relationships or evaluate the true probability of identified patterns. To address these challenges, we present a novel approach to learning multimodal and interpersonal representations of behavior dynamics during one-on-one interaction. Our approach is enabled by the introduction of a multiview extension of latent change score models, which facilitates the concurrent capture of both inter-modal and interpersonal behavior dynamics and the identification of directional relationships between them. A core advantage of our approach is its high level of interpretability while simultaneously achieving strong predictive performance. We evaluate our approach within the domain of therapist-client interactions, with the objective of gaining a deeper understanding about the collaborative relationship between the two, a crucial element of the therapeutic process. Our results demonstrate improved performance over conventional approaches that rely upon summary statistics or correlational metrics. Furthermore, since our multiview approach includes the explicit modeling of uncertainty, it naturally lends itself to integration with probabilistic classifiers, such as Gaussian process models. We demonstrate that this integration leads to even further improved performance, all the while maintaining highly interpretable qualities. Our analysis provides compelling motivation for further exploration of stochastic systems within computational models of behavior.; Year: 2023; Venue: International Conference on Multimodal Interaction; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents a novel approach to learning multimodal and interpersonal representations of behavior dynamics during one-on-one interaction enabled by the introduction of a multiview extension of latent change score models that demonstrates improved performance over conventional approaches that rely upon summary statistics or correlational metrics.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Comparative Knowledge Distillation; Authors: Alex Wilf, Alex Tianyi Xu, P. Liang, A. Obolenskiy, Daniel Fried, Louis-Philippe Morency; Abstract: In the era of large scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally heavy teacher models to lightweight, efficient student models while preserving performance. Traditional KD paradigms, however, assume readily available access to teacher models for frequent inference -- a notion increasingly at odds with the realities of costly, often proprietary, large scale models. Addressing this gap, our paper considers how to minimize the dependency on teacher model inferences in KD in a setting we term Few Teacher Inference Knowledge Distillation (FTI KD). We observe that prevalent KD techniques and state of the art data augmentation strategies fall short in this constrained setting. Drawing inspiration from educational principles that emphasize learning through comparison, we propose Comparative Knowledge Distillation (CKD), which encourages student models to understand the nuanced differences in a teacher model's interpretations of samples. Critically, CKD provides additional learning signals to the student without making additional teacher calls. We also extend the principle of CKD to groups of samples, enabling even more efficient learning from limited teacher calls. Empirical evaluation across varied experimental settings indicates that CKD consistently outperforms state of the art data augmentation and KD techniques.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"Drawing inspiration from educational principles that emphasize learning through comparison, CKD is proposed, which encourages student models to understand the nuanced differences in a teacher model's interpretations of samples, and consistently outperforms state of the art data augmentation and KD techniques.\\\"} Author (LTI's Professor): Louis-Philippe Morency; Title: MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models; Authors: P. Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, R. Salakhutdinov; Abstract: The nature of human and computer interactions are inherently multimodal, which has led to substantial interest in building interpretable, interactive, and reliable multimodal interfaces. However, modern multimodal models and interfaces are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize their internal workings in order to empower stakeholders to visualize model behavior, perform model debugging, and promote trust in these models? Our paper proposes MultiViz, a method for analyzing the behavior of multimodal models via 4 stages: (1) unimodal importance, (2) cross-modal interactions, (3) multimodal representations and (4) multimodal prediction. MultiViz includes modular visualization tools for each stage before combining outputs from all stages through an interactive and human-in-the-loop API. Through user studies with 21 participants on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available at https://github.com/pliang279/MultiViz, will be regularly updated with new visualization tools and metrics, and welcomes input from the community1.; Year: 2023; Venue: CHI Extended Abstracts; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that the complementary stages in MultiViz together enable users to simulate model predictions, assign interpretable concepts to features, perform error analysis on model misclassifications, and use insights from error analysis to debug models.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models; Authors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang; Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE, and formulate the underlying data-generating process as a hierarchical latent variable model, and shows that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy; Authors: P. Liang, Zihao Deng, Martin Q. Ma, James Y. Zou, Louis-Philippe Morency, R. Salakhutdinov; Abstract: In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks; Year: 2023; Venue: Neural Information Processing Systems; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'FactorCL is a new multimodal representation learning method to go beyond multi-view redundancy and captures both shared and unique information and achieves state-of-the-art results on six benchmarks.'} Author (LTI's Professor): Louis-Philippe Morency; Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents; Authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap; Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.; Year: 2023; Venue: arXiv.org; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': 'SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence, is presented and it is found that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions; Authors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, P. Liang, Louis-Philippe Morency; Abstract: Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper empirically shows that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced, and argues that the few-shot de-biasing approach is highly feasible and practical.'} Author (LTI's Professor): David Mortensen; Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages; Authors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig; Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\\u2019s resource level is the most important feature in determining ChatGPT\\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.; Year: 2023; Venue: Conference on Machine Translation; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work presents the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark, and reveals that a language\\u2019s resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests thatChatGPT is especially disadvantaged for LRLs and African languages.\\\"} Author (LTI's Professor): David Mortensen; Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models; Authors: Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov; Abstract: Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work conducts a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages and shows evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results.\\\"} Author (LTI's Professor): David Mortensen; Title: African Substrates Rather Than European Lexifiers to Augment African-diaspora Creole Translation; Authors: David R. Mortensen; Abstract: Machine translation (MT) model training is difficult for low-resource languages. This is especially true for African-diaspora Creole languages because of data scarcity. Cross-lingual data augmentation methods with knowledge transfer from related high-resource languages are a common technique to overcome this disadvantage. For instance, practitioners may transfer knowledge from a language in the same language family as the low-resource language of interest. Africandiaspora Creole languages are low-resource and simultaneously have relationships with multiple language groups. These languages, such as Haitian Creole and Jamaican Patois, are typically lexified by colonial European languages, but they are structurally similar to African languages. We explore the advantages of transferring knowledge from the European lexifier language versus the phylogenetic and typological relatives of the African substrate languages. We analysed Haitian and Jamaican MT: both controlling tightly for data properties across compared transfer languages and later allowing use of all data we collected. Our inquiry demonstrates a significant advantage in using African transfer languages in some settings.; Year: 2023; Venue: AfricaNLP; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This inquiry demonstrates a significant advantage in using African transfer languages in some settings with the advantages of transferring knowledge from the European lexifier language versus the phylogenetic and typological relatives of the African substrate languages.'} Author (LTI's Professor): David Mortensen; Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing; Authors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin; Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.; Year: 2023; Venue: Special Interest Group on Computational Morphology and Phonology Workshop; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'In this submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), approaches to data augmentation and modeling across seven low-resource languages are explored and token classification models are found to be the best performing.'} Author (LTI's Professor): David Mortensen; Title: Multilingual TTS Accent Impressions for Accented ASR; Authors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo; Abstract: None; Year: 2023; Venue: International Conference on Text, Speech and Dialogue; Citations: 0; TLDR: None Author (LTI's Professor): David Mortensen; Title: Construction Grammar Provides Unique Insight into Neural Language Models; Authors: Leonie Weissweiler, Taiqi He, Naoki Otani, David R. Mortensen, L. Levin, Hinrich Sch\\u00fctze; Abstract: Construction Grammar (CxG) has recently been used as the basis for probing studies that have investigated the performance of large pretrained language models (PLMs) with respect to the structure and meaning of constructions. In this position paper, we make suggestions for the continuation and augmentation of this line of research. We look at probing methodology that was not designed with CxG in mind, as well as probing methodology that was designed for specific constructions. We analyse selected previous work in detail, and provide our view of the most important challenges and research questions that this promising new field faces.; Year: 2023; Venue: CXGSNLP; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The view of the most important challenges and research questions that this promising new field of research faces is provided, and an analysis of selected previous work in detail is provided.'} Author (LTI's Professor): David Mortensen; Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation; Authors: David R. Mortensen, Ela Gulsen, Taiqi He, Nathaniel R. Robinson, Jonathan D. Amith, Lindia Tjuatja, L. Levin; Abstract: Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation convention\\u00e2\\u20ac\\u201dGeneralized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.; Year: 2023; Venue: Special Interest Group on Computational Morphology and Phonology Workshop; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An annotation convention is proposed that combines all of these positive properties using an Item-and-Process (IP) framework, and its linguistic adequacy is demonstrated, and it is compared with two other interlinear glossed text annotation schemes.'} Author (LTI's Professor): David Mortensen; Title: Transformed Protoform Reconstruction; Authors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen; Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The Meloni et al (2021) model is updated with the state-of-the-art seq2seq model: the Transformer, which outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognate spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties.'} Author (LTI's Professor): David Mortensen; Title: PWESuite: Phonetic Word Embeddings and Tasks They Facilitate; Authors: Vil\\u00e9m Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel R. Robinson, Mrinmaya Sachan, David R. Mortensen; Abstract: Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Three methods that use articulatory features to build phonetically informed word embeddings are developed that address the inconsistent evaluation of existing phonetic word embedding methods and contribute a task suite to fairly evaluate past, current, and future methods.'} Author (LTI's Professor): Graham Neubig; Title: DiffusER: Diffusion via Edit-based Reconstruction; Authors: Machel Reid, V. Hellendoorn, Graham Neubig; Abstract: In text generation, models that generate text from scratch one token at a time are currently the dominant paradigm. Despite being performant, these models lack the ability to revise existing text, which limits their usability in many practical scenarios. We look to address this, with DIFFUSER (Diffusion via Edit-based Reconstruction), a new edit-based generative model for text based on denoising diffusion models \\u2013 a class of models that use a Markov chain of denoising steps to incrementally generate data. DIFFUSER is not only a strong generative model in general, rivalling autoregressive models on several tasks spanning machine translation, summarization, and style transfer; it can also perform other varieties of generation that standard autoregressive models are not well-suited for. For instance, we demonstrate that DIFFUSER makes it possible for a user to condition generation on a prototype, or an incomplete sequence, and continue revising based on previous edit steps.; Year: 2023; Venue: International Conference on Learning Representations; Citations: 14; TLDR: {'model': 'tldr@v2.0.0', 'text': 'DIFFUSER (Diffusion via Edit-based Reconstruction), a new edit-based generative model for text based on denoising diffusion models \\u2013 a class of models that use a Markov chain of denoising steps to incrementally generate data.'} Author (LTI's Professor): Graham Neubig; Title: Cross-Modal Fine-Tuning: Align then Refine; Authors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar; Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.; Year: 2023; Venue: International Conference on Machine Learning; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work proposes ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities and highlights the importance of data alignment via a series of ablation studies and demonstrates ORCA's utility in data-limited regimes.\\\"} Author (LTI's Professor): Graham Neubig; Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages; Authors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig; Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\\u2019s resource level is the most important feature in determining ChatGPT\\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.; Year: 2023; Venue: Conference on Machine Translation; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work presents the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark, and reveals that a language\\u2019s resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests thatChatGPT is especially disadvantaged for LRLs and African languages.\\\"} Author (LTI's Professor): Graham Neubig; Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing; Authors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, FAHIM FAISAL, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig; Abstract: Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces GlobalBench, an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages and tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world.'} Author (LTI's Professor): Graham Neubig; Title: Learning Performance-Improving Code Edits; Authors: Aman Madaan, Alex Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, A. Yazdanbakhsh; Abstract: The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.; Year: 2023; Venue: arXiv.org; Citations: 28; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper investigates the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits, and hypothesizes that language models can suggest such edits in ways that would be impractical for static analysis alone.'} Author (LTI's Professor): Graham Neubig; Title: CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code; Authors: Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig; Abstract: Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 26; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is found that generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed, than all existing metrics.'} Author (LTI's Professor): Graham Neubig; Title: Divergences between Language Models and Human Brains; Authors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe; Abstract: Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories.'} Author (LTI's Professor): Graham Neubig; Title: Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction; Authors: Manuel Mager, R. Bhatnagar, Graham Neubig, Ngoc Thang Vu, Katharina Kann; Abstract: Neural models have drastically advanced state of the art for machine translation (MT) between high-resource languages. Traditionally, these models rely on large amounts of training data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of parallel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open questions, product of an increased interest of the NLP community in these languages.; Year: 2023; Venue: AMERICASNLP; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for high- resource languages between high-resource languages.'} Author (LTI's Professor): Graham Neubig; Title: User-Centric Evaluation of OCR Systems for Kwak\\u2019wala; Authors: Shruti Rijhwani, Daisy Rosenblum, Michayla King, Antonios Anastasopoulos, Graham Neubig; Abstract: There has been recent interest in improving optical character recognition (OCR) for endangered languages, particularly because a large number of documents and books in these languages are not in machine-readable formats. The performance of OCR systems is typically evaluated using automatic metrics such as character and word error rates. While error rates are useful for the comparison of different models and systems, they do not measure whether and how the transcriptions produced from OCR tools are useful to downstream users. In this paper, we present a human-centric evaluation of OCR systems, focusing on the Kwak'wala language as a case study. With a user study, we show that utilizing OCR reduces the time spent in the manual transcription of culturally valuable documents -- a task that is often undertaken by endangered language community members and researchers -- by over 50%. Our results demonstrate the potential benefits that OCR tools can have on downstream language documentation and revitalization efforts.; Year: 2023; Venue: COMPUTEL; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This paper presents a human-centric evaluation of OCR systems, focusing on the Kwak'wala language as a case study, and shows that utilizing OCR reduces the time spent in the manual transcription of culturally valuable documents by over 50%.\\\"} Author (LTI's Professor): Graham Neubig; Title: EXCALIBUR: Encouraging and Evaluating Embodied Exploration; Authors: Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Weihs; Abstract: Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: \\u201cis the small heavy red bowl made from glass?\\u201d or \\u201cis there a silver spoon heavier than the egg?\\u201d. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures and represents the next frontier for embodied AI research.'} Author (LTI's Professor): Graham Neubig; Title: Multi-Dimensional Evaluation of Text Summarization with In-Context Learning; Authors: Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, Chunting Zhou; Abstract: Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 14; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency.'} Author (LTI's Professor): Graham Neubig; Title: A Gold Standard Dataset for the Reviewer Assignment Problem; Authors: Ivan Stelmakh, J. Wieting, Graham Neubig, Nihar B. Shah; Abstract: Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\\\"similarity score\\\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information.; Year: 2023; Venue: arXiv.org; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel dataset of similarity scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously is collected and used to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders.'} Author (LTI's Professor): Graham Neubig; Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing; Authors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin; Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.; Year: 2023; Venue: Special Interest Group on Computational Morphology and Phonology Workshop; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'In this submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), approaches to data augmentation and modeling across seven low-resource languages are explored and token classification models are found to be the best performing.'} Author (LTI's Professor): Graham Neubig; Title: Alignment for Honesty; Authors: Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, Pengfei Liu; Abstract: Recent research has made significant strides in applying alignment techniques to enhance the helpfulness and harmlessness of large language models (LLMs) in accordance with human intentions. In this paper, we argue for the importance of alignment for honesty, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning the limits of an LLM's knowledge, which is far from straightforward. This challenge demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. In this paper, we address these challenges by first establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM's honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source a wealth of resources to facilitate future research at https://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned models, training and evaluation datasets for honesty alignment, concept glossary, as well as all relevant source code.; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This paper argues for the importance of alignment for honesty, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative, by establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius.\\\"} Author (LTI's Professor): Graham Neubig; Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation; Authors: Patrick Fernandes, Aman Madaan, Emmy Liu, Ant\\u00f3nio Farinhas, Pedro Henrique Martins, Amanda Bertsch, Jos\\u00e9 G. C. de Souza, Shuyan Zhou, Tongshuang Sherry Wu, Graham Neubig, Andr\\u00e9 F. T. Martins; Abstract: Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.; Year: 2023; Venue: arXiv.org; Citations: 23; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An overview of the recent research that has leveraged human feedback to improve natural language generation and the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention is provided.'} Author (LTI's Professor): Graham Neubig; Title: Program-Aided Reasoners (better) Know What They Know; Authors: Anubha Kabra, Sanketh Rangreji, Yash Mathur, Aman Madaan, Emmy Liu, Graham Neubig; Abstract: Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to\\\"know what they know\\\", which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper compares the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models and demonstrates that, in the majority of cases, program-aided reasoners better know what they know thanText-based counterparts.'} Author (LTI's Professor): Graham Neubig; Title: Learning to Filter Context for Retrieval-Augmented Generation; Authors: Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, Graham Neubig; Abstract: On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.; Year: 2023; Venue: arXiv.org; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'FILCO is proposed, a method that improves the quality of the context provided to the generator by identifying useful context based on lexical and information-theoretic approaches, and training context filtering models that can filter retrieved contexts at test time.'} Author (LTI's Professor): Graham Neubig; Title: FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios; Authors: Ethan Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu; Abstract: The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .; Year: 2023; Venue: arXiv.org; Citations: 48; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT), and demonstrates the efficacy of the proposed method.'} Author (LTI's Professor): Graham Neubig; Title: Active Retrieval Augmented Generation; Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig; Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 50; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'} Author (LTI's Professor): Graham Neubig; Title: Large Language Models Enable Few-Shot Clustering; Authors: Vijay Viswanathan, Kiril Gashteovski, Carolin (Haas) Lawrence, Tongshuang Sherry Wu, Graham Neubig; Abstract: Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.; Year: 2023; Venue: arXiv.org; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is found that incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters.'} Author (LTI's Professor): Graham Neubig; Title: Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach; Authors: Masahiro Kaneko, Graham Neubig, Naoaki Okazaki; Abstract: Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.; Year: 2023; Venue: arXiv.org; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue and shows that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.'} Author (LTI's Professor): Graham Neubig; Title: Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting; Authors: Emmy Liu, Aditi Chaudhary, Graham Neubig; Abstract: Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute accuracy, but also holds potential benefits for non-idiomatic sentences.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'To improve translation of natural idioms, this work introduces two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models.'} Author (LTI's Professor): Graham Neubig; Title: Why do Nearest Neighbor Language Models Work?; Authors: Frank F. Xu, Uri Alon, Graham Neubig; Abstract: Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.; Year: 2023; Venue: International Conference on Machine Learning; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper identifies three main reasons why k-nearest neighbor language models (kNN-LM) perform better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution.'} Author (LTI's Professor): Graham Neubig; Title: Syntax and Semantics Meet in the \\u201cMiddle\\u201d: Probing the Syntax-Semantics Interface of LMs Through Agentivity; Authors: Lindia Tjuatja, Emmy Liu, L. Levin, Graham Neubig; Abstract: Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms\\u2014i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.; Year: 2023; Venue: STARSEM; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far and suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.'} Author (LTI's Professor): Graham Neubig; Title: DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions; Authors: Vijay Viswanathan, Luyu Gao, Tongshuang Sherry Wu, Pengfei Liu, Graham Neubig; Abstract: Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We operationalize the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval algorithms on our test set and present a superior bi-encoder retriever for text-based dataset recommendation. This system, trained on the DataFinder Dataset, finds more relevant search results than existing third-party dataset search engines. To encourage progress on dataset recommendation, we release our dataset and models to the public.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work operationalizes the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs, and builds the DataFinder Dataset, a larger automatically-constructed training set and a smaller expert-annotated evaluation set.'} Author (LTI's Professor): Graham Neubig; Title: Multi-lingual and Multi-cultural Figurative Language Understanding; Authors: Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, Graham Neubig; Abstract: Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, \\\\datasetname, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 11; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work assesses multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings, and reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region.\\\"} Author (LTI's Professor): Graham Neubig; Title: It\\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk; Authors: Amanda Bertsch, Alex Xie, Graham Neubig, Matthew R. Gormley; Abstract: Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area.; Year: 2023; Venue: BIGPICTURE; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical.'} Author (LTI's Professor): Graham Neubig; Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input; Authors: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley; Abstract: Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .; Year: 2023; Venue: Neural Information Processing Systems; Citations: 42; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores.'} Author (LTI's Professor): Graham Neubig; Title: WebArena: A Realistic Web Environment for Building Autonomous Agents; Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig; Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.; Year: 2023; Venue: arXiv.org; Citations: 73; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'} Author (LTI's Professor): Graham Neubig; Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions; Authors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig; Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment.'} Author (LTI's Professor): Graham Neubig; Title: Computational Language Acquisition with Theory of Mind; Authors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig; Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.; Year: 2023; Venue: International Conference on Learning Representations; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"It is found that training speakers with a highly weighted ToM listener component leads to performance gains in the authors' image referential game setting, and suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.\\\"} Author (LTI's Professor): Graham Neubig; Title: DeMuX: Data-efficient Multilingual Learning; Authors: Simran Khanuja, Srinivas Gowriraj, L. Dery, Graham Neubig; Abstract: We consider the task of optimally fine-tuning pre-trained multilingual models, given small amounts of unlabelled target data and an annotation budget. In this paper, we introduce DEMUX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for complex tasks. Our code is released here: https://github.com/simran-khanuja/demux.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'DEMUX is introduced, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set, to optimally fine-tuning pre-trained multilingual models.'} Author (LTI's Professor): Graham Neubig; Title: Improving Factuality of Abstractive Summarization via Contrastive Reward Learning; Authors: Ethan Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig; Abstract: Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries. Code and human evaluation results will be publicly available at \\\\url{https://github.com/EthanC111/factuality_summarization}.; Year: 2023; Venue: TRUSTNLP; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations, suggesting that further advances in learning and evaluation algorithms can feed directly into providing morefactuality summaries.'} Author (LTI's Professor): Graham Neubig; Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents; Authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap; Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.; Year: 2023; Venue: arXiv.org; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': 'SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence, is presented and it is found that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.'} Author (LTI's Professor): Graham Neubig; Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation; Authors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat; Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.; Year: 2023; Venue: Conference on Machine Translation; Citations: 22; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations, and finds that it improves performance compared to just prompting for scores.'} Author (LTI's Professor): Eric Nyberg; Title: GameQA: Gamified Mobile App Platform for Building Multiple-Domain Question-Answering Datasets; Authors: Njall Skarphedinsson, Breki Gudmundsson, Steinar Smari, M. L\\u00e1rusd\\u00f3ttir, H. Einarsson, Abuzar Khan, Eric Nyberg, H. Loftsson; Abstract: The methods used to create many of the well-known Question-Answering (QA) datasets are hard to replicate for low-resource languages. A commonality amongst these methods is hiring annotators to source answers from the internet by querying a single answer source, such as Wikipedia. Applying these methods for low-resource languages can be problematic since there is no single large answer source for these languages. Consequently, this can result in a high ratio of unanswered questions, since the amount of information in any single source is limited. To address this problem, we developed a novel crowd-sourcing platform to gather multiple-domain QA data for low-resource languages. Our platform, which consists of a mobile app and a web API, gamifies the data collection process. We successfully released the app for Icelandic (a low-resource language with about 350,000 native speakers) to build a dataset which rivals large QA datasets for high-resource languages both in terms of size and ratio of answered questions. We have made the platform open source with instructions on how to localize and deploy it to gather data for other low-resource languages.; Year: 2023; Venue: Conference of the European Chapter of the Association for Computational Linguistics; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work developed a novel crowd-sourcing platform to gather multiple-domain QA data for low-resource languages and successfully released the app for Icelandic to build a dataset which rivals large QA datasets for high- resource languages both in terms of size and ratio of answered questions.'} Author (LTI's Professor): Eric Nyberg; Title: InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers; Authors: Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayan Kundu, R. Ramanathan, Eric Nyberg; Abstract: We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available. https://github.com/searchivarius/inpars_light/; Year: 2023; Venue: arXiv.org; Citations: 11; TLDR: {'model': 'tldr@v2.0.0', 'text': 'InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25.'} Author (LTI's Professor): Eric Nyberg; Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA; Authors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg; Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.; Year: 2023; Venue: Workshop on Document-grounded Dialogue and Conversational Question Answering; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.'} Author (LTI's Professor): Eric Nyberg; Title: Chain-of-Skills: A Configurable Model for Open-Domain Question Answering; Authors: Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, Jianfeng Gao; Abstract: The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transfer- ability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a modular retriever where individual modules correspond to key skills that can be reused across datasets and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.'} Author (LTI's Professor): Bhiksha Raj; Title: FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding; Authors: Thanh-Dat Truong, Ngan T. H. Le, B. Raj, J. Cothren, Khoa Luu; Abstract: Although Domain Adaptation in Semantic Scene Segmentation has shown impressive improvement in recent years, the fairness concerns in the domain adaptation have yet to be well defined and addressed. In addition, fairness is one of the most critical aspects when deploying the segmentation models into human-related real-world applications, e.g., autonomous driving, as any unfair predictions could influence human safety. In this paper, we propose a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In particular, from the proposed formulated fairness objective, a new adaptation framework will be introduced based on the fair treatment of class distributions. Moreover, to generally model the context of structural dependency, a new conditional structural constraint is introduced to impose the consistency of predicted segmentation. Thanks to the proposed Conditional Structure Network, the self-attention mechanism has sufficiently modeled the structural information of segmentation. Through the ablation studies, the proposed method has shown the performance improvement of the segmentation models and promoted fairness in the model predictions. The experimental results on the two standard benchmarks, i.e., SYNTHIA $\\\\rightarrow$ Cityscapes and GTA5 $\\\\rightarrow$ Cityscapes, have shown that our method achieved State-of-the-Art (SOTA) performance11The implementation of FREDOM is available at https://github.com/uark-cviu/FREDOM; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation, where a new adaptation framework will be introduced based on the fair treatment of class distributions to generally model the context of structural dependency.'} Author (LTI's Professor): Bhiksha Raj; Title: UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation; Authors: Pha Nguyen, Kha Gia Quach, J. Gauch, S. Khan, B. Raj, Khoa Luu; Abstract: Multiple Object Tracking (MOT) aims to find bounding boxes and identities of targeted objects in consecutive video frames. While fully-supervised MOT methods have achieved high accuracy on existing datasets, they cannot generalize well on a newly obtained dataset or a new unseen domain. In this work, we first address the MOT problem from the cross-domain point of view, imitating the process of new data acquisition in practice. Then, a new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects. It can also learn and update itself from the target data feedback. The intensive experiments are designed on four challenging settings, including MOTSynth to MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then prove the adaptability of the proposed self-supervised learning strategy. The experiments also show superior performance on tracking metrics MOTA and IDF1, compared to fully supervised, unsupervised, and self-supervised state-of-the-art methods.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects, and it can also learn and update itself from the target data feedback.'} Author (LTI's Professor): Bhiksha Raj; Title: SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning; Authors: Hao Chen, R. Tao, Yue Fan, Yidong Wang, Jindong Wang, B. Schiele, Xingxu Xie, B. Raj, M. Savvides; Abstract: The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.; Year: 2023; Venue: International Conference on Learning Representations; Citations: 34; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper revisits the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrates the inherent quantity-quality trade-off problem of pseudo-labels with thresholding, which may prohibit learning.'} Author (LTI's Professor): Bhiksha Raj; Title: Fixed Inter-Neuron Covariability Induces Adversarial Robustness; Authors: Muhammad A Shah, B. Raj; Abstract: The vulnerability to adversarial perturbations is a major flaw of Deep Neural Networks (DNNs) that raises question about their reliability when in real-world scenarios. On the other hand, human perception, which DNNs are supposed to emulate, is highly robust to such perturbations, indicating that there may be certain features of the human perception that make it robust but are not represented in the current class of DNNs. One such feature is that the activity of biological neurons is correlated and the structure of this correlation tends to be rather rigid over long spans of times, even if it hampers performance and learning. We hypothesize that integrating such constraints on the activations of a DNN would improve its adversarial robustness, and, to test this hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern. When evaluated on image and sound recognition tasks, the models with a SCA layer achieved high accuracy, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks \\\\textit{without being trained on adversarially perturbed data; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The SCA layer is developed, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks.'} Author (LTI's Professor): Bhiksha Raj; Title: Understanding political polarization using language models: A dataset and method; Authors: Samiran Gode, Supreeth Bare, B. Raj, H. Yoo; Abstract: Our paper aims to analyze political polarization in US political system using language models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates' views on the economy, healthcare, education, and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model\\u2010based method that helps analyze how polarized a candidate is. Our data are divided into two parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, and so forth. We further split this data into four phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization, we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer\\u2010based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background. The code and data for the project will be available here: \\u201chttps://github.com/samirangode/Understanding_Polarization\\u201d; Year: 2023; Venue: The AI Magazine; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model\\u2010based method that helps analyze how polarized a candidate is.'} Author (LTI's Professor): Bhiksha Raj; Title: Prolonged school closure during the pandemic time in successive waves of COVID-19- vulnerability of children to sexual abuses \\u2013 A case study in Tamil Nadu, India; Authors: Kandaswamy Paramasivan, B. Raj, Nandan Sudarasanam, R. Subburaj; Abstract: None; Year: 2023; Venue: Heliyon; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Considering that the median delay in filing CSA complaints was above 30 days in the mild and post-intervention periods, the upsurge of cases in the more relaxed phases indicates increased occurrences of CSA during strict lockdowns.'} Author (LTI's Professor): Bhiksha Raj; Title: BASS: Block-wise Adaptation for Speech Summarization; Authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj; Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.; Year: 2023; Venue: Interspeech; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'} Author (LTI's Professor): Bhiksha Raj; Title: Understanding Political Polarisation using Language Models: A dataset and method; Authors: Samiran Gode, Supreeth Bare, B. Raj, H. Yoo; Abstract: Our paper aims to analyze political polarization in US political system using Language Models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates views on the economy, healthcare, education and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is. Our data is divided into 2 parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, etc. We further split this data into 4 phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is are used to understand the polarization.'} Author (LTI's Professor): Bhiksha Raj; Title: An Approach to Ontological Learning from Weak Labels; Authors: Ankit Shah, Larry Tang, Po Hao Chou, Yilun Zheng, Ziqian Ge, B. Raj; Abstract: Ontologies encompass a formal representation of knowledge through the definition of concepts or properties of a domain, and the relationships between those concepts. In this work, we seek to investigate whether using this ontological information will improve learning from weakly labeled data, which are easier to collect since it requires only the presence or absence of an event to be known. We use the AudioSet ontology and dataset, which contains audio clips weakly labeled with the ontology concepts and the ontology providing the \\\"Is A\\\" relations between the concepts. We first re-implemented the model proposed by [1] with modifications to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts. We find that the baseline Twin Neural Network (TNN) does not perform better by incorporating ontology information in the weak and multi-label scenario, but that the GCN does capture the ontology knowledge better for weak, multi-labeled data. We also investigate how different modules can tolerate noises introduced from weak labels and better incorporate ontology information. Our best TNN-GCN model achieves mAP=0.45 and AUC=0.87 for lower-level concepts and mAP=0.72 and AUC=0.86 for higher-level concepts, which is an improvement over the baseline TNN but about the same as our models that do not use ontology information.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work re-implements the model proposed by [1] with modifications to fit the multi-label scenario and expands on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts.'} Author (LTI's Professor): Bhiksha Raj; Title: Rethinking Voice-Face Correlation: A Geometry View; Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, B. Raj; Abstract: Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.; Year: 2023; Venue: ACM Multimedia; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction and finds significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium.'} Author (LTI's Professor): Bhiksha Raj; Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement; Authors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj; Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \\u2013 such as spectral tilt, spectral flux, shimmer, etc. \\u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance.'} Author (LTI's Professor): Bhiksha Raj; Title: Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms; Authors: Joseph Konan, Ojas Bhargave, Shikhar Agnihotri, Hojeong Lee, Ankit Shah, Shuo Han, YUNYANG ZENG, Amanda Shu, Haohui Liu, Xuankai Chang, Hamza Khalid, Minseon Gwak, Kawon Lee, Minjeong Kim, B. Raj; Abstract: In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assistants, and telecommunication.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A multi-task learning framework forVoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement and outperforms both industry performance and state-of-the-art methods for speech Enhancement on VoIP applications is proposed.'} Author (LTI's Professor): Bhiksha Raj; Title: Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms; Authors: Ankit Shah, Shuyi Chen, Kejun Zhou, Yue Chen, B. Raj; Abstract: General-purpose embedding is highly desirable for few-shot even zero-shot learning in many application scenarios, including audio tasks. In order to understand representations better, we conducted a thorough error analysis and visualization of HEAR 2021 submission results. Inspired by the analysis, this work experiments with different front-end audio preprocessing methods, including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT), and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover a more holistic simulation of the frequency information received by the human auditory system. We tested the models on the suite of HEAR 2021 tasks, which encompass a broad category of tasks. Preliminary results show (1) the proposed BECR can incur a more dispersed embedding on the test set, (2) BECR improves the PaSST model without extra computation complexity, and (3) STFT preprocessing outperforms CQT in all tasks we tested. Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Experiments with different front-end audio preprocessing methods are experiments, and a Batch Embedding Covariance Regularization (BECR) term is proposed to uncover a more holistic simulation of the frequency information received by the human auditory system.'} Author (LTI's Professor): Bhiksha Raj; Title: Training on Foveated Images Improves Robustness to Adversarial Attacks; Authors: Muhammad A Shah, B. Raj; Abstract: Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks -- subtle, perceptually indistinguishable perturbations of inputs that change the response of the model. In the context of vision, we hypothesize that an important contributor to the robustness of human visual perception is constant exposure to low-fidelity visual stimuli in our peripheral vision. To investigate this hypothesis, we develop \\\\RBlur, an image transform that simulates the loss in fidelity of peripheral vision by blurring the image and reducing its color saturation based on the distance from a given fixation point. We show that compared to DNNs trained on the original images, DNNs trained on images transformed by \\\\RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\\\\% higher accuracy on perturbed data.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'DNNs trained on images transformed by \\\\\\\\RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\\\\\\\\% higher accuracy on perturbed data.'} Author (LTI's Professor): Bhiksha Raj; Title: Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations; Authors: Hao Chen, Ankit Shah, Jindong Wang, R. Tao, Yidong Wang, Xingxu Xie, Masashi Sugiyama, Rita Singh, B. Raj; Abstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \\\\textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Imprecise label learning (ILL) is introduced, a framework for the unification of learning with various imprecise label configurations, marking the first unified framework with robust and effective performance across various challenging settings.'} Author (LTI's Professor): Bhiksha Raj; Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features; Authors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj; Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.; Year: 2023; Venue: Interspeech; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes an analysis pipeline to help explore the voice-face relationship in a fine-grained manner, i.s. phonemes v. facial anthropometric measurements (AM), and indicates that AMs are more predictable from vowels compared to consonants, particularly with plosives.'} Author (LTI's Professor): Bhiksha Raj; Title: Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments; Authors: Thanh-Dat Truong, Hoang-Quan Nguyen, B. Raj, Khoa Luu; Abstract: Continual semantic segmentation aims to learn new classes while maintaining the information from the previous classes. Although prior studies have shown impressive progress in recent years, the fairness concern in the continual semantic segmentation needs to be better addressed. Meanwhile, fairness is one of the most vital factors in deploying the deep learning model, especially in human-related or safety applications. In this paper, we present a novel Fairness Continual Learning approach to the semantic segmentation problem. In particular, under the fairness objective, a new fairness continual learning framework is proposed based on class distributions. Then, a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning, i.e., catastrophic forgetting and background shift. Our proposed loss has also been proven as a novel, generalized learning paradigm of knowledge distillation commonly used in continual learning. Moreover, the proposed Conditional Structural Consistency loss further regularized the structural constraint of the predicted segmentation. Our proposed approach has achieved State-of-the-Art performance on three standard scene understanding benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairness of the segmentation model.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel Fairness Continual Learning approach to the semantic segmentation problem is presented, in particular, a new fairness continual learning framework is proposed based on class distributions and a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning.'} Author (LTI's Professor): Bhiksha Raj; Title: PaintSeg: Training-free Segmentation via Painting; Authors: Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang, B. Raj; Abstract: The paper introduces PaintSeg, a new unsupervised method for segmenting objects without any training. We propose an adversarial masked contrastive painting (AMCP) process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models. During the painting process, inpainting and outpainting are alternated, with the former masking the foreground and filling in the background, and the latter masking the background while recovering the missing part of the foreground object. Inpainting and outpainting, also referred to as I-step and O-step, allow our method to gradually advance the target segmentation mask toward the ground truth without supervision or training. PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points. Our experimental results demonstrate that PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks, providing a training-free solution suitable for unsupervised segmentation.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An adversarial masked contrastive painting process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models, providing a training-free solution suitable for unsupervised segmentation.'} Author (LTI's Professor): Bhiksha Raj; Title: TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement; Authors: YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj; Abstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work shows that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility and uses data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from the method.'} Author (LTI's Professor): Bhiksha Raj; Title: Improving sound event detection with ontologies; Authors: B. Raj; Abstract: Sound event recognition is the task of identifying and categorizing sounds in audio data. Automated algorithms for sound event recognition depend on having explicit models for individual sound event types to be recognized, which are trained on data tagged explicitly for those classes. The approach is data hungryand is fundamentally limited by the number of classes for which such data may be obtained. It also ignores the relationship between sounds being modeled. In this work, we attempt to address these deficiencies through the use of a human-generated sound ontology which represents sibling and parent\\u2013child relations between sound classes. We incorporate the relationships in the ontology through the design of an appropriate \\u201closs\\u201d function (the objective function optimized to train sound-classifier models) that incorporates the relationships in the ontology, and through appropriate model update rules which utilize data from a class to update parameters (of both ontological siblings and parents). Through experiments run on the \\u201cAudioset\\u201d (a popular, large-scale dataset of 600 sound categories), we find that better-performing models can be trained for sound classes with a given dataset, and that the amount of new data required to train models for a novel sound class can be significantly reduced.; Year: 2023; Venue: Journal of the Acoustical Society of America; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Through experiments run on the \\u201cAudioset\\u201d, it is found that better-performing models can be trained for sound classes with a given dataset, and that the amount of new data required to train models for a novel sound class can be significantly reduced.'} Author (LTI's Professor): Bhiksha Raj; Title: Synergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session; Authors: L. Heller, Benjamin Elizalde, B. Raj, Soham Deshmukh; Abstract: Machine Listening, as usually formalized, attempts to perform a task that is, from our perspective, fundamentally human-performable, and performed by humans. Current automated models of Machine Listening vary from purely data-driven approaches to approaches imitating human systems. In recent years, the most promising approaches have been hybrid in that they have used data-driven approaches informed by models of the perceptual, cognitive, and semantic processes of the human system. Not only does the guidance provided by models of human perception and domain knowledge enable better, and more generalizable Machine Listening, in the converse, the lessons learned from these models may be used to verify or improve our models of human perception themselves. This paper summarizes advances in the development of such hybrid approaches, ranging from Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds. The research described herein was presented in a special session on\\\"Synergy between human and machine approaches to sound/scene recognition and processing\\\"at the 2023 ICASSP meeting.; Year: 2023; Venue: arXiv.org; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Advances in the development of hybrid approaches to Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds are summarized.'} Author (LTI's Professor): Carolyn Ros\\u00e9; Title: High school students\\u2019 data modeling practices and processes: from modeling unstructured data to evaluating automated decisions; Authors: Shiyan Jiang, Hengtao Tang, Can Tatar, C. Ros\\u00e9, J. Chao; Abstract: ABSTRACT It\\u2019s critical to foster artificial intelligence (AI) literacy for high school students, the first generation to grow up surrounded by AI, to understand working mechanism of data-driven AI technologies and critically evaluate automated decisions from predictive models. While efforts have been made to engage youth in understanding AI through developing machine learning models, few provided in-depth insights into the nuanced learning processes. In this study, we examined high school students\\u2019 data modeling practices and processes. Twenty-eight students developed machine learning models with text data for classifying negative and positive reviews of ice cream stores. We identified nine data modeling practices that describe students\\u2019 processes of model exploration, development, and testing and two themes about evaluating automated decisions from data technologies. The results provide implications for designing accessible data modeling experiences for students to understand data justice as well as the role and responsibility of data modelers in creating AI technologies.; Year: 2023; Venue: Journal of Educational Media; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It\\u2019s critical to foster artificial intelligence literacy for high school students to understand working mechanism of data-driven AI technologies and critically evaluate automated decisions from predictive models.'} Author (LTI's Professor): Carolyn Ros\\u00e9; Title: Linguistic representations for fewer-shot relation extraction across domains; Authors: Sireesh Gururaja, Ritam Dutt, Ting-gen Liao, C. Ros\\u00e9; Abstract: Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolds on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic representations enhance generalizability by providing features that function as cross-domain pivots. We focus on the task of relation extraction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer-based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains. We find that while the inclusion of these graphs results in significantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work explores the impact of linguistic representations on cross-domain performance in a few-shot transfer setting, and investigates whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains.'} Author (LTI's Professor): Carolyn Ros\\u00e9; Title: Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning; Authors: Armineh Nourbakhsh, Sameena Shah, C. Ros\\u00e9; Abstract: In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast. Instead of a data augmentation approach, CounterComp is based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels. Our proposed auxiliary metric learning loss improves the performance of three state of the art models on four recently released datasets. We also show how the approach can improve OOD performance on unseen domains, as well as unseen compositions. Lastly, we demonstrate how the method can lead to better compositional attention patterns during training.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels.'} Author (LTI's Professor): Carolyn Ros\\u00e9; Title: Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models; Authors: James Fiacco, David Adamson, C. Ros\\u00e9; Abstract: By aligning the functional components derived from the activations of transformer models trained for AES with external knowledge such as human-understandable feature groups, the proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for performing such analyses on further neural AES systems. The analysis focuses on models trained to score essays based on organization, main idea, support, and language. The findings provide insights into the models\\u2019 decision-making processes, biases, and limitations, contributing to the development of more transparent and reliable AES systems.; Year: 2023; Venue: Workshop on Innovative Use of NLP for Building Educational Applications; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for performing such analyses on further neural AES systems.'} Author (LTI's Professor): Carolyn Ros\\u00e9; Title: Exploring Artificial Intelligence in English Language Arts with StoryQ; Authors: J. Chao, Rebecca Ellis, Shiyan Jiang, C. Ros\\u00e9, W. Finzer, Can Tatar, James Fiacco, Kenia Wiedemann; Abstract: Exploring Artificial Intelligence (AI) in English Language Arts (ELA) with StoryQ is a 10-hour curriculum module designed for high school ELA classes. The module introduces students to fundamental AI concepts and essential machine learning workflow using StoryQ, a web-based GUI environment for Grades 6-12 learners. In this module, students work with unstructured text data and learn to train, test, and improve text classification models such as intent recognition, clickbait filter, and sentiment analysis. As they interact with machine-learning language models deeply, students also gain a nuanced understanding of language and how to wield it, not just as a data structure, but as a tool in our human-human encounters as well. The current version contains eight lessons, all delivered through a full-featured online learning and teaching platform. Computers and Internet access are required to implement the module. The module was piloted in an ELA class in the Spring of 2022, and the student learning outcomes were positive. The module is currently undergoing revision and will be further tested and improved in Fall 2022.; Year: 2023; Venue: AAAI Conference on Artificial Intelligence; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Alexander Rudnicky; Title: Advancing Regular Language Reasoning in Linear Recurrent Neural Networks; Authors: Ting-Han Fan, Ta-Chung Chi, Alexander I. Rudnicky; Abstract: In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work theoretically analyze some existing LRNNs and proposes a new LRNN equipped with a block-diagonal and input-dependent transition matrix that is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.'} Author (LTI's Professor): Alexander Rudnicky; Title: Structured Dialogue Discourse Parsing; Authors: Ta-Chung Chi, Alexander I. Rudnicky; Abstract: Dialogue discourse parsing aims to uncover the internal structure of a multi-participant conversation by finding all the discourse links and corresponding relations. Previous work either treats this task as a series of independent multiple-choice problems, in which the link existence and relations are decoded separately, or the encoding is restricted to only local interaction, ignoring the holistic structural information. In contrast, we propose a principled method that improves upon previous work from two perspectives: encoding and decoding. From the encoding side, we perform structured encoding on the adjacency matrix followed by the matrix-tree learning algorithm, where all discourse links and relations in the dialogue are jointly optimized based on latent tree-level distribution. From the decoding side, we perform structured inference using the modified Chiu-Liu-Edmonds algorithm, which explicitly generates the labeled multi-root non-projective spanning tree that best captures the discourse structure. In addition, unlike in previous work, we do not rely on hand-crafted features; this improves the model\\u2019s robustness. Experiments show that our method achieves new state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).; Year: 2023; Venue: SIGDIAL Conferences; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a principled method that improves upon previous work from two perspectives: encoding and decoding and achieves new state-of-the-art results, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).'} Author (LTI's Professor): Alexander Rudnicky; Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech; Authors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky; Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.; Year: 2023; Venue: AAAI Conference on Artificial Intelligence; Citations: 17; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces the MQTTS system, a Text-to-Speech system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality, and shows that MqTTS outperforms existing TTS systems in several objective and subjective measures.'} Author (LTI's Professor): Alexander Rudnicky; Title: Tartan: an LLM Driven SocialBot; Authors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky; Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users; Year: 2023; Venue: ; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work has laid the groundwork for looking more closely at using LLMs in a conversational system and analyzes common patterns in conversations the authors' bot has had with users.\\\"} Author (LTI's Professor): Alexander Rudnicky; Title: Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4; Authors: Mario Rodr'iguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, Jo\\u00e3o Sedoc, L. F. D\\u2019Haro, Alexander I. Rudnicky; Abstract: The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics\\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.; Year: 2023; Venue: DSTC; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The datasets and baselines provided to participants are described and the submission and result details of the two proposed subtasks are discussed, which promote robust and multilingual automatic evaluation metrics.'} Author (LTI's Professor): Alexander Rudnicky; Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking; Authors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, Jo\\u00e3o Magalh\\u00e3es; Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.; Year: 2023; Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents a method for performing zero-shot Dialogue State Tracking by casting the task as a learning-to-ask-questions framework that outperforms template-based question generation and shows that QG methods need to be aligned with the same grammatical person used in the dialogue.'} Author (LTI's Professor): Maarten Sap; Title: FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions; Authors: Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, R. L. Bras, Gunhee Kim, Yejin Choi, Maarten Sap; Abstract: Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'FANToM is introduced, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering that is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain- of-thought reasoning or fine-tuning.'} Author (LTI's Professor): Maarten Sap; Title: Modeling Empathic Similarity in Personal Narratives; Authors: Jocelyn Shen, Maarten Sap, Pedro Colon-Hernandez, Hae Won Park, C. Breazeal; Abstract: The most meaningful connections between people are often fostered through expression of shared vulnerability and emotional experiences in personal narratives. We introduce a new task of identifying similarity in personal stories based on empathic resonance, i.e., the extent to which two people empathize with each others' experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP. Using insights from social psychology, we craft a framework that operationalizes empathic similarity in terms of three key features of stories: main events, emotional trajectories, and overall morals or takeaways. We create EmpathicStories, a dataset of 1,500 personal stories annotated with our empathic similarity features, and 2,000 pairs of stories annotated with empathic similarity scores. Using our dataset, we fine-tune a model to compute empathic similarity of story pairs, and show that this outperforms semantic similarity models on automated correlation and retrieval metrics. Through a user study with 150 participants, we also assess the effect our model has on retrieving stories that users empathize with, compared to naive semantic similarity-based retrieval, and find that participants empathized significantly more with stories retrieved by our model. Our work has strong implications for the use of empathy-aware models to foster human connection and empathy between people.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"A new task of identifying similarity in personal stories based on empathic resonance is introduced, i.e., the extent to which two people empathize with each others' experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP.\\\"} Author (LTI's Professor): Maarten Sap; Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements; Authors: Xuhui Zhou, Haojie Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap; Abstract: Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance\\\"your English is very good\\\"may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'COBRA frames are introduced, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context, and the importance and feasibility of contextualized NLP by modeling social factors are highlighted.'} Author (LTI's Professor): Maarten Sap; Title: Riveter: Measuring Power and Social Dynamics Between Entities; Authors: Maria Antoniak, Anjalie Field, Jimin Mun, Melanie Walsh, Lauren F. Klein, Maarten Sap; Abstract: Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research by organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions.'} Author (LTI's Professor): Maarten Sap; Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting; Authors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap; Abstract: None; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: None Author (LTI's Professor): Maarten Sap; Title: What are Adapters Really Efficient At?; Authors: Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Mitesh M. Khapra, Pratyush Kumar, V. Rudra, Murthy Anoop, Kunchukuttan. 2022, Naama-677, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir R. Radev, Al-683 ham, Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Jonas Pfeiffer, Aishwarya Kamath, Andreas R\\u00fcckl\\u00e9, Kyunghyun Cho, Iryna Gurevych, Clifton Poth, Aishwarya, Ivan Kamath, Sebastian Vuli\\u00b4c, Kyunghyun Ruder, Gregor Geigle, Max Glockner, Jonas Beck, Nils Pfeiffer, Reimers Iryna, Victor Sanh, Colin Raffel, Lintang Bach, Zaid Sutawika, Antoine Alyafeai, Arnaud Chaffin, Arun Stiegler, Manan Raja, Dey Saiful, Canwen Bari, Urmish Xu, Thakker, Shanya Sharma, Eliza Szczechla, Taewoon, Gunjan Kim, Nihal Chhablani, Nayak, Debajyoti, Jonathan Datta, Mike Tian-Jian Chang, Han Jiang, Matteo Wang, S. Manica, Zheng Xin Shen, Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Tripathi Neeraj, Jos Rozen, Abheesht Sharma, A. Santilli, Thibault F\\u00e9vry, Jason Alan Fries, Maarten Sap, Hannah Rashkin, Derek Chen, Ronan, Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Adam R. Brown, Adam Santoro, Adri\\u00e0 Gupta, Agnieszka Garriga-Alonso, Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par-765, Allen Nie, Aman Hussain, Amanda Askell, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, A. Santilli, Andreas Stuhlm\\u00fcller, Andrew M. Dai, Andrew La, Andrew Lampinen, Angela Zou, Angelica Jiang, Anh Chen, Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-773, Arul Menezes, Arun Kirubarajan, Asher Mul-774, Ashish lokandov, Austin Sabharwal, Herrick, Avia, A. Efrat, Ayla Erdem, B. Karaka\\u00b8s, Ryan Roberts, B. S. Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan \\u00d6zyurt, Behnam Hedayatnia, Behnam, Benjamin Neyshabur, Benno Inden, Berk Stein, Ek-779 mekci, Bill Yuchen, Blake Lin, Cameron Howald, Cameron Diao, Catherine Dour, Cedrick Stinson, Ar-781 C\\u00e9sar, Chandan Ferri Ram\\u00edrez, Charles Singh, Christopher D. Manning, Christopher Potts, Cindy 785 Ramirez, Clara Rivera, Clemencia Siro, Colin Raf-786, Courtney Ashcraft, Cristina Garbacea, Dan Sileo, Daniel H Garrette, Dan Hendrycks, Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\\u00ed Gonz\\u00e1lez, Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, D. Gilboa, David Dohan, D. Drakard, David Ju-792, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Tam, m\\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-807 L\\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijase-808 vic, Hannah Kim, Harsh Mehta, H. Bogar, Henry Shevlin, Hinrich Sch\\u00fctze, H. Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jae-813 hoon Lee, J. Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Ko-815 co\\u00b4n, Jana Thompson, Jared Kaplan, Jarema Radom, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, K. Markert, Kaustubh D. Dhole, Kevin Gim-827 pel, Kevin Omondi, K. Mathewson, Kristen Chi-828 afullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-829 Donell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras Ochando, Louis-Philippe Morency, Luca Moschella, Maarten \\u00b8Senel, Maarten Bosma, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Jose Maru, Marie Ram\\u00edrez Quintana, Tolkiehn Mario, Martha Giulianelli, Martin Lewis, L. PotthastMatthew, Matthew L. Leavitt, M\\u00e1ty\\u00e1s Schu-840 bert Hagen, Medina Orduna, Melody Baitemirova, Arnaud Melvin, Michael A McElrath, Michael Yee, Michael Co-842 hen, Michael Gu, M. Ivanitskiy, Michael Star-843 ritt, M. Strube, Michele Sw\\u02dbedrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mo Suzgun, Monica Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T. Varma, Nanyun Peng, tish Shirish Keskar, Niveditha Iyer, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio, Moreno Casares, Parth Doshi, Jason Wei, Maarten Bosma, Vincent Y. Zhao, Adams Wei Guu, Brian Yu, Nan Lester, An-921 Du, M. Dai, Quoc V. Le, Finetuned, Adina Williams, Nikita Nangia, Samuel R. Bowman, Thomas Wolf, Lysandre Debut, Clement Chaumond, Anthony Delangue, Pier-339 Moi, Tim ric Cistac, R\\u00e9mi Rault, Morgan Louf, Funtow-900 Joe, Sam Davison, Patrick Shleifer, von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Sylvain Gugger, Mariama Drame, Yinfei Yang, Yuan Zhang, Chris Tar, Hailey Schoelkopf, Niklas Muen-954, Alham Fikri, David Ifeoluwa Adelani, M Saiful Bari, Lintang Sutawika, Zhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo, Jonathan May; Abstract: Adapters have been positioned as a parameter-001 efficient fine-tuning (PEFT) approach, whereby 002 a minimal number of parameters are added to 003 the model and fine-tuned. However, adapters 004 have not been sufficiently analyzed to under-005 stand if PEFT translates to benefits in train-006 ing/deployment efficiency and maintainabil-007 ity/extensibility. Through extensive experi-008 ments on many adapters, tasks, and languages 009 in supervised and cross-lingual zero-shot set-010 tings, we clearly show that for Natural Lan-011 guage Understanding tasks, the parameter ef-012 ficiency in adapters does not translate to effi-013 ciency gains compared to full fine-tuning of 014 models. More precisely, adapters are relatively 015 expensive to train and have slightly higher de-016 ployment latency. Furthermore, the maintain-017 ability /extensibility benefits of adapters can be 018 achieved with simpler approaches like multi-019 task training via full fine-tuning, which also 020 provide relatively faster training times. We, 021 therefore, recommend that for moderately sized 022 models practitioners should rely on full fine-023 tuning or multi-task training rather than using 024 adapters. 025; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is recommended that for moderately sized models practitioners should rely on full fine-023 tuning or multi-task training rather than using adapters, as adapters are relatively expensive to train and have slightly higher de-016 ployment latency.'} Author (LTI's Professor): Maarten Sap; Title: Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language; Authors: Jimin Mun, Emily Allaway, Akhila Yerukola, Laura Vianna, Sarah-Jane Leslie, Maarten Sap; Abstract: Counterspeech, i.e., responses to counteract potential harms of hateful speech, has become an increasingly popular solution to address online hate speech without censorship. However, properly countering hateful language requires countering and dispelling the underlying inaccurate stereotypes implied by such language. In this work, we draw from psychology and philosophy literature to craft six psychologically inspired strategies to challenge the underlying stereotypical implications of hateful language. We first examine the convincingness of each of these strategies through a user study, and then compare their usages in both human- and machine-generated counterspeech datasets. Our results show that human-written counterspeech uses countering strategies that are more specific to the implied stereotype (e.g., counter examples to the stereotype, external factors about the stereotype's origins), whereas machine-generated counterspeech uses less specific strategies (e.g., generally denouncing the hatefulness of speech). Furthermore, machine-generated counterspeech often employs strategies that humans deem less convincing compared to human-produced counterspeech. Our findings point to the importance of accounting for the underlying stereotypical implications of speech when generating counterspeech and for better machine reasoning about anti-stereotypical examples.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 2; TLDR: None Author (LTI's Professor): Maarten Sap; Title: BiasX: \\\"Thinking Slow\\\" in Toxic Content Moderation with Explanations of Implied Social Biases; Authors: Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap; Abstract: Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, is introduced and it is shown that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content.\\\"} Author (LTI's Professor): Maarten Sap; Title: Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory; Authors: Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi; Abstract: The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.; Year: 2023; Venue: arXiv.org; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs, and underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.'} Author (LTI's Professor): Maarten Sap; Title: Improving Language Models with Advantage-based Offline Policy Gradients; Authors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark O. Riedl; Abstract: Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data. We also release our experimental code. https://github.com/abaheti95/LoL-RL; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data that assumes the entire LM output sequence as a single action, and allows incorporating sequence-level classifiers or human-designed scoring functions as rewards.'} Author (LTI's Professor): Maarten Sap; Title: From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models; Authors: Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap; Abstract: Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word \\u201ccosmopolitan\\u201d in a sentence such as \\u201cwe need to end the cosmopolitan experiment\\u201d can mean \\u201cworldly\\u201d to many but also secretly mean \\u201cJewish\\u201d to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians\\u2019 speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3\\u2019s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources to facilitate future research in modeling dogwhistles and mitigating their online harms.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 7; TLDR: None Author (LTI's Professor): Maarten Sap; Title: NLPositionality: Characterizing Design Biases of Datasets and Models; Authors: Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap; Abstract: Design biases in NLP systems, such as performance differences for different populations, often stem from their creator\\u2019s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks\\u2014social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries.We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 17; TLDR: {'model': 'tldr@v2.0.0', 'text': 'NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models, is introduced and it is found that dataset and models align predominantly with Western, White, college-educated, and younger populations.'} Author (LTI's Professor): Maarten Sap; Title: Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting; Authors: Akhila Yerukola, Xuhui Zhou, Maarten Sap; Abstract: Most existing stylistic text rewriting methods and evaluation metrics operate on a sentence level, but ignoring the broader context of the text can lead to preferring generic, ambiguous, and incoherent rewrites. In this paper, we investigate integrating the preceding textual context into both the $\\\\textit{rewriting}$ and $\\\\textit{evaluation}$ stages of stylistic text rewriting, and introduce a new composite contextual evaluation metric $\\\\texttt{CtxSimFit}$ that combines similarity to the original sentence with contextual cohesiveness. We comparatively evaluate non-contextual and contextual rewrites in formality, toxicity, and sentiment transfer tasks. Our experiments show that humans significantly prefer contextual rewrites as more fitting and natural over non-contextual ones, yet existing sentence-level automatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences ($\\\\rho$=0--0.3). In contrast, human preferences are much better reflected by both our novel $\\\\texttt{CtxSimFit}$ ($\\\\rho$=0.7--0.9) as well as proposed context-infused versions of common metrics ($\\\\rho$=0.4--0.7). Overall, our findings highlight the importance of integrating context into the generation and especially the evaluation stages of stylistic text rewriting.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A new composite contextual evaluation metric is introduced that combines similarity to the original sentence with contextual cohesiveness and highlights the importance of integrating context into the generation and especially the evaluation stages of stylistic text rewriting.'} Author (LTI's Professor): Maarten Sap; Title: Where Do People Tell Stories Online? Story Detection Across Online Communities; Authors: Maria Antoniak, Joel Mire, Maarten Sap, Elliott Ash, Andrew Piper; Abstract: Story detection in online communities is a challenging task as stories are scattered across communities and interwoven with non-storytelling spans within a single text. We address this challenge by building and releasing the StorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts and comments, a detailed codebook adapted to the social media context, and models to predict storytelling at the document and span level. Our dataset is sampled from hundreds of popular English-language Reddit communities ranging across 33 topic categories, and it contains fine-grained expert annotations, including binary story labels, story spans, and event spans. We evaluate a range of detection methods using our data, and we identify the distinctive textual features of online storytelling, focusing on storytelling span detection, which we introduce as a new task. We illuminate distributional characteristics of storytelling on a large community-centric social media platform, and we also conduct a case study on r/ChangeMyView, where storytelling is used as one of many persuasive strategies, illustrating that our data and models can be used for both inter- and intra-community research. Finally, we discuss implications of our tools and analyses for narratology and the study of online communities.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The StorySeeker toolkit is built, including a richly annotated dataset of 502 Reddit posts and comments, a detailed codebook adapted to the social media context, and models to predict storytelling at the document and span level, to identify the distinctive textual features of online storytelling.'} Author (LTI's Professor): Maarten Sap; Title: Queer In AI: A Case Study in Community-Led Participatory AI; Authors: AI OrganizersOfQueerin, Anaelia Ovalle, Arjun Subramonian, Ashwin Singh, C. Voelcker, Danica J. Sutherland, Davide Locatelli, Eva Breznik, Filip Klubicka, Hang Yuan, J. Hetvi, Huan Zhang, Jaidev Shriram, Kruno Lehman, Luca Soldaini, Maarten Sap, M. Deisenroth, Maria Leonor Pacheco, Maria Ryskina, Martin Mundt, M. Agarwal, Nyx McLean, Pan Xu, Pranav A, Raj Korpan, Ruchira Ray, Sarah Mathew, Sarthak Arora, S. T. John, Tanvi Anand, Vishakha Agrawal, William Agnew, Yanan Long, Zijie J. Wang, Zeerak Talat, Avijit Ghosh, N. Dennler, Michael Noseworthy, Sharvani Jha, Emi Baylor, Aditya Joshi, Natalia Y. Bilenko, Andrew McNamara, Raphael Gontijo-Lopes, Alex Markham, Evyn D\\u01d2ng, J. Kay, Manu Saraswat, Nikhil Vytla, Luke Stark; Abstract: Queerness and queer people face an uncertain future in the face of ever more widely deployed and invasive artificial intelligence (AI). These technologies have caused numerous harms to queer people, including privacy violations, censoring and downranking queer content, exposing queer people and spaces to harassment by making them hypervisible, deadnaming and outing queer people. More broadly, they have violated core tenets of queerness by classifying and controlling queer identities. In response to this, the queer community in AI has organized Queer in AI, a global, decentralized, volunteer-run grassroots organization that employs intersectional and community-led participatory design to build an inclusive and equitable AI future. In this paper, we present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community\\u2019s programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization\\u2019s impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative participatory practices, and bringing participation to institutions outside of individual research projects. Queer in AI\\u2019s work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods.; Year: 2023; Venue: Conference on Fairness, Accountability and Transparency; Citations: 12; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper examines how participatory design and intersectional tenets started and shaped this community\\u2019s programs over the years, and discusses different challenges that emerged in the process, and looks at ways this organization has fallen short of operationalizing participatory and intersectionsal principles.'} Author (LTI's Professor): Maarten Sap; Title: Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties; Authors: Taylor Sorensen, Liwei Jiang, Jena D. Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, Maarten Sap, J. Tasioulas, Yejin Choi; Abstract: Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT-4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.; Year: 2023; Venue: arXiv.org; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Kaleido is built, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence of human values, rights, and duties within a specific context and demonstrates that Kaleido can help explain variability in human decision-making by outputting contrasting values.'} Author (LTI's Professor): Maarten Sap; Title: Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models; Authors: Natalie Shapira, Mosh Levy, S. Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz; Abstract: The escalating debate on AI's capabilities warrants developing reliable metrics to assess machine\\\"intelligence\\\". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.; Year: 2023; Venue: arXiv.org; Citations: 41; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is found that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust, indicating reliance on shallow heuristics rather than robust ToM abilities.'} Author (LTI's Professor): Maarten Sap; Title: Towards Countering Essentialism through Social Bias Reasoning; Authors: Emily Allaway, Nina Taneja, S. Leslie, Maarten Sap; Abstract: Essentialist beliefs (i.e., believing that members of the same group are fundamentally alike) play a central role in social stereotypes and can lead to harm when left unchallenged. In our work, we conduct exploratory studies into the task of countering essentialist beliefs (e.g., ``liberals are stupid''). Drawing on prior work from psychology and NLP, we construct five types of counterstatements and conduct human studies on the effectiveness of these different strategies. Our studies also investigate the role in choosing a counterstatement of the level of explicitness with which an essentialist belief is conveyed. We find that statements that broaden the scope of a stereotype (e.g., to other groups, as in ``conservatives can also be stupid'') are the most popular countering strategy. We conclude with a discussion of challenges and open questions for future work in this area (e.g., improving factuality, studying community-specific variation) and we emphasize the importance of work at the intersection of NLP and psychology.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: None Author (LTI's Professor): Maarten Sap; Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents; Authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap; Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.; Year: 2023; Venue: arXiv.org; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': 'SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence, is presented and it is found that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.'} Author (LTI's Professor): Rita Singh; Title: BASS: Block-wise Adaptation for Speech Summarization; Authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj; Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.; Year: 2023; Venue: Interspeech; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'} Author (LTI's Professor): Rita Singh; Title: Rethinking Voice-Face Correlation: A Geometry View; Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, B. Raj; Abstract: Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.; Year: 2023; Venue: ACM Multimedia; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction and finds significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium.'} Author (LTI's Professor): Rita Singh; Title: A Gene-Based Algorithm for Identifying Factors That May Affect a Speaker\\u2019s Voice; Authors: Rita Singh; Abstract: Over the past decades, many machine-learning- and artificial-intelligence-based technologies have been created to deduce biometric or bio-relevant parameters of speakers from their voice. These voice profiling technologies have targeted a wide range of parameters, from diseases to environmental factors, based largely on the fact that they are known to influence voice. Recently, some have also explored the prediction of parameters whose influence on voice is not easily observable through data-opportunistic biomarker discovery techniques. However, given the enormous range of factors that can possibly influence voice, more informed methods for selecting those that may be potentially deducible from voice are needed. To this end, this paper proposes a simple path-finding algorithm that attempts to find links between vocal characteristics and perturbing factors using cytogenetic and genomic data. The links represent reasonable selection criteria for use by computational by profiling technologies only, and are not intended to establish any unknown biological facts. The proposed algorithm is validated using a simple example from medical literature\\u2014that of the clinically observed effects of specific chromosomal microdeletion syndromes on the vocal characteristics of affected people. In this example, the algorithm attempts to link the genes involved in these syndromes to a single example gene (FOXP2) that is known to play a broad role in voice production. We show that in cases where strong links are exposed, vocal characteristics of the patients are indeed reported to be correspondingly affected. Validation experiments and subsequent analyses confirm that the methodology could be potentially useful in predicting the existence of vocal signatures in na\\u00efve cases where their existence has not been otherwise observed.; Year: 2023; Venue: Entropy; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A simple path-finding algorithm that attempts to find links between vocal characteristics and perturbing factors using cytogenetic and genomic data and shows that in cases where strong links are exposed, vocal characteristics of the patients are indeed reported to be correspondingly affected.'} Author (LTI's Professor): Rita Singh; Title: Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation; Authors: Wayne Zhao, Rita Singh; Abstract: During phonation, the vocal folds exhibit a self-sustained oscillatory motion, which is influenced by the physical properties of the speaker\\u2019s vocal folds and driven by the balance of bio-mechanical and aerodynamic forces across the glottis. Subtle changes in the speaker\\u2019s physical state can affect voice production and alter these oscillatory patterns. Measuring these can be valuable in developing computational tools that analyze voice to infer the speaker\\u2019s state. Traditionally, vocal fold oscillations (VFOs) are measured directly using physical devices in clinical settings. In this paper, we propose a novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker-by-speaker basis. The approach, called the ADLES-VFT algorithm, is proposed in the context of a joint model that combines a phonation model (with a glottal flow waveform as the output) and a vocal tract acoustic wave propagation model such that the output of the joint model is an estimated waveform. The ADLES-VFT algorithm is a forward-backward algorithm which minimizes the error between the recorded waveform and the output of this joint model to estimate its parameters. Once estimated, these parameter values are used in conjunction with a phonation model to obtain its solutions. Since the parameters correlate with the physical properties of the vocal folds of the speaker, model solutions obtained using them represent the individualized VFOs for each speaker. The approach is flexible and can be applied to various phonation models. In addition to presenting the methodology, we show how the VFOs can be quantified from a dynamical systems perspective for classification purposes. Mathematical derivations are provided in an appendix for better readability.; Year: 2023; Venue: Entropy; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker- by-speaker basis is proposed and it is shown how the V FOs can be quantified from a dynamical systems perspective for classification purposes.'} Author (LTI's Professor): Rita Singh; Title: Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations; Authors: Hao Chen, Ankit Shah, Jindong Wang, R. Tao, Yidong Wang, Xingxu Xie, Masashi Sugiyama, Rita Singh, B. Raj; Abstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \\\\textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Imprecise label learning (ILL) is introduced, a framework for the unification of learning with various imprecise label configurations, marking the first unified framework with robust and effective performance across various challenging settings.'} Author (LTI's Professor): Rita Singh; Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features; Authors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj; Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.; Year: 2023; Venue: Interspeech; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes an analysis pipeline to help explore the voice-face relationship in a fine-grained manner, i.s. phonemes v. facial anthropometric measurements (AM), and indicates that AMs are more predictable from vowels compared to consonants, particularly with plosives.'} Author (LTI's Professor): Rita Singh; Title: Pengi: An Audio Language Model for Audio Tasks; Authors: Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang; Abstract: In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question&Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding; Year: 2023; Venue: Neural Information Processing Systems; Citations: 34; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Pengi is introduced, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks, and shows that connecting language models with audio models is a major step towards general-purpose audio understanding.'} Author (LTI's Professor): Emma Strubell; Title: To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing; Authors: Sireesh Gururaja, Amanda Bertsch, Clara Na, D. Widder, Emma Strubell; Abstract: NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as well as new shifts without historical parallel, including changes in benchmark culture and software infrastructure. We complement this discussion with quantitative analysis of citation, authorship, and language use in the ACL Anthology over time. We conclude by discussing shared visions, concerns, and hopes for the future of NLP. We hope that this study of our field's past and present can prompt informed discussion of our community's implicit norms and more deliberate action to consciously shape the future.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work conducts long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity to study factors that shape NLP as a field, including culture, incentives, and infrastructure.'} Author (LTI's Professor): Emma Strubell; Title: Energy and Carbon Considerations of Fine-Tuning BERT; Authors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni; Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities is performed to place fine- Tuning energy and carbon costs into perspective with respect to pre-training and inference.'} Author (LTI's Professor): Emma Strubell; Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models; Authors: Gustavo Gon\\u00e7alves, Emma Strubell; Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs finds that longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.'} Author (LTI's Professor): Emma Strubell; Title: Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research; Authors: Ji-Ung Lee, Haritz Puerto, Betty van Aken, Yuki Arase, J. Forde, Leon Derczynski, Andreas Ruckl'e, Iryna Gurevych, Roy Schwartz, Emma Strubell, Jesse Dodge; Abstract: Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found disparities, some of which already successfully implemented. Finally, we discuss additional concerns raised by many participants in free-text responses.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work captures existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process; and provides an analysis and devise recommendations to mitigate found disparities.'} Author (LTI's Professor): Emma Strubell; Title: On the Interactions of Structural Constraints and Data Resources for Structured Prediction; Authors: Zhisong Zhang, Emma Strubell, E. Hovy; Abstract: ,; Year: 2023; Venue: SUSTAINLP; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Emma Strubell; Title: Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation; Authors: Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Pete Walsh, Noah A. Smith, Hannaneh Hajishirzi; Abstract: Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"Pentathlon is a benchmark for holistic and realistic evaluation of model efficiency, which focuses on inference, which accounts for a majority of the compute in a model's lifecycle, and is designed to mirror real-world applications scenarios.\\\"} Author (LTI's Professor): Emma Strubell; Title: Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models; Authors: Harnoor Dhingra, Preetiha Jayashanker, Sayali S. Moghe, Emma Strubell; Abstract: Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.'} Author (LTI's Professor): Emma Strubell; Title: Power Hungry Processing: Watts Driving the Cost of AI Deployment?; Authors: A. Luccioni, Yacine Jernite, Emma Strubell; Abstract: Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of\\\"generality\\\"comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.; Year: 2023; Venue: arXiv.org; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is found that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters.'} Author (LTI's Professor): Emma Strubell; Title: Making Scalable Meta Learning Practical; Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing; Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'} Author (LTI's Professor): Emma Strubell; Title: Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints; Authors: Rajshekhar Das, Jonathan M Francis, Sanket Vaibhav Mehta, Jean Oh, Emma Strubell, Jose Moura; Abstract: Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for semantic segmentation problems. A notable drawback, however, is that this family of approaches is susceptible to erroneous pseudo labels that arise from confirmation biases in the source domain and that manifest as nuisance factors in the target domain. A possible source for this mismatch is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub-optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise conventional self-training objectives. Specifically, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal clustering. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for unsupervised domain adaptation. In this work, we show that our regularizer significantly improves top performing self-training methods (by up to $2$ points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The regularizer significantly improves top performing self-training methods in various UDA benchmarks for semantic segmentation and introduces a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart.'} Author (LTI's Professor): Emma Strubell; Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment; Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell; Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\\\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'} Author (LTI's Professor): Emma Strubell; Title: Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training; Authors: Zhisong Zhang, Emma Strubell, E. Hovy; Abstract: In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work proposes a pragmatic method that reduces the annotation cost for structured label spaces using active learning by adopting an error estimator to adaptively decide the partial selection ratio according to the current model's capability.\\\"} Author (LTI's Professor): Emma Strubell; Title: How To Train Your (Compressed) Large Language Model; Authors: A. Jha, Dirk Groeneveld, Emma Strubell, Iz Beltagy; Abstract: With the increase in the size of large language models (LLMs), we need compression methods that can reduce the model size while preserving the generality and zero-shot promptability of the model. This goal is more ambitious than the typical compression setup, which reduces the model's size at the expense of specializing it to a specific end-task. To study this, we develop a task-agnostic compression pipeline with a large-scale evaluation comprising language modeling perplexity and 12 zero-shot end-tasks. Our results show that a simple layer-wise pruning followed by continued language model pretraining matches or outperforms three existing state-of-the-art baselines while being 1.5x more computationally efficient. However, unlike typical task-specialized compression, our best-compressed model significantly underperforms a similar-sized model trained from scratch. We posit the half-sized pretrained model as an upper bound for task-agnostic compression and call for future work to bridge this gap under a reasonable token budget. Our findings highlight the inadequacy of existing compression methods for LLMs and establish a requirement for new methods that preserve a model's generality and zero-shot promptability under compression. We release our code and evaluation setup to facilitate reproducibility and help iterate on method design.; Year: 2023; Venue: ; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A task-agnostic compression pipeline with a large-scale evaluation comprising language modeling perplexity and 12 zero-shot end-tasks is developed and shows that a simple layer-wise pruning followed by continued language model pretraining matches or outperforms three existing state-of-the-art baselines while being 1.5x more computationally efficient.'} Author (LTI's Professor): Alexander Waibel; Title: SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization; Authors: Tuan-Nam Nguyen, Ngoc-Quan Pham, A. Waibel; Abstract: Conventional multi-speaker text-to-speech synthesis (TTS) is known to be capable of synthesizing speech for multiple voices, yet it cannot generate speech in different accents. This limitation has motivated us to develop SYNTACC (Synthesizing speech with accents) which adapts conventional multi-speaker TTS to produce multi-accent speech. Our method uses the YourTTS model and involves a novel multi-accent training mechanism. The method works by decomposing each weight matrix into a shared component and an accent-dependent component, with the former being initialized by the pretrained multi-speaker TTS model and the latter being factorized into vectors using rank-1 matrices to reduce the number of training parameters per accent. This weight factorization method proves to be effective in fine-tuning the SYNTACC on multi-accent data sets in a low-resource condition. Our SYNTACC model eventually allows speech synthesis in not only different voices but also in different accents.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The SYNTACC model eventually allows speech synthesis in not only different voices but also in different accents, and the weight factorization method proves to be effective in fine-tuning the SYNT ACC on multi-accent data sets in a low-resource condition.'} Author (LTI's Professor): Alexander Waibel; Title: KIT\\u2019s Multilingual Speech Translation System for IWSLT 2023; Authors: Danni Liu, T. Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, A. Waibel, J. Niehues; Abstract: Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk translation, although their performance remains similar on TED talks.; Year: 2023; Venue: International Workshop on Spoken Language Translation; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper describes the speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks, and observes that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules.'} Author (LTI's Professor): Alexander Waibel; Title: Convoifilter: A case study of doing cocktail party speech recognition; Authors: T. Nguyen, A. Waibel; Abstract: This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise (ConVoiFilter) and an ASR module. The model can decrease ASR's word error rate (WER) from 80% to 26.4% through this approach. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning. We openly share our pre-trained model to foster further research hf.co/nguyenvulebinh/voice-filter.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"An end-to-end model designed to improve automatic speech recognition for a particular speaker in a crowded, noisy environment that utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise and an ASR module.\\\"} Author (LTI's Professor): Alexander Waibel; Title: Continually learning new languages; Authors: Ngoc-Quan Pham, J. Niehues, A. Waibel; Abstract: Multilingual speech recognition with neural networks is often implemented with batch-learning, when all of the languages are available before training. An ability to add new languages after the prior training sessions can be economically bene-\\ufb01cial, but the main challenge is catastrophic forgetting. In this work, we combine the qualities of weight factorization, transfer learning and Elastic Weight Consolidation in order to counter catastrophic forgetting and facilitate learning new languages quickly. Such combination allowed us to eliminate catastrophic forgetting while still achieving performance for the new languages comparable with having all languages at once, in experiments of learning from an initial 10 languages to achieve 27 languages.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work combines the qualities of weight factorization, transfer learning and Elastic Weight Consolidation in order to counter catastrophic forgetting and facilitate learning new languages quickly.'} Author (LTI's Professor): Alexander Waibel; Title: Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff; Authors: Peter Pol\\u00e1k, Brian Yan, Shinji Watanabe, A. Waibel, Ondrej Bojar; Abstract: Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \\\\textit{incremental} translation to users. Further, this method lacks mechanisms for \\\\textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.; Year: 2023; Venue: Interspeech; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control is proposed and applied to models trained for online or offline translation and it is demonstrated that both types can be effectively used in online mode.'} Author (LTI's Professor): Alexander Waibel; Title: End-to-End Evaluation for Low-Latency Simultaneous Speech Translation; Authors: Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc-Quan Pham, T. Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, J. Niehues, A. Waibel; Abstract: The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components. Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions and directly compares state-of-the-art cascaded as well as end-to-end systems.'} Author (LTI's Professor): Alexander Waibel; Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN; Authors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, R. Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Est\\u00e8ve, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, D\\u00e1vid Javorsk\\u00fd, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Pol\\u00e1k, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian St\\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, Marco Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos; Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.; Year: 2023; Venue: International Workshop on Spoken Language Translation; Citations: 26; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Shinji Watanabe; Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval; Authors: Sho Miyamoto, Y. Kuroda, T. Kanno, A. Ueno, N. Shiwa-Sudo, N. Iwata-Yoshikawa, Yusuke Sakai, N. Nagata, T. Arashiro, A. Ainai, Saya Moriyama, N. Kishida, Shinji Watanabe, K. Nojima, Y. Seki, T. Mizukami, H. Hasegawa, H. Ebihara, S. Fukushi, Yoshimasa Takahashi, Maeda Ken, Tadaki Suzuki; Abstract: None; Year: 2023; Venue: iScience; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The results highlight the importance of vaccine dosage intervals of 4 months or longer, regardless of the antigenicity of the exposed antigen, to maximize the breadth of serum cross-neutralization covering SARS-CoV-2 Omicron lineages.'} Author (LTI's Professor): Shinji Watanabe; Title: Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning; Authors: William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe; Abstract: Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM\\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than $10 \\\\%$ of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain $94 \\\\%$ of XLS-R\\u2019s performance with only $3 \\\\%$ of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet.; Year: 2023; Venue: Automatic Speech Recognition & Understanding; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes WavLabLM, which extends WavLM\\u2019s joint prediction and denoising to 40k hours of data across 136 languages, and devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data.'} Author (LTI's Professor): Shinji Watanabe; Title: Tensor decomposition for minimization of E2E SLU model toward on-device processing; Authors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe; Abstract: Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and aims to minimize the computational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in our E2E SLU models. We propose to apply singular value decomposition to linear layers and the Tucker decomposition to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposition to the Tucker decomposition. Since the E2E model is represented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 million parameters.; Year: 2023; Venue: Interspeech; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper aims to minimize the computational cost of the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and reduces the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in the E2E SLU models.'} Author (LTI's Professor): Shinji Watanabe; Title: Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation; Authors: E. Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe; Abstract: Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes using a decoder-only architecture for ASR with simple text augmentation training that had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.'} Author (LTI's Professor): Shinji Watanabe; Title: ML-SUPERB: Multilingual Speech Universal PERformance Benchmark; Authors: Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Huang, Xuankai Chang, Shang-Wen Li, Abdel-rahman Mohamed, Hung-yi Lee, Shinji Watanabe; Abstract: Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.; Year: 2023; Venue: Interspeech; Citations: 17; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Shinji Watanabe; Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding; Authors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe; Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 11; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes three task-specific structured pruning methods to deal with heterogeneous speech models that not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning.'} Author (LTI's Professor): Shinji Watanabe; Title: Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization; Authors: Puyuan Peng, Brian Yan, Shinji Watanabe, David F. Harwath; Abstract: We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper; Year: 2023; Venue: Interspeech; Citations: 17; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work investigates the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering, and designs task-specific prompts that improve performance on the three zero-shot tasks and even outperform SotA supervised models on some datasets.'} Author (LTI's Professor): Shinji Watanabe; Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks; Authors: Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe; Abstract: Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.; Year: 2023; Venue: Interspeech; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training.'} Author (LTI's Professor): Shinji Watanabe; Title: Efficient Sequence Transduction by Jointly Predicting Tokens and Durations; Authors: Hainan Xu, Fei Jia, Somshubra Majumdar, Hengguan Huang, Shinji Watanabe, Boris Ginsburg; Abstract: This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.; Year: 2023; Venue: International Conference on Machine Learning; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel Token-and-Duration Transducer architecture for sequence-to-sequence tasks by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token.'} Author (LTI's Professor): Shinji Watanabe; Title: Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders; Authors: Takatomo Kano, A. Ogawa, Marc Delcroix, Roshan Sharma, Kohei Matsuura, Shinji Watanabe; Abstract: Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube), and exploits auxiliary text information from ASR transcriptions to improve the modeling capabilities.'} Author (LTI's Professor): Shinji Watanabe; Title: UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures; Authors: Zhong-Qiu Wang, Shinji Watanabe; Abstract: In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR, an algorithm for over-determined training mixtures that can promote unsupervised separation of speakers.'} Author (LTI's Professor): Shinji Watanabe; Title: Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference; Authors: Masao Someki, N. Eng, Yosuke Higuchi, Shinji Watanabe; Abstract: Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothesis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Experimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy.; Year: 2023; Venue: Automatic Speech Recognition & Understanding; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture, which is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy.'} Author (LTI's Professor): Shinji Watanabe; Title: Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study; Authors: Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, W. El-Hajj, Ahmed Ali; Abstract: .; Year: 2023; Venue: arXiv.org; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Shinji Watanabe; Title: BASS: Block-wise Adaptation for Speech Summarization; Authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj; Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.; Year: 2023; Venue: Interspeech; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'} Author (LTI's Professor): Shinji Watanabe; Title: I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition; Authors: Yifan Peng, Jaesong Lee, Shinji Watanabe; Abstract: Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs and interesting analysis on the gate probabilities and the input-dependency, which helps to better understand deep encoders.'} Author (LTI's Professor): Shinji Watanabe; Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech; Authors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky; Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.; Year: 2023; Venue: AAAI Conference on Artificial Intelligence; Citations: 17; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces the MQTTS system, a Text-to-Speech system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality, and shows that MqTTS outperforms existing TTS systems in several objective and subjective measures.'} Author (LTI's Professor): Shinji Watanabe; Title: A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning; Authors: Jiyang Tang, William Chen, Xuankai Chang, Shinji Watanabe, B. MacWhinney; Abstract: Aphasia is a language disorder that affects the speaking ability of millions of patients. This paper presents a new benchmark for Aphasia speech recognition and detection tasks using state-of-the-art speech recognition techniques with the AphsiaBank dataset. Specifically, we introduce two multi-task learning methods based on the CTC/Attention architecture to perform both tasks simultaneously. Our system achieves state-of-the-art speaker-level detection accuracy (97.3%), and a relative WER reduction of 11% for moderate Aphasia patients. In addition, we demonstrate the generalizability of our approach by applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.; Year: 2023; Venue: Interspeech; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A new benchmark for Aphasia speech recognition and detection tasks using state-of-the-art speech recognition techniques with the AphsiaBank dataset is presented and two multi-task learning methods based on the CTC/Attention architecture are introduced to perform both tasks simultaneously.'} Author (LTI's Professor): Shinji Watanabe; Title: Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses; Authors: E. Takashita, S. Murakami, Y. Matsuzaki, Seiichiro Fujisaki, H. Morita, Shiho Nagata, Misa Katayama, K. Mizuta, H. Nishimura, Shinji Watanabe, T. Horimoto, H. Hasegawa; Abstract: The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.; Year: 2023; Venue: Viruses; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses.'} Author (LTI's Professor): Shinji Watanabe; Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement; Authors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj; Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \\u2013 such as spectral tilt, spectral flux, shimmer, etc. \\u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance.'} Author (LTI's Professor): Shinji Watanabe; Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing; Authors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin; Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.; Year: 2023; Venue: Special Interest Group on Computational Morphology and Phonology Workshop; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'In this submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), approaches to data augmentation and modeling across seven low-resource languages are explored and token classification models are found to be the best performing.'} Author (LTI's Professor): Shinji Watanabe; Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation; Authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhongqiu Wang, Nobutaka Ono, Y. Qian, Shinji Watanabe; Abstract: Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).; Year: 2023; Venue: IEEE Workshop on Applications of Signal Processing to Audio and Acoustics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end, and employs the recent self-supervised learning representation (SSLR) as a feature and improves the recognition performance from the case with filterbank features.'} Author (LTI's Professor): Shinji Watanabe; Title: Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech; Authors: Chien-yu Huang, Ke-Han Lu, Shi Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee; Abstract: Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion, and invites the community to collaborate and contribute, facilitating the dynamic growth of the benchmark.'} Author (LTI's Professor): Shinji Watanabe; Title: Speaker-Independent Acoustic-to-Articulatory Speech Inversion; Authors: Peter Wu, Li-Wei Chen, Cheol Jun Cho, Shinji Watanabe, L. Goldstein, A. Black, G. Anumanchipalli; Abstract: To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work builds an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers, and proposes a resynthesis-based AAI evaluation metric that does not rely on articulatory labels.'} Author (LTI's Professor): Shinji Watanabe; Title: Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens; Authors: Minsu Kim, J. Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, Y. Ro; Abstract: In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper starts with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp, and sets the output of the proposed Im2 Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model.'} Author (LTI's Professor): Shinji Watanabe; Title: Enhancing Speech-To-Speech Translation with Multiple TTS Targets; Authors: Jiatong Shi, Yun Tang, Ann Lee, H. Inaguma, Changhan Wang, J. Pino, Shinji Watanabe; Abstract: It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually utilize text-to-speech (TTS) systems to generate samples in the target language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the synthesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for direct S2ST models. We find that simply combining the target speech from different TTS systems can potentially improve the S2ST performances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is found that simply combining the target speech from different TTS systems can potentially improve the S2ST performances, and a multi-task framework is proposed that jointly optimizes the S1ST system with multiple targets from differentTTS systems.'} Author (LTI's Professor): Shinji Watanabe; Title: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head; Authors: Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jia-Bin Huang, Jinglin Liu, Yixiang Ren, Zhou Zhao, Shinji Watanabe; Abstract: Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \\\\url{https://github.com/AIGC-Audio/AudioGPT}.; Year: 2023; Venue: arXiv.org; Citations: 83; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A multi-modal AI system named AudioGPT is proposed, which complements LLMs with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.'} Author (LTI's Professor): Shinji Watanabe; Title: Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation; Authors: Shih-Lun Wu, Xuankai Chang, G. Wichern, Jee-weon Jung, Franccois G. Germain, Jonathan Le Roux, Shinji Watanabe; Abstract: Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATs to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work utilizes BEATs to extract fine-grained audio features and proposes a novel data augmentation method that uses ChatGPT to produce caption mix-ups which increase not only the amount but also the complexity and diversity of training data.'} Author (LTI's Professor): Shinji Watanabe; Title: Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks; Authors: Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, Shinji Watanabe; Abstract: We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.; Year: 2023; Venue: arXiv.org; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A decoder-only language model that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation, VoxtLM is proposed, which exhibits a significant improvement in speech synthesis and improves speech intelligibility and objective quality.'} Author (LTI's Professor): Shinji Watanabe; Title: Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining; Authors: Takaaki Saeki, Soumi Maiti, Xinjian Li, Shinji Watanabe, Shinnosuke Takamichi, H. Saruwatari; Abstract: While neural text-to-speech (TTS) has achieved human-like natural synthetic speech, multilingual TTS systems are limited to resource-rich languages due to the need for paired text and studio-quality audio data. This paper proposes a method for zero-shot multilingual TTS using text-only data for the target language. The use of text-only data allows the development of TTS systems for low-resource languages for which only textual resources are available, making TTS accessible to thousands of languages. Inspired by the strong cross-lingual transferability of multilingual language models, our framework first performs masked language model pretraining with multilingual text-only data. Then we train this model with a paired data in a supervised manner, while freezing a language-aware embedding layer. This allows inference even for languages not included in the paired data but present in the text-only data. Evaluation results demonstrate highly intelligible zero-shot TTS with a character error rate of less than 12% for an unseen language.; Year: 2023; Venue: International Joint Conference on Artificial Intelligence; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Inspired by the strong cross-lingual transferability of multilingual language models, this framework first performs masked language model pretraining with multilingual text-only data, and trains this model with a paired data in a supervised manner, while freezing a language-aware embedding layer.'} Author (LTI's Professor): Shinji Watanabe; Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling; Authors: Zhongqiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeonghak Kim, Shinji Watanabe; Abstract: We propose FSB-LSTM, a novel long short-term memory (LSTM) based architecture that integrates full- and sub-band (FSB) modeling, for single- and multi-channel speech enhancement in the short-time Fourier transform (STFT) domain. The model maintains an information highway to flow an over-complete input representation through multiple FSB-LSTM modules. Each FSB-LSTM module consists of a full-band block to model spectro-temporal patterns at all frequencies and a sub-band block to model patterns within each sub-band, where each of the two blocks takes a down-sampled representation as input and returns an up-sampled discriminative representation to be added to the block input via a residual connection. The model is designed to have a low algorithmic complexity, a small run-time buffer and a very low algorithmic latency, at the same time producing a strong enhancement performance on a noisy-reverberant speech enhancement task even if the hop size is as low as 2 ms.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The proposed FSB-LSTM model is designed to have a low algorithmic complexity, a small run-time buffer and a very lowgorithmic latency, at the same time producing a strong enhancement performance on a noisy-reverberant speech enhancement task even if the hop size is as low as 2 ms.'} Author (LTI's Professor): Shinji Watanabe; Title: Improving Massively Multilingual ASR with Auxiliary CTC Objectives; Authors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe; Abstract: Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 18; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID), and investigates techniques inspired from recent Connectionist Temporal Classification studies to help the model handle the large number of languages.'} Author (LTI's Professor): Shinji Watanabe; Title: Toward Universal Speech Enhancement For Diverse Input Conditions; Authors: Wangyou Zhang, Kohei Saijo, Zhong-Qiu Wang, Shinji Watanabe, Yanmin Qian; Abstract: The past decade has witnessed substantial growth of data-driven speech enhancement (SE) techniques thanks to deep learning. While existing approaches have shown impressive performance in some common datasets, most of them are designed only for a single condition (e.g., single-channel, multi-channel, or a fixed sampling frequency) or only consider a single task (e.g., denoising or dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.; Year: 2023; Venue: Automatic Speech Recognition & Understanding; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies, and designs a universal SE benchmark by combining existing public corpora with multiple conditions.'} Author (LTI's Professor): Shinji Watanabe; Title: The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge; Authors: Hayato Futami, Jessica Huynh, Siddhant Arora, Shih-Lun Wu, Yosuke Kashiwagi, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe; Abstract: This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023, adopts a pipeline approach of ASR and NLU and applies masked LM (MLM) -based data augmentation.'} Author (LTI's Professor): Shinji Watanabe; Title: A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge; Authors: Siddhant Arora, Hayato Futami, Shih-Lun Wu, Jessica Huynh, Yifan Peng, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe; Abstract: Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper describes the proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023.'} Author (LTI's Professor): Shinji Watanabe; Title: A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023; Authors: E. Takashita, Seiichiro Fujisaki, H. Morita, Shiho Nagata, H. Miura, Yuki Matsuura, Saya Yamamoto, Shoko Chiba, Yumiko Inoue, Iori Minami, Sayaka Yoshikawa, Seiko Yamazaki, N. Kishida, Kazuya Nakamura, Masayuki Shirakura, Shinji Watanabe, Hideki Hasegawa; Abstract: A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.; Year: 2023; Venue: Euro surveillance : bulletin Europeen sur les maladies transmissibles = European communicable disease bulletin; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023 and showed reduced susceptibility to baloxavir in vitro; however, the clinical significance remains unclear.'} Author (LTI's Professor): Shinji Watanabe; Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study; Authors: Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang; Abstract: Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This study undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models, demonstrating that discrete units achieve reasonably good results in almost all the settings.'} Author (LTI's Professor): Shinji Watanabe; Title: Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History; Authors: Siddhant Arora, Hayato Futami, E. Tsunoo, Brian Yan, Shinji Watanabe; Abstract: Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel model architecture is proposed that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance and achieves similar results to task-specific classifiers and can effectively integrateDialog context to further improve the SLU performance.'} Author (LTI's Professor): Shinji Watanabe; Title: Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization; Authors: A. Hussein, Brian Yan, Antonios Anastasopoulos, Shinji Watanabe, S. Khudanpur; Abstract: Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments, and proposes context dropout to ensure robustness to the absence of context, and improves performance by adding speaker information.'} Author (LTI's Professor): Shinji Watanabe; Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter; Authors: Shinji Watanabe; Abstract: None; Year: 2023; Venue: Oleoscience; Citations: 0; TLDR: None Author (LTI's Professor): Shinji Watanabe; Title: The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction; Authors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao; Abstract: Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge is delivered, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments.'} Author (LTI's Professor): Shinji Watanabe; Title: Intrusion of Coastal Oyashio water to Funka Bay and Tsugaru Strait occasionally disturbed by Kuroshio-originating warm core ring; Authors: H. Abe, Y. Yahiro, T. Hasegawa, T. Hirawake, H. Onishi, A. Ooki, T. Takatsu, K. Sasaki, M. Wakita, H. Kaneko, Shinji Watanabe, T. Tanaka, T. Okunishi, S. Ohno, S. Hashizume; Abstract: None; Year: 2023; Venue: Journal of Oceanography; Citations: 0; TLDR: None Author (LTI's Professor): Shinji Watanabe; Title: Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data; Authors: Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, Shinji Watanabe; Abstract: Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet; Year: 2023; Venue: Automatic Speech Recognition & Understanding; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data and even supports more translation directions and can be more efficient to train.'} Author (LTI's Professor): Shinji Watanabe; Title: Visual Speech Recognition for Languages with Limited Labeled Data using Automatic Labels from Whisper; Authors: Jeong Hun Yeo, Minsu Kim, Shinji Watanabe, Y. Ro; Abstract: This paper proposes a powerful Visual Speech Recognition (VSR) method for multiple languages, especially for low-resource languages that have a limited number of labeled data. Different from previous methods that tried to improve the VSR performance for the target language by using knowledge learned from other languages, we explore whether we can increase the amount of training data itself for the different languages without human intervention. To this end, we employ a Whisper model which can conduct both language identification and audio-based speech recognition. It serves to filter data of the desired languages and transcribe labels from the unannotated, multilingual audio-visual data pool. By comparing the performances of VSR models trained on automatic labels and the human-annotated labels, we show that we can achieve similar VSR performance to that of human-annotated labels even without utilizing human annotations. Through the automated labeling process, we label large-scale unlabeled multilingual databases, VoxCeleb2 and AVSpeech, producing 1,002 hours of data for four low VSR resource languages, French, Italian, Spanish, and Portuguese. With the automatic labels, we achieve new state-of-the-art performance on mTEDx in four languages, significantly surpassing the previous methods. The automatic labels are available online: https://github.com/JeongHun0716/Visual-Speech-Recognition-for-Low-Resource-Languages; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that VSR performance can be similar to that of human-annotated labels even without utilizing human annotations, and new state-of-the-art performance on mTEDx in four languages is achieved, significantly surpassing the previous methods.'} Author (LTI's Professor): Shinji Watanabe; Title: The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios; Authors: Samuele Cornell, Matthew Wiesner, Shinji Watanabe, Desh Raj, Xuankai Chang, Paola Garc\\u00eda, Yoshiki Masuyama, Zhong-Qiu Wang, S. Squartini, S. Khudanpur; Abstract: The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).; Year: 2023; Venue: 7th International Workshop on Speech Processing in Everyday Environments (CHiME 2023); Citations: 19; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The ChiME-7 distant ASR (DASR) task, within the 7th CHiME challenge, is introduced and the baseline system is presented, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).'} Author (LTI's Professor): Shinji Watanabe; Title: Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing; Authors: Brian Yan, Xuankai Chang, Antonios Anastasopoulos, Yuya Fujita, Shinji Watanabe; Abstract: Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally and reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length.'} Author (LTI's Professor): Shinji Watanabe; Title: ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit; Authors: Brian Yan, Jiatong Shi, Yun Tang, H. Inaguma, Yifan Peng, Siddharth Dalmia, Peter Pol'ak, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, J. Pino, Shinji Watanabe; Abstract: ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \\u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2 are described, which is publicly available at https://github.com/espnet/esp net.'} Author (LTI's Professor): Shinji Watanabe; Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge; Authors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono; Abstract: This paper describes our submission to the Second Clarity Enhancement Challenge (CEC2), which consists of target speech enhancement for hearing-aid (HA) devices in noisy-reverberant environments with multiple interferers such as music and competing speakers. Our approach builds upon the powerful iterative neural/beamforming enhancement (iNeuBe) framework introduced in our recent work, and this paper extends it for target speaker extraction. We therefore name the proposed approach as iNeuBe-X, where the X stands for extraction. To address the challenges encountered in the CEC2 setting, we introduce four major novelties: (1) we extend the state-of-the-art TF-GridNet model, originally designed for monaural speaker separation, for multi-channel, causal speech enhancement, and large improvements are observed by replacing the TCNDenseNet used in iNeuBe with this new architecture; (2) we leverage a recent dual window size approach with future-frame prediction to ensure that iNueBe-X satisfies the 5 ms constraint on algorithmic latency required by CEC2; (3) we introduce a novel speaker-conditioning branch for TF-GridNet to achieve target speaker extraction; (4) we propose a fine-tuning step, where we compute an additional loss with respect to the target speaker signal compensated with the listener audiogram. Without using external data, on the official development set our best model reaches a hearing-aid speech perception index (HASPI) score of 0.942 and a scale-invariant signal-to-distortion ratio improvement (SI-SDRi) of 18.8 dB. These results are promising given the fact that the CEC2 data is extremely challenging (e.g., on the development set the mixture SI-SDR is -12.3 dB). A demo of our submitted system is available at WAVLab CEC2 demo.; Year: 2023; Venue: arXiv.org; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The approach builds upon the powerful iterative neural/beamforming enhancement (iNeuBe) framework introduced in recent work, and this paper extends it for target speaker extraction, and is named as iNeu be-X, where the X stands for extraction.'} Author (LTI's Professor): Shinji Watanabe; Title: Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model; Authors: Takashi Maekaku, Yuya Fujita, Xuankai Chang, Shinji Watanabe; Abstract: Unsupervised topic clustering of spoken audio is an important research topic for zero-resourced unwritten languages. A classical approach is to find a set of spoken terms from only the audio based on dynamic time warping or generative modeling (e.g., hidden Markov model), and apply a topic model to classify topics. The spoken term discovery is the most important and difficult part. In this paper, we propose to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models. Most SSRL methods pre-train a model which predicts high-quality pseudo labels generated from an audio-only corpus. These pseudo labels can be used to produce a sequence of pseudo subwords by applying deduplication and a subword model. Then, we apply a topic model based on latent Dirichlet allocation for these pseudo-subword sequences in an unsupervised manner. The clustering performance is evaluated on the Fisher corpus using normalized mutual information. We confirm the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models although the experimental setups are not directly comparable.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models and confirms the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models.'} Author (LTI's Professor): Shinji Watanabe; Title: TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement; Authors: YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj; Abstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work shows that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility and uses data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from the method.'} Author (LTI's Professor): Shinji Watanabe; Title: End-to-End Speech Recognition: A Survey; Authors: Rohit Prabhavalkar, Takaaki Hori, Tara N. Sainath, R. Schluter, Shinji Watanabe; Abstract: In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.; Year: 2023; Venue: IEEE/ACM Transactions on Audio Speech and Language Processing; Citations: 38; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A taxonomy of E2E ASR models and corresponding improvements is provided, and their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures are discussed.'} Author (LTI's Professor): Shinji Watanabe; Title: The Multimodal Information Based Speech Processing (Misp) 2022 Challenge: Audio-Visual Diarization And Recognition; Authors: Zhe Wang, Shilong Wu, Hang Chen, Maokui He, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu; Abstract: The Multi-modal Information based Speech Processing (MISP) challenge aims to extend the application of signal processing technology in specific scenarios by promoting the research into wake-up words, speaker diarization, speech recognition, and other technologies. The MISP2022 challenge has two tracks: 1) audio-visual speaker diarization (AVSD), aiming to solve \\\"who spoken when\\\" using both audio and visual data; 2) a novel audio-visual diarization and recognition (AVDR) task that focuses on addressing \\\"who spoken what when\\\" with audio-visual speaker diarization results. Both tracks focus on the Chinese language, and use far-field audio and video in real home-tv scenarios: 2-6 people communicating each other with TV noise in the background. This paper introduces the dataset, track settings, and baselines of the MISP2022 challenge. Our analyses of experiments and examples indicate the good performance of AVDR baseline system, and the potential difficulties in this challenge due to, e.g., the far-field video quality, the presence of TV noise in the background, and the indistinguishable speakers.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The dataset, track settings, and baselines of the MISP2022 challenge are introduced, and analyses of experiments and examples indicate the good performance of AVDR baseline system, and the potential difficulties in this challenge due to, e.g., the far-field video quality, the presence of TV noise in the background, andThe indistinguishable speakers.'} Author (LTI's Professor): Shinji Watanabe; Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models; Authors: Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe; Abstract: Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.; Year: 2023; Venue: Interspeech; Citations: 12; TLDR: {'model': 'tldr@v2.0.0', 'text': 'DPHuBERT is proposed, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning that requires little training time and performs well with limited training data, making it suitable for resource-constrained applications.'} Author (LTI's Professor): Shinji Watanabe; Title: An external quality assessment feasibility study; cross laboratory comparison of haemagglutination inhibition assay and microneutralization assay performance for seasonal influenza serology testing: A FLUCOP study; Authors: J. Waldock, C. Weiss, Wei Wang, M. Levine, Stacie N. Jefferson, S. Ho, K. Hoschler, B. Londt, E. Masat, Louise A. Carolan, Stephany S\\u00e1nchez-Ovando, A. Fox, Shinji Watanabe, Miki Akimoto, Aya Sato, N. Kishida, A. Buys, Lorens Maake, Cardia Fourie, Catherine Caillet, Sandrine Raynaud, R. Webby, J. Debeauchamp, R. Cox, Sarah Lartey, C. Trombetta, S. Marchi, E. Montomoli, I. Sanz-Mu\\u00f1oz, J. Eiros, Javier S\\u00e1nchez-Mart\\u00ednez, D. Duijsings, O. Engelhardt; Abstract: Introduction External Quality Assessment (EQA) schemes are designed to provide a snapshot of laboratory proficiency, identifying issues and providing feedback to improve laboratory performance and inter-laboratory agreement in testing. Currently there are no international EQA schemes for seasonal influenza serology testing. Here we present a feasibility study for conducting an EQA scheme for influenza serology methods. Methods We invited participant laboratories from industry, contract research organizations (CROs), academia and public health institutions who regularly conduct hemagglutination inhibition (HAI) and microneutralization (MN) assays and have an interest in serology standardization. In total 16 laboratories returned data including 19 data sets for HAI assays and 9 data sets for MN assays. Results Within run analysis demonstrated good laboratory performance for HAI, with intrinsically higher levels of intra-assay variation for MN assays. Between run analysis showed laboratory and strain specific issues, particularly with B strains for HAI, whilst MN testing was consistently good across labs and strains. Inter-laboratory variability was higher for MN assays than HAI, however both assays showed a significant reduction in inter-laboratory variation when a human sera pool is used as a standard for normalization. Discussion This study has received positive feedback from participants, highlighting the benefit such an EQA scheme would have on improving laboratory performance, reducing inter laboratory variation and raising awareness of both harmonized protocol use and the benefit of biological standards for seasonal influenza serology testing.; Year: 2023; Venue: Frontiers in Immunology; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A feasibility study for conducting an EQA scheme for influenza serology methods showing good laboratory performance for HAI, with intrinsically higher levels of intra-assay variation for MN assays, and a significant reduction in inter-laboratory variation when a human sera pool is used as a standard for normalization.'} Author (LTI's Professor): Shinji Watanabe; Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge; Authors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini; Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work details the submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments, and builds on the previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X.'} Author (LTI's Professor): Shinji Watanabe; Title: AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models; Authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David F. Harwath, Yu Tsao, Shinji Watanabe, Abdel-rahman Mohamed, Chi-Luen Feng, Hung-yi Lee; Abstract: Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned representations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task. We release our benchmark with evaluation code and a model submission platform to encourage further research in audio-visual learning.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The AV-SUPERB benchmark is proposed that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing and shows that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task.'} Author (LTI's Professor): Shinji Watanabe; Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation; Authors: Yu-Kuan Fu, Liang-Hsuan Tseng, Jiatong Shi, Chen-An Li, Tsung-Yuan Hsu, Shinji Watanabe, Hung-yi Lee; Abstract: Most of the speech translation models heavily rely on parallel data, which is hard to collect especially for low-resource languages. To tackle this issue, we propose to build a cascaded speech translation system without leveraging any kind of paired data. We use fully unpaired data to train our unsupervised systems and evaluate our results on CoVoST 2 and CVSS. The results show that our work is comparable with some other early supervised methods in some language pairs. While cascaded systems always suffer from severe error propagation problems, we proposed denoising back-translation (DBT), a novel approach to building robust unsupervised neural machine translation (UNMT). DBT successfully increases the BLEU score by 0.7--0.9 in all three translation directions. Moreover, we simplified the pipeline of our cascaded system to reduce inference latency and conducted a comprehensive analysis of every part of our work. We also demonstrate our unsupervised speech translation results on the established website.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposed denoising back-translation (DBT), a novel approach to building robust unsupervised neural machine translation (UNMT), which successfully increases the BLEU score by 0.7--0.9 in all three translation directions.'} Author (LTI's Professor): Shinji Watanabe; Title: Deep Speech Synthesis from MRI-Based Articulatory Representations; Authors: Peter Wu, Tingle Li, Yijingxiu Lu, Yubin Zhang, Jiachen Lian, A. Black, L. Goldstein, Shinji Watanabe, G. Anumanchipalli; Abstract: In this paper, we study articulatory synthesis, a speech synthesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable synthesizers. While recent advances have enabled intelligible articulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excitation and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to enhance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Finally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and identify the most suitable MRI feature subset for articulatory synthesis.; Year: 2023; Venue: Interspeech; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An MRI-to-speech model that improves both computational efficiency and speech fidelity is proposed and the proposed MRI representation is more comprehensive than EMA and the most suitable MRI feature subset for articulatory synthesis is identified.'} Author (LTI's Professor): Shinji Watanabe; Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN; Authors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, R. Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Est\\u00e8ve, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, D\\u00e1vid Javorsk\\u00fd, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Pol\\u00e1k, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian St\\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, Marco Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos; Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.; Year: 2023; Venue: International Workshop on Spoken Language Translation; Citations: 26; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Shinji Watanabe; Title: Speech collage: code-switched audio generation by collaging monolingual corpora; Authors: A. Hussein, Dorsa Zeinali, Ondrej Klejch, Matthew Wiesner, Brian Yan, Shammur A. Chowdhury, Ahmed Ali, Shinji Watanabe, S. Khudanpur; Abstract: Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"Speech Collage is introduced, a method that synthesizes CS data from monolingual corpora by splicing audio segments that improves the smoothness quality of audio generation using an overlap-add approach and demonstrates that CS augmentation bolsters the model's code-switching inclination and reduces itsmonolingual bias.\\\"} Author (LTI's Professor): Shinji Watanabe; Title: Exploration on HuBERT with Multiple Resolutions; Authors: Jiatong Shi, Yun Tang, H. Inaguma, Hongyu Gong, J. Pino, Shinji Watanabe; Abstract: Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.; Year: 2023; Venue: Interspeech; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Through experiments, it is demonstrated that HuBERT with multiple resolutions outperforms the original model, highlighting the potential of utilizing multiple resolutions in SSL models like HuberT to capture diverse information from speech signals.'} Author (LTI's Professor): Sean Welleck; Title: Self-Refine: Iterative Refinement with Self-Feedback; Authors: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, Peter Clark; Abstract: Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 415; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Self-Refine is introduced, an approach for improving initial outputs from LLMs through iterative feedback and refinement that demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using this simple, standalone approach.'} Author (LTI's Professor): Sean Welleck; Title: D RAFT , S KETCH , AND P ROVE : G UIDING F ORMAL T HEOREM P ROVERS WITH I NFORMAL P ROOFS; Authors: Albert Qiaochu Jiang, S. Welleck, J. Zhou, Wen-Ding Li, Jiacheng Liu, M. Jamnik, Timoth\\u00e9e Lacroix, Guillaume Lample, Yuhuai Wu; Abstract: The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce wellstructured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems. Figure 1: Draft, Sketch, and Prove. Starting with an informal statement, our framework yields a formal proof through a three-stage process: drafting informal proofs, mapping them into formal sketches, and proving the remaining conjectures. Concretely, an informal statement is a mathematical problem described in a mixture of natural and mathematical languages (e.g., formulae in LTEX). Then, we use a large language model to autoformalize each informal proof into a formal sketch, which is a skeleton of the formal proof with open conjectures left unproven (indicated by the <proof> blocks). The formal sketch mirrors the structure of the informal proof. Finally, the open conjectures/gaps inside each formal sketch are proved by an off-the-shelf prover. \\u2020Equal contributions as leading authors. Correspondence to: qj213@cam.ac.uk. \\u2021Equal contributions as senior authors.; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems.'} Author (LTI's Professor): Sean Welleck; Title: Faith and Fate: Limits of Transformers on Compositionality; Authors: Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, S. Welleck, Xiang Ren, Allyson Ettinger, Za\\u00efd Harchaoui, Yejin Choi; Abstract: Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\\\,increased\\\\,task\\\\,complexity.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 96; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills.'} Author (LTI's Professor): Sean Welleck; Title: STEER: Unified Style Transfer with Expert Reinforcement; Authors: Skyler Hallinan, Faeze Brahman, Ximing Lu, Jaehun Jung, S. Welleck, Yejin Choi; Abstract: While text style transfer has many applications across natural language processing, the core premise of transferring from a single source style is unrealistic in a real-world setting. In this work, we focus on arbitrary style transfer: rewriting a text from an arbitrary, unknown style to a target style. We propose STEER: Unified Style Transfer with Expert Reinforcement, a unified frame-work developed to overcome the challenge of limited parallel data for style transfer. STEER involves automatically generating a corpus of style-transfer pairs using a product of experts during decoding. The generated offline data is then used to pre-train an initial policy before switching to online, off-policy reinforcement learning for further improvements via fine-grained reward signals. STEER is unified and can transfer to multiple target styles from an arbitrary, unknown source style, making it particularly flexible and efficient. Experimental results on a challenging dataset with text from a diverse set of styles demonstrate state-of-the-art results compared to competitive baselines. Remarkably, STEER outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size. We also show STEER is robust, maintaining its style transfer capabilities on out-of-domain data, and surpassing nearly all baselines across various styles. The success of our method highlights the potential of RL algorithms when augmented with controllable decoding to overcome the challenge of limited data supervision.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The proposed STEER: Unified Style Transfer with Expert Reinforcement, a unified frame-work developed to overcome the challenge of limited parallel data for style transfer, outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size.'} Author (LTI's Professor): Sean Welleck; Title: LLMSTEP: LLM proofstep suggestions in Lean; Authors: S. Welleck, Rahul Saha; Abstract: We present LLMSTEP, a tool for integrating a language model into the Lean proof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to a server hosting a language model. The language model generates suggestions, which are checked in Lean and displayed to a user in their development environment. We provide a baseline language model, along with code for fine-tuning and evaluation to support further development. We provide server implementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a step towards fast, effective language model suggestions for any user.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A baseline language model is provided, along with code for fine-tuning and evaluation to support further development, and server implementations that run on CPU, a CUDA GPU, or a Google Colab notebook are provided as a step towards fast, effective language model suggestions for any user.'} Author (LTI's Professor): Sean Welleck; Title: Llemma: An Open Language Model For Mathematics; Authors: Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, S. Welleck; Abstract: We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.; Year: 2023; Venue: arXiv.org; Citations: 47; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Llemma is a large language model for mathematics that outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis, and is capable of tool use and formal theorem proving without any further finetuning.'} Author (LTI's Professor): Sean Welleck; Title: Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning; Authors: Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Raghavi Chandu, Abhilasha Ravichander, Lianhui Qin, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian R. Fisher, Bill Yuchen Lin, Skyler Hallinan, Xiang Ren, S. Welleck, Yejin Choi; Abstract: While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Inference-time Policy Adapters (IPA) is proposed, which efficiently tailors a language model such as GPT-3 without fine-tuning it, and consistently brings significant improvements over off-the-shelf language models.'} Author (LTI's Professor): Eric P. Xing; Title: Identification of Nonlinear Latent Hierarchical Models; Authors: Lingjing Kong, Biwei Huang, Feng Xie, E. Xing, Yuejie Chi, Kun Zhang; Abstract: Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of causal structures and latent variables (up to invertible transformations) can be achieved under mild assumptions: on causal structures, we allow for multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we permit general nonlinearity and multi-dimensional continuous variables, alleviating existing work's parametric assumptions. Specifically, we first develop an identification criterion in the form of novel identifiability guarantees for an elementary latent variable model. Leveraging this criterion, we show that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure. To the best of our knowledge, our work is the first to establish identifiability guarantees for both causal structures and latent variables in nonlinear latent hierarchical models.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work develops an identification criterion in the form of novel identifiability guarantees for an elementary latent variable model and shows that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure.'} Author (LTI's Professor): Eric P. Xing; Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields; Authors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, E. Xing; Abstract: 3D style transfer aims to render stylized novel views of a 3D scene with multiview consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 12; TLDR: {'model': 'tldr@v2.0.0', 'text': 'StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field, achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner.'} Author (LTI's Professor): Eric P. Xing; Title: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena; Authors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, E. Xing, Haotong Zhang, Joseph Gonzalez, Ion Stoica; Abstract: Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 780; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans, and LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.'} Author (LTI's Professor): Eric P. Xing; Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models; Authors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang; Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE, and formulate the underlying data-generating process as a hierarchical latent variable model, and shows that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model.'} Author (LTI's Professor): Chenyan Xiong; Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases; Authors: Zhenghao Liu, Senkun Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu; Abstract: This paper proposes Text mAtching based SequenTial rEcommenda-tion model (TASTE), which maps items and users in an embedding space and recommends items by matching their text representations. TASTE verbalizes items and user-item interactions using identifiers and attributes of items. To better characterize user behaviors, TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding. Our experiments show that TASTE outperforms the state-of-the-art methods on widely used sequential recommendation datasets. TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems. Our further analyses illustrate that TASTE significantly improves the recommendation accuracy by reducing the popularity bias of previous item id based recommendation models and returning more appropriate and text-relevant items to satisfy users. All codes are available at https://github.com/OpenMatch/TASTE.; Year: 2023; Venue: International Conference on Information and Knowledge Management; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Chenyan Xiong; Title: Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval; Authors: S. Yu, Cheng-Chung Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, Zhenghao Liu Tsinghua University, Huazhong University of Science, Technology, Microsoft Research, M. I. O. Technology, N. University; Abstract: Common IR pipelines are typically cascade systems that may involve multiple rankers and/or fusion models to integrate different information step-by-step. In this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5 significantly improves ranking performance over prior pipelines. Analyses find that through global attention, FiT5 is able to jointly utilize the ranking features via gradually attending to related documents, and thus improve the detection of subtle nuances between them. Our code will be open-sourced.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention, is proposed.'} Author (LTI's Professor): Chenyan Xiong; Title: OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit; Authors: Shi Yu, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu; Abstract: Pre-trained language models (PLMs) have emerged as the foundation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel models, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch.; Year: 2023; Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure.'} Author (LTI's Professor): Yiming Yang; Title: Accurate detection of reactive oxygen species by tuning an elastic motif (GPGGA)4 in nanopores.; Authors: Cunli Wang, Yiming Yang, Shuai Shao, Hangyu Zhang, Na Li, Zheng-Zhu Zhang, Bo Liu; Abstract: We have developed a reactive oxygen species (ROS) sensor based on nanopores modified with GGGCEG(GPGGA)4CEG. The formation of an intramolecular disulfide bond oxidized by ROS leads to conformation changes in GGGCEG(GPGGA)4CEG, which then induces an obvious change in the size of the nanopores and a corresponding ionic current change. This work allows the accurate and dynamic monitoring of ROS through the combination of (GPGGA)4 and nanopores.; Year: 2023; Venue: Chemical Communications; Citations: 0; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation; Authors: Junwei Huang, Zhiqing Sun, Yiming Yang; Abstract: Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Progressive distillation is proposed to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process.'} Author (LTI's Professor): Yiming Yang; Title: Automatic synchrotron tomographic alignment schemes based on genetic algorithms and human-in-the-loop software; Authors: Zhen Zhang, Xiaoxue Bi, Pengcheng Li, Chenglong Zhang, Yiming Yang, Yu Liu, Gang Chen, Yuhui Dong, Gongfa Liu, Yi Zhang; Abstract: A highly automatic alignment scheme is proposed to address the pressing challenge in tomographic alignment of future scanning tomography experiments. The results show that the proposed method exhibits excellent sub-pixel alignment accuracy and high time efficiency.; Year: 2023; Venue: Journal of Synchrotron Radiation; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The results show that the proposed method exhibits excellent sub-pixel alignment accuracy and high time efficiency.'} Author (LTI's Professor): Yiming Yang; Title: High CD8+tumor-infiltrating lymphocytes indicate severe exhaustion and poor prognosis in angioimmunoblastic T-cell lymphoma; Authors: Qiqi Zhu, Yiming Yang, Xueqin Deng, Ningning Chao, Zihang Chen, Y. Ye, Wenyan Zhang, Weiping Liu, Sha Zhao; Abstract: Background Exhaustion of CD8+ tumor-infiltrating lymphocytes (TILs), characterized by the overexpression of immune checkpoints (IC), is a major impediment to anti-tumor immunity. However, the exhaustion status of CD8+TILs in angioimmunoblastic T cell lymphoma (AITL) remains unclear. Therefore, we aimed to elucidate the exhaustion status of CD8+TILs in AITL and its influence on prognosis. Methods The correlation between CD8+TILs and IC expression in AITL was analyzed using single-cell RNA sequencing (n = 2), flow cytometry (n = 20), and RNA sequencing (n = 20). Biological changes related to CD8+TILs exhaustion at different cytotoxic T lymphocyte (CTL) levels (mean expression levels of CD8A, CD8B, GZMA, GZMB, and PRF1) in AITL were evaluated using RNA sequencing (n = 20) and further validated using the GEO dataset (n = 51). The impact of CD8 protein expression and CTL levels on patient prognosis was analyzed using flow cytometry and RNA sequencing, respectively. Results Our findings demonstrated that the higher the infiltration of CD8+TILs, the higher was the proportion of exhausted CD8+TILs characterized by the overexpression of multiple IC. This was accompanied by extensive exhaustion-related biological changes, which suggested severe exhaustion in CD8+TILs and may be one of the main reasons for the poor prognosis of patients with high CD8+TILs and CTL. Conclusion Our study comprehensively reveals the exhaustion status of CD8+TILs and their potential negative impact on AITL prognosis, which facilitates further mechanistic studies and is valuable for guiding immunotherapy strategies.; Year: 2023; Venue: Frontiers in Immunology; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The study comprehensively reveals the exhaustion status of CD8+TILs and their potential negative impact on AITL prognosis, which facilitates further mechanistic studies and is valuable for guiding immunotherapy strategies.'} Author (LTI's Professor): Yiming Yang; Title: Robust Cross-Domain Pseudo-Labeling and Contrastive Learning for Unsupervised Domain Adaptation NIR-VIS Face Recognition; Authors: Yiming Yang, Weipeng Hu, Haiqi Lin, Haifeng Hu; Abstract: Near-infrared and visible face recognition (NIR-VIS) is attracting increasing attention because of the need to achieve face recognition in low-light conditions to enable 24-hour secure retrieval. However, annotating identity labels for a large number of heterogeneous face images is time-consuming and expensive, which limits the application of the NIR-VIS face recognition system to larger scale real-world scenarios. In this paper, we attempt to achieve NIR-VIS face recognition in an unsupervised domain adaptation manner. To get rid of the reliance on manual annotations, we propose a novel Robust cross-domain Pseudo-labeling and Contrastive learning (RPC) network which consists of three key components, i.e., NIR cluster-based Pseudo labels Sharing (NPS), Domain-specific cluster Contrastive Learning (DCL) and Inter-domain cluster Contrastive Learning (ICL). Firstly, NPS is presented to generate pseudo labels by exploring robust NIR clusters and sharing reliable label knowledge with VIS domain. Secondly, DCL is designed to learn intra-domain compact yet discriminative representations. Finally, ICL dynamically combines and refines intrinsic identity relationships to guide the instance-level features to learn robust and domain-independent representations. Extensive experiments are conducted to verify an accuracy of over 99% in pseudo label assignment and the advanced performance of RPC network on four mainstream NIR-VIS datasets.; Year: 2023; Venue: IEEE Transactions on Image Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a novel Robust cross-domain Pseudo-labeling and Contrastive learning (RPC) network which consists of three key components, i.e., NIR cluster-based Pseudo labels Sharing (NPS), Domain-specific cluster Contrastive Learning (DCL) and Inter-domain cluster Contrastives Learning (ICL).'} Author (LTI's Professor): Yiming Yang; Title: Numerical Investigation of Fatigue Crack Propagation Behaviour of 550E High-Performance Steel; Authors: Linfa Xiao, Heng Lin, Yongxiang Wang, Yiming Yang, Huapeng Chen; Abstract: The fatigue crack propagation behaviour of Q550E high-performance steel (HPS) is studied in this paper. Static tensile testing and fatigue crack propagation testing were carried out, and the results were compared with those of Q235. Finite element models were developed and verified against the experimental results. The impacts of the initial crack angle, crack depth ratio, stress ratio, thickness, and corrosion pitting on the fatigue crack propagation behaviour of the HPS were analysed. The results show that the fatigue life of Q550 was reduced by 18% due to the corrosion pitting, but it did not change the crack propagation path. When the stress intensity factor is higher than a certain value, the fatigue performance of Q235 is better than that of Q550E. The initial crack angle of 52.5\\u00b0 is the critical angle of the crack stress intensity factor. The steel tends to fracture as the crack depth ratio increases, and more attention should be paid to the effective crack length in engineering practice. An increasing stress ratio leads to a smaller stress intensity factor, and the thickness affects the stress intensity factor in the later stage. The crack stress intensity factor around the corrosion pits gradually decreases along the thickness direction, and the crack tips around the corrosion pits tend to reach the yield state initially, accelerating the fatigue fracture of the specimen and ultimately leading to a decrease in fatigue life.; Year: 2023; Venue: Metals; Citations: 0; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Phase Behaviors of Charged Macromolecules in Aqueous Solutions; Authors: Yiming Yang, Di Jia; Abstract: Compared to the charge\\u2013charge interaction, the role of the dipole\\u2010dipole interaction has long been ignored in the phase behaviors of charged macromolecules in solutions. Charged macromolecules in solutions exhibit rich phase behaviors due to their complexity and they have been studied extensively. Phase separation can happen for charged macromolecules in the presence of monovalent salt, multivalent salt, and oppositely charged polymers, surfactants, etc., and for more advanced charged macromolecules such as polyzwitterions and polyampholytes, the phase diagram is even richer. In this perspective, the unacknowledged role of dipole\\u2010dipole interaction in the phase behaviors of charged macromolecular solutions will be introduced. Dipolar polymers can form complex, self\\u2010regulating structures which can be employed in various fields from drug\\u2010delivery systems to next\\u2010generation polymers. More importantly, it will shed light on how some of the life's basic and coherent structures such as biomolecular condensates and membrane\\u2010less organelles are assembled and built by charged biomacromolecules such as DNA, RNA, and proteins.; Year: 2023; Venue: Macromolecular Chemistry and Physics; Citations: 1; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Association between albumin-to-globulin ratio and the risk of overall survival in advanced non-small cell lung cancer patients with anlotinib treatment: a retrospective cohort study; Authors: Jinzhan Chen, Cong-jun Xie, Yiming Yang, Shuwen Yang, Jin-xiang Huang, Feiyang Ye, Zhenyang Lin, L. Tong, Jiaxin Liu; Abstract: None; Year: 2023; Venue: BMC Pulmonary Medicine; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'AGR level is an independent protective factor for OS in advanced NSCLC patients who received anlotinib therapy, and was positively associated with OS when AGR was larger than 1.24, for every 1 unit increase in AGR, the risk of death lowered approximately by 80%.'} Author (LTI's Professor): Yiming Yang; Title: Modification Effect of Pt on the Active Sites of Sulfated CeO2 Nanorods for the Selective Catalytic Reduction of NO; Authors: Hao Fan, Yiming Yang, Xu Yang, Xuefeng He, Jian Sun, Liu Yang, Jiao Li, Zhenxing Shen; Abstract: None; Year: 2023; Venue: ACS Applied Nano Materials; Citations: 1; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology; Authors: Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu; Abstract: To investigate the volatile components of Schisandra chinensis (Turcz.) Bail (commonly known as northern Schisandra) of different colors and to explore their similarities and differences, to identify the main flavor substances in the volatile components of the branch exudates of northern schisandra, and finally to establish a fingerprint map of the volatile components of the dried fruits and branch exudates of northern Schisandra of different colors, we used GC-IMS technology to analyze the volatile components of the dried fruits and branch exudates of three different colors of northern Schisandra and established a fingerprint spectra. The results showed that a total of 60 different volatile chemical components were identified in the branch exudates and dried fruits of Schisandra. The components of germplasm resources with different fruit colors were significantly different. The ion mobility spectrum and OPLS-DA results showed that white and yellow fruits were more similar compared to red fruits. The volatile components in dried fruits were significantly higher than those in branch exudates. After VIP (variable importance in projection) screening, 41 key volatile substances in dried fruits and 30 key volatile substances in branch exudates were obtained. After screening by odor activity value (OAV), there were 24 volatile components greater than 1 in both dried fruits and branch exudates. The most important contributing volatile substance was 3-methyl-butanal, and the most important contributing volatile substance in white fruit was (E)-2-hexenal.; Year: 2023; Venue: Molecules; Citations: 2; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Secreted endogenous macrosomes reduce A\\u03b2 burden and ameliorate Alzheimer\\u2019s disease; Authors: Cunli Wang, Yiming Yang, Xiaoyu Zhang, Zhenqiang Shi, Huiling Gao, Manli Zhong, Yong-gang Fan, Hongyan Zhang, Bo Liu, Guangyan Qing; Abstract: Innovative therapeutic strategies are urgently needed for Alzheimer\\u2019s disease (AD) due to the increasing size of the aging population and the lack of effective drug treatment. Here, we report the therapeutic effects of extracellular vesicles (EVs) secreted by microglia, including macrosomes and small EVs, on AD-associated pathology. Macrosomes strongly inhibited \\u03b2-amyloid (A\\u03b2) aggregation and rescued cells from A\\u03b2 misfolding\\u2013induced cytotoxicity. Furthermore, macrosome administration reduced A\\u03b2 plaques and ameliorated cognitive impairment in mice with AD. In contrast, small EVs slightly promoted A\\u03b2 aggregation and did not improve AD pathology. Proteomic analysis of small EVs and macrosomes revealed that macrosomes harbor several important neuroprotective proteins that inhibit A\\u03b2 misfolding. In particular, the small integral membrane protein 10\\u2013like protein 2B in macrosomes has been shown to inhibit A\\u03b2 aggregation. Our observations provide an alternative therapeutic strategy for the treatment of AD over conventional ineffective drug treatments.; Year: 2023; Venue: Science Advances; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The therapeutic effects of extracellular vesicles secreted by microglia, including macrosomes and small EVs, on AD-associated pathology are reported and macrosomes strongly inhibited \\u03b2-amyloid aggregation and rescued cells from A\\u03b2 misfolding\\u2013induced cytotoxicity.'} Author (LTI's Professor): Yiming Yang; Title: Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition; Authors: Yiming Yang, Weipeng Hu, Haifeng Hu; Abstract: To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.; Year: 2023; Venue: IEEE transactions on circuits and systems for video technology (Print); Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network is proposed to disentangle the latent attributes of heterogeneous faces and learn neutral face representations to solve the challenge of heterogeneity and non-neutrality in face recognition.'} Author (LTI's Professor): Yiming Yang; Title: DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization; Authors: Zhiqing Sun, Yiming Yang; Abstract: Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 20; TLDR: {'model': 'tldr@v2.0.0', 'text': 'DIFUSCO is introduced, a new graph-based diffusion framework for NPC combinatorial optimization that outperforms the previous state-of-the-art neural solvers on the challenging SATLIB benchmark and investigates two types of diffusion models with Gaussian and Bernoulli noise, respectively.'} Author (LTI's Professor): Yiming Yang; Title: Effects of S Content on Inclusion Formation in the Al and Ti\\u2013Mg Complex Deoxidized Steel; Authors: Pengliang Jin, Yiming Yang, Lei Cao, Xinghu Yuan, Guocheng Wang; Abstract: Two groups of Al and Ti\\u2013Mg complex deoxidized steels with different S contents are designed, and inclusion characteristics of two groups of steel samples are observed by field\\u2010emission scanning electron microscopy\\u2013energy\\u2010dispersive spectroscopy. The results show that there are single TiS inclusions, MgAl2O4 (Al2O3)\\u2013TiS, and MgAl2O4\\u2013TiN\\u2013TiS complex inclusions in No. 1 steel (low\\u2010sulfur content) and No. 2 steel (high\\u2010sulfur content). However, there are also complex inclusions containing MnS in the No. 2 steel but not in No. 1 steel. In order to reveal the precipitation mechanism of MnS, equilibrium phase of inclusion from 1873 K to liquidus temperature is further analyzed, and the mass fractions of different inclusions from liquidus to solidus temperature are quantitatively calculated using the element segregation model combined with FactSage 7.2 thermodynamic software. Furthermore, the mismatch values between different crystal planes of MnS, TiS, and TiN are calculated. The results show that MnS (110) is most likely to precipitate on TiS (001), which is consistent with the observation that there is TiS\\u2013MnS interface in the complex inclusions containing MnS in No. 2 steel. This study could be helpful to the controlling sulfide precipitation in Al and Ti\\u2013Mg complex deoxidized steel.; Year: 2023; Venue: Steel Research International; Citations: 2; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs; Authors: Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu; Abstract: Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into sub goal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on sub goal representation functions and sub goal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent sub goal representations and lack an efficient sub goal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.; Year: 2023; Venue: IEEE International Joint Conference on Neural Network; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome limitations in GCHRL and develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures.'} Author (LTI's Professor): Yiming Yang; Title: Comprehensive evaluation of nine grape varieties based on fundamental physical and chemical indicators, color and volatile compounds; Authors: W. Cao, Nan Shu, Yiming Yang, Jinli Wen, Wenpeng Lu; Abstract: BACKGROUND: In todays\\u2019 society, the rapid development of the wine industry and the improvement of peoples\\u2019 living standards make people pay more and more attention to wine grape quality. OBJECTIVE: To evaluate the wine grape varieties in Northeast Chinas\\u2019 grape growing regions for better wine grape quality, we evaluated the quality of different varieties of wine grapes. METHODS: The grape varieties \\u2018Hassan\\u2019 \\u2018Zuoshaner\\u2019 \\u2018Beibinghong\\u2019 \\u2018Zuoyouhong\\u2019 \\u2018Beta\\u2019 \\u2018Shuanghong\\u2019 \\u2018Zijingganlu\\u2019 \\u2018Cabernet Sauvignon\\u2019 and \\u2018Syrah\\u2019 were planted in the grape growing area of Jilin, Northeast China, were used as the subjects of this study. The grape berries were analyzed and tested for morphological indicators, basic physicochemical indicators, color, and phenolic and aromatic composition. RESULTS: According to lab results, \\u2018Hassan\\u2019 contained the highest amount of total phenolics; \\u2018Zuoyouhong\\u2019 had the highest solids and total sugar content; \\u2018Shuanghong\\u2019 had the most elevated total acid and anthocyanin content; \\u2018Zijngganlu\\u2019 had the highest tannin content and acid fixation ratio; Seventy-one volatile compounds were detected in nine grape varieties. CONCLUSIONS: Each of the nine grape varieties has a distinctive flavor, and because of this, grape processing products with regional flavors can be created. The same offer valuable data for future scientific grape resource collection, conservation, and exploitation.; Year: 2023; Venue: Journal of Berry Research; Citations: 1; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Aligning Large Multimodal Models with Factually Augmented RLHF; Authors: Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, K. Keutzer, Trevor Darrell; Abstract: Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in\\\"hallucination\\\", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io.; Year: 2023; Venue: arXiv.org; Citations: 51; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A new alignment algorithm called Factually Augmented RLHF is proposed that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance.'} Author (LTI's Professor): Yiming Yang; Title: Experimental and numerical research on the static behavior of locally corroded OSBD; Authors: J. Peng, Yi Liu, Yiming Yang, Yadong Zhou, Longzhen Xie; Abstract: None; Year: 2023; Venue: Journal of constructional steel research; Citations: 1; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Syncretic Space Learning Network for NIR-VIS Face Recognition; Authors: Yiming Yang, Weipeng Hu, Haifeng Hu; Abstract: To overcome the technical bottleneck of face recognition in low-light scenarios, Near-InfraRed and VISible (NIR-VIS) heterogeneous face recognition is proposed for matching well-lit VIS faces with poorly lit NIR faces. Current cross-modal synthesis methods visually convert the NIR modality to the VIS modality and then perform face matching in the VIS modality. However, using a heavyweight GAN network on unpaired NIR-VIS faces may lead to high synthesis difficulty, low inference efficiency, and other problems. To alleviate the above problems, we simultaneously synthesize NIR and VIS images into modality-independent syncretic images and propose a novel syncretic space learning (SSL) model to eliminate the modal gap. First, Syncretic Modality Generator (SMG) synthesizes NIR and VIS images into syncretic images using channel-level convolution with a shallow CNN. In particular, the discriminative structural information is well preserved and the face quality can be further improved with small modal variations in a self-supervised learning manner. Second, Modality-adversarial Syncretic space Learning (MSL) projects NIR and VIS images into the syncretic space by a syncretic-modality adversarial learning strategy with syncretic pattern guided objective, so the modal gap of NIR-VIS faces can be effectively reduced. Finally, the Syncretic Distribution Consistency (SDC) constructed by NIR-syncretic, syncretic-syncretic, and VIS-syncretic consistency can enhance the intra-class compactness and learn discriminative representations. Extensive experiments on three challenging datasets demonstrate the effectiveness of the SSL method.; Year: 2023; Venue: ACM Trans. Multim. Comput. Commun. Appl.; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work simultaneously synthesizes NIR and VIS images into modality-independent syncretic images and proposes a novelsyncretic space learning (SSL) model to eliminate the modal gap, and develops the Syncretic Distribution Consistency (SDC), which can enhance the intra-class compactness and learn discriminative representations.'} Author (LTI's Professor): Yiming Yang; Title: An Experimental Study on Secondary Transfer Performances of Prestress after Anchoring Failure of Steel Wire Strands; Authors: Rihua Yang, Yiming Yang, Xuhui Zhang, Xinzhong Wang; Abstract: To understand the secondary transfer performances of residual prestress after the anchoring failure of end-anchored steel wire strands due to corrosion fracture, six steel wire strand components of post-tensioning prestress were designed and fabricated. One-side fast corrosion was applied to the steel wire strand components using the electrochemical method until anchoring failure was reached. The sphere of influence, stress changes, and the retraction and swelling effect of broken beams after failure were investigated. The influences of factors such as concrete strength, stirrup area, and the length of the component on the secondary transfer length of residual prestress were discussed. Based on the deformation relationship between prestressed steel wire strands and concrete in the stress transfer zone, a stress equation was established and solved through a bond constitutive model. A prediction model of the effective stress transfer length of prestressed steel wire strand after failure was proposed. The results demonstrated that residual prestress can have a secondary transfer after the corrosion fracture of end-anchored steel wire strands, but some effective prestress may be lost. Moreover, the loss of prestress is inversely proportional to concrete compressive strength. When the specimens are relatively short, the prestress loss increases significantly. Concrete strength has significant influences on the length of secondary transfer. The proposed simplified calculation method of the secondary transfer length of residual prestress has a relatively high accuracy, with an average error of 2.9% and a maximum error of 5.2%.; Year: 2023; Venue: Metals; Citations: 1; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models; Authors: Lei Zhao, Chengcheng Liu, Youhua H. Wang, Yiming Yang; Abstract: Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.; Year: 2023; Venue: International Journal of Applied Electromagnetics and Mechanics; Citations: 0; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Distributed cooperative dual closed loop velocity-attitude consensus controller for rendezvous of the underactuated AUV swarm in 3-dimensional space; Authors: Yu Zhang, W. Zhang, G. Xia, Yiming Yang, Yan-luan Zheng, Peiyu Han; Abstract: None; Year: 2023; Venue: Ocean Engineering; Citations: 2; TLDR: None Author (LTI's Professor): Yiming Yang; Title: MRI Features for Predicting Microvascular Invasion and Postoperative Recurrence in Hepatocellular Carcinoma Without Peritumoral Hypointensity; Authors: Zhiyuan Chen, Xiaohuan Li, Yu Zhang, Yiming Yang, Yan Zhang, Dongjing Zhou, Yu Yang, Shuping Zhang, Yupin Liu; Abstract: Purpose To identify MRI features of hepatocellular carcinoma (HCC) that predict microvascular invasion (MVI) and postoperative intrahepatic recurrence in patients without peritumoral hepatobiliary phase (HBP) hypointensity. Patients and Methods One hundred and thirty patients with HCC who underwent preoperative gadoxetate-enhanced MRI and curative hepatic resection were retrospectively reviewed. Two radiologists reviewed all preoperative MR images and assessed the radiological features of HCCs. The ability of peritumoral HBP hypointensity to identify MVI and intrahepatic recurrence was analyzed. We then assessed the MRI features of HCC that predicted the MVI and intrahepatic recurrence-free survival (RFS) in the subgroup without peritumoral HBP hypointensity. Finally, a two-step flowchart was constructed to assist in clinical decision-making. Results Peritumoral HBP hypointensity (odds ratio, 3.019; 95% confidence interval: 1.071\\u20138.512; P=0.037) was an independent predictor of MVI. The sensitivity, specificity, positive predictive value, negative predictive value, and AUROC of peritumoral HBP hypointensity in predicting MVI were 23.80%, 91.04%, 71.23%, 55.96%, and 0.574, respectively. Intrahepatic RFS was significantly shorter in patients with peritumoral HBP hypointensity (P<0.001). In patients without peritumoral HBP hypointensity, the only significant difference between MVI-positive and MVI-negative HCCs was the presence of a radiological capsule (P=0.038). Satellite nodule was an independent risk factor for intrahepatic RFS (hazard ratio,3.324; 95% CI: 1.733\\u20136.378; P<0.001). The high-risk HCC detection rate was significantly higher when using the two-step flowchart that incorporated peritumoral HBP hypointensity and satellite nodule than when using peritumoral HBP hypointensity alone (P<0.001). Conclusion In patients without peritumoral HBP hypointensity, a radiological capsule is useful for identifying MVI and satellite nodule is an independent risk factor for intrahepatic RFS.; Year: 2023; Venue: Journal of Hepatocellular Carcinoma; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'In patients without peritumoral HBP hypointensity, a radiological capsule is useful for identifying MVI and satellite nodule is an independent risk factor for intrahepatic RFS, and a two-step flowchart was constructed to assist in clinical decision-making.'} Author (LTI's Professor): Yiming Yang; Title: Impact of local governments\\u2019 construction land allocation strategies on innovation-driven development of China; Authors: Jian Wang, Shangui Peng, Yuhao Feng, Yiming Yang, Qun Wu; Abstract: None; Year: 2023; Venue: \\u8d44\\u6e90\\u79d1\\u5b66; Citations: 0; TLDR: None \",\n          \"Professor Eric P. Xing Title: Professor (On Leave), Language Technologies Institute Professor Eric P. Xing Office: 8101 Gates & Hillman Centers Professor Eric P. Xing Email: epxing@andrew.cmu.edu Professor Eric P. Xing Phone: 412-268-2559 Professor Eric P. Xing Research Area: None Professor Eric P. Xing Research: The major theme of Professor Xing's research lies in the development of machine learning and statistical methodology; especially for building quantitative models and predictive understandings of the evolutionary mechanism, regulatory circuitry, and developmental processes of biological systems; and for building intelligent systems for a wide range of applications in vision, IR and NLP that involves computational learning and reasoning under uncertainty.Foundations of Statistical Learning , including theory and algorithms for: 1) Time/space varying-coefficient models with evolving structures; 2) Sparse structured input/output models in high-dimensional problems; 3) Nonparametric Bayesian techniques for infinite-dimensional models; 4) RKHS embedding, nonparametric inference, and spectral methods for graphical models; 5) Distributed and online algorithms for optimization, approximate inference, and sampling on massive data.Large-scale Information & Intelligent System: 1) Development of scalable parallel architecture, protocol, programming interface, generic algorithms and models, for Big Learning; 2) Multi-view latent space models, topics models, and sparse coding for image/text/relational data mining; 3) Evolving structure, stable metrics, and prediction for dynamic social networks, goal-driven network design and optimization; 4) Web-scale image understanding, search, prediction, and storyline synthesis; 5) Information visualization, indexing and storage, web/mobile app development.Computational Biology: 1) Understanding genome-microenvironment interactions in cancer and embryogenesis via joint analysis of genomic, proteomic, and pathway signaling data; 2) Genetic analysis of population variation, demography and evolution; 3) Statistical inference of genome-transcriptome-phenome association in complex diseases; 4) Personalized diagnosis and treatment of spectrum diseases via next generation sequencing and computational \\\"omic\\\" analysis; 5) Biological image and text mining. Professor Eric P. Xing Projects: None Professor Eric P. Xing Bio: None Professor Eric P. Xing Education: None \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# corpus = []\n",
    "# counter = 0\n",
    "# knowledge_source_path = \"knowledge_source\"\n",
    "# file_names = os.listdir(knowledge_source_path)\n",
    "# for file_name in file_names:\n",
    "#   with open(os.path.join(knowledge_source_path, file_name), \"r\") as file:\n",
    "#       text = file.read()\n",
    "#       text = text.replace(\"<sep>\", \" \")\n",
    "#       # text = ' '.join(re.findall(\"[a-z0-9/.|,\\():;-{}-]+\", text))\n",
    "#       corpus.append({\"title\": file_name, \"text\": text})\n",
    "\n",
    "# df = pd.DataFrame(corpus)\n",
    "# df.to_csv(\"aggregated_documents.csv\", index=False)\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"aggregated_documents.csv\")\n",
    "df_category = pd.read_excel(\"categorized_doc.xlsx\")\n",
    "df_categorized = pd.merge(df, df_category[[\"title\", \"category\", \"description\"]], on=\"title\", how=\"left\")\n",
    "df_categorized['title'] = df_categorized['title'].str.replace('|', '/').str.replace('.txt', '')"
   ],
   "metadata": {
    "id": "cyRsYjeisaXj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "grF9xL5JtKdY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710385467326,
     "user_tz": 240,
     "elapsed": 1501,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "2cf916b4-62e2-44cf-882a-650ba7770bb9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                title  \\\n",
       "0   https:||lti.cs.cmu.edu|sites|default|files|MCD...   \n",
       "1   https:||www.cmu.edu|commencement|schedule|inde...   \n",
       "2   https:||enr-apps.as.cmu.edu|assets|SOC|sched_l...   \n",
       "3     https:||www.cmu.edu|about|cmu_fact_sheet_02.txt   \n",
       "4    https:||lti.cs.cmu.edu|directory|all|154|200.txt   \n",
       "..                                                ...   \n",
       "56  https:||lti.cs.cmu.edu|people|faculty|strubell...   \n",
       "57  https:||lti.cs.cmu.edu|people|faculty|mitamura...   \n",
       "58  https:||lti.cs.cmu.edu|people|faculty|diaz-fer...   \n",
       "59  https:||lti.cs.cmu.edu|people|faculty|sap-maar...   \n",
       "60  https:||lti.cs.cmu.edu|people|faculty|morency-...   \n",
       "\n",
       "                                                 text  \n",
       "0   1 Welcome Welcome to the Language Technologies...  \n",
       "1   Carnegie Mellon University Search Search this ...  \n",
       "2   schedule title: Summer Two 2024 Schedule | cou...  \n",
       "3   Carnegie Mellon University has been a birthpla...  \n",
       "4   Jump to navigation Academics Research Partners...  \n",
       "..                                                ...  \n",
       "56  Professor Emma Strubell Title: Assistant Profe...  \n",
       "57  Professor Teruko Mitamura Title: Research Prof...  \n",
       "58  Professor Fernando DIaz Title: Associate Profe...  \n",
       "59  Professor Maarten Sap Title: Assistant Profess...  \n",
       "60  Professor Louis-Philippe Morency Title: Leonar...  \n",
       "\n",
       "[61 rows x 2 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-b41c26ba-77da-4a73-bd04-745b0217d839\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https:||lti.cs.cmu.edu|sites|default|files|MCD...</td>\n",
       "      <td>1 Welcome Welcome to the Language Technologies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https:||www.cmu.edu|commencement|schedule|inde...</td>\n",
       "      <td>Carnegie Mellon University Search Search this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https:||enr-apps.as.cmu.edu|assets|SOC|sched_l...</td>\n",
       "      <td>schedule title: Summer Two 2024 Schedule | cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https:||www.cmu.edu|about|cmu_fact_sheet_02.txt</td>\n",
       "      <td>Carnegie Mellon University has been a birthpla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https:||lti.cs.cmu.edu|directory|all|154|200.txt</td>\n",
       "      <td>Jump to navigation Academics Research Partners...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https:||lti.cs.cmu.edu|people|faculty|strubell...</td>\n",
       "      <td>Professor Emma Strubell Title: Assistant Profe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>https:||lti.cs.cmu.edu|people|faculty|mitamura...</td>\n",
       "      <td>Professor Teruko Mitamura Title: Research Prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https:||lti.cs.cmu.edu|people|faculty|diaz-fer...</td>\n",
       "      <td>Professor Fernando DIaz Title: Associate Profe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>https:||lti.cs.cmu.edu|people|faculty|sap-maar...</td>\n",
       "      <td>Professor Maarten Sap Title: Assistant Profess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>https:||lti.cs.cmu.edu|people|faculty|morency-...</td>\n",
       "      <td>Professor Louis-Philippe Morency Title: Leonar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 2 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b41c26ba-77da-4a73-bd04-745b0217d839')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-b41c26ba-77da-4a73-bd04-745b0217d839 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-b41c26ba-77da-4a73-bd04-745b0217d839');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-af69dae1-047b-4c0a-ac5c-19ce93d1fc33\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af69dae1-047b-4c0a-ac5c-19ce93d1fc33')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-af69dae1-047b-4c0a-ac5c-19ce93d1fc33 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_a7926ce9-9f14-47ed-bc3c-629f43bf0b9f\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_a7926ce9-9f14-47ed-bc3c-629f43bf0b9f button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 61,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 61,\n        \"samples\": [\n          \"https:||lti.cs.cmu.edu|sites|default|files|MCDS%20Handbook%2023-24%20AY.txt\",\n          \"https:||api.semanticscholar.org|graph|v1|paper|search.txt\",\n          \"https:||lti.cs.cmu.edu|people|faculty|xing-eric.html.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 61,\n        \"samples\": [\n          \"1 Welcome Welcome to the Language Technologies Institute, Master of Computational Data Science Program. While this handbook is specific to your academic experience in the department, there are several other resources and offices graduate students are encouraged to consult during their tenure at Carnegie Mellon University. Information about The Word, the student handbook, the Office of Graduate and Postdoctoral Affairs, the Office of the Dean of Students, and others are included in Appendix A of this handbook. 1.1 The MCDS Degree The MCDS Degree The Master of Computational Data Science (MCDS) degree is a professional Master of Science degree offered by the Language Technologies Institute (LTI), a department in the School of Computer Science at Carnegie Mellon University. The MCDS degree offers students with a Bachelor's degree the opportunity to improve their training with advanced study in Computer Science and Machine Learning. We cater to students with basic analytic skills and a strong aptitude for mathematics, programming, and logical reasoning. An undergraduate degree in Computer Science is not required. Most students will complete the program in three semesters; students coming from other disciplines and students focus on developing applied research skills in preparation for further graduate study or research-oriented employment may require an additional fourth semester. The MCDS Program offers a core curriculum and several concentrations; students entering the program enroll in core courses in their first semester and select further courses to satisfy at least one concentration (see Section 3.3.6). Students construct their own course of study, in consultation with their academic advisor, in order to satisfy broad guidelines. Thus, a student may tailor their coursework in a given concentration to follow a particular area of emphasis. The MCDS program is typically a 16-month program consisting of courses, seminars, a required Capstone Project and a required summer internship or practical training. While some MCDS graduates continue on to PhD programs in the LTI or other leading universities, most graduates go on to jobs in corporate research and development laboratories. The program consists entirely of coursework and a Capstone Project, and no Master\\u2019s Thesis is required. All Capstone projects are structured as research activities and may lead to a publication. There is no Doctoral program in Computational Data Science. Because of the highly selective nature of the MCDS program and quality of the MCDS curriculum, performing well in the program will give a boost to a PhD application. MS graduates are welcome to apply to CMU PhD programs but will not receive preferential treatment. There are significant differences between CMU's different departments and degree programs in philosophical approach, procedures, policies and regulations. Each department issues a handbook that informs graduate students of their program requirements and procedures and ensures that students have written access to the standard information outlined below. This handbook describes the policies, procedures, and requirements for the Master of Computational Data Science (MCDS) degree. All policies not explicitly described in this document conform to School of Computer Science (SCS) policies and university policies described in The Word, Carnegie Mellon University Student Handbook and at the University Policies website. 1.2 Vision Carnegie Mellon University will have a transformative impact on society through continual innovation in education, research, creativity, and entrepreneurship. 1.3 Mission To create a transformative educational experience for students focused on deep disciplinary knowledge; problem solving; leadership, communication, and interpersonal skills; and personal health and well- being. To cultivate a transformative university community committed to (a) attracting and retaining diverse, world-class talent; (b) creating a collaborative environment open to the free exchange of ideas, where<sep    -4525 Phone: (412) 268-9870 Fax: (412) 268-7287 Fax: (412) 268-7287 Robert Frederking Mona Diab, LTI Director Graduate Program Chair Language Technologies Institute Language Technologies Institute School of Computer Science School of Computer Science Carnegie Mellon University Carnegie Mellon University Gates-Hillman Center 5723 Gates-Hillman Center 6515 5000 Forbes Avenue, Pgh, PA 15213 5000 Forbes Avenue, Pgh, PA 15213 Phone: (412) 268-3669 Phone: (412) 268-6656 The Language Technologies Institute is located primarily on the 5 th and 6 th floors of the Gates Hillman Complex (GHC) on Carnegie Mellon\\u2019s Pittsburgh campus: Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Gates Hillman Complex 5402, LTI Pittsburgh, PA 15241-3891 412-268-6591 (phone) 412-268-6298 (fax) http://www.lti.cs.cmu.edu/ 1.5 University Policies and Expectations Each member of the Carnegie Mellon community must be familiar with university policies and guidelines. In addition to this departmental graduate student handbook, the following resources are available to assist you in understanding community expectations: The Word/Student http://www.cmu.edu/student- Handbook: affairs/theword/index.html Academic Integrity https://www.cmu.edu/policies/student-and-student- Website: life/academic-integrity.html University Policies http://www.cmu.edu/policies/ Website: Office of Graduate and http://www.cmu.edu/graduate/policies/index.html Post-Doc Affairs: Please see Appendix A for additional information about university resources. 1.6 Carnegie Mellon University Statement of Assurance Carnegie Mellon University does not discriminate in admission, employment or administration of its programs or activities on the basis of race, color, national origin, sex, handicap or disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status or genetic information. Furthermore, Carnegie Mellon University does not discriminate and is required not to discriminate in violation of federal, state or local laws or executive orders. Inquiries concerning the application of and compliance with this statement should be directed to the university ombudsman, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, telephone 412-268-1018. Obtain general information about Carnegie Mellon University by calling 412-268-2000. Carnegie Mellon University publishes an annual campus security and fire safety report. describing the university's security, alcohol and drug, sexual assault and fire safety policies, and containing statistics about the number and type of crimes committed on the campus, and the number and cause of fires in campus residence facilities during the preceding three years. You can obtain a copy by contacting the Carnegie Mellon Police Department at 412-268-2323. The annual security and fire safety report also is available online at www.cmu.edu/police/annualreports. Information regarding the application of Title IX, including to admission and employment decisions, the sexual misconduct grievance procedures and process, including how to file a report or a complaint of sex discrimination, how to file a report of sexual harassment, and how the university responds to such reports is available at www.cmu.edu/title-ix. The Title IX coordinator may be reached at 412-268-7125 or tix@cmu.edu. 1.7 The Carnegie Mellon Code Students at Carnegie Mellon, because they are members of an academic community dedicated to the achievement of excellence, are expected to meet the highest standards of personal, ethical and moral conduct possible. These standards require personal integrity, a commitment to honesty without compromise, as well as truth without equivocation and a willingness to place the good of the community above the good of the self. Obligations once undertaken must be met, commitments kept. As members of the Carnegie Mellon community, individuals are expected to uphold the standards of the community in addition to holding others accountable for said standards. It is rare that the life of a student in an academic community can be so private that it will not affect the community as a whole or that the above standards do not apply. The discovery, advancement and communication of knowledge are not possible without a commitment to these standards. Creativity cannot exist without acknowledgment of the creativity of others. New knowledge cannot be developed without credit for prior knowledge. Without the ability to trust that these principles will be observed, an academic community cannot exist. The commitment of its faculty, staff and students to these standards contributes to the high respect in which the Carnegie Mellon degree is held. Students must not destroy that respect by their failure to meet these standards. Students who cannot meet them should voluntarily withdraw from the university. The Carnegie Mellon Code can also be found on-line at: https://www.cmu.edu/student-affairs/theword/. 2 The Language Technologies Institute 2.1 Main Office The Gates Hillman Complex: Mailboxes, printers, copiers, and other departmental resources are in GHC 5404. 2.2 Photocopies and Printers The use of a printer/copier requires a CS user id (see the \\u2018Computers\\u2019 section). The School of Computer Science provides several black-and-white and color printers for use by students. SCS Computing Facilities maintains a list of printers: http://www.cs.cmu.edu/~help/printing/. 2.3 Office Space for MS Students Full-time students in the LTI\\u2019s MS degree programs on the Pittsburgh campus have access to a shared working space to create a sense of community and provide space for working when on campus. 2.4 Computers for MS Students Students are expected to provide their own laptop computers that can be used to access university resources and complete course assignments. Laptops running Windows, MacOS, and Linux software are all acceptable. MS students will be given a CS user id. A CS user id is required to use the LTI computer cluster, department printers, and other SCS services. The School of Computer Science has a Help Center located at 4203 GHC. They can be contacted at help@cs.cmu.edu, extension 8-4231 from a campus phone, or 412-268-4231 from an outside line. MS students will be given access to the LTI\\u2019s computer cluster on an as-needed basis, to be used for course assignments, directed study projects, and/or the capstone project. The LTI cluster provides storage and computation for projects involving large datasets and/or lengthy computation. 3 MCDS Degree Completion and Certification This section describes the various rules and regulations that determine the attainment of a MCDS degree by the student. 3.1 CMU Degree Completion and Statute of Limitations Carnegie Mellon graduate students are expected to complete their degree requirements within the standard length of time for their program of study as outlined in the relevant Graduate Student Handbook. Standard program lengths for graduate students vary significantly \\u2013 ranging from two semesters for some full-time master\\u2019s programs to several or more years for doctoral programs. Upon completion of the graduate program degree requirements, the degree will be certified by the student\\u2019s academic program in the semester in which the student completes the requirements. Early Completion Graduate students who consider the completion of all degree requirements in less than the standard length of time for their program of study may consult with their degree- granting program or department to determine if early degree certification is allowed and under what circumstances. Extended or Longer-than-Standard Completion Longer-than-standard degree completion may occur due to academic interruptions in making progress toward the degree as defined by the academic program, interruptions of full-time study or progress towards the degree due to serious, documented medical issues, or other unusual or unforeseen circumstances. Master\\u2019s students who require longer than the standard time to complete their degree requirements are expected to remain in close contact with their graduate program and will be certified at the end of the semester in which they have completed their degree requirements. Policy on Master\\u2019s Student Statute of Limitations www.cmu.edu/policies/student-and-student-life/masters-students-statute-of- limitations.html See the above link regarding guidelines and restrictions which place an upper limit on the maximum length of time allowable for master\\u2019s degree completion and certification. Additional Guidance for Students Program of study. Students seeking guidance about their program of study and degree requirements should consult with their academic advisor and/or appropriate associate dean. Financial aid and student account Students are expected to make normal progress toward their degree to graduate within the standard timeframe for their program of study. Under U.S. Federal Title IV regulations, student eligibility for federal financial aid is contingent upon enrollment in and successful completion of courses that are counted as credit toward their current degree program. To receive the maximum amount of federal financial aid for which they may be eligible, students must enroll each semester in at least 36 units that count toward their current degree level. (See separate guidance regarding integrated degree completion.) Students should consult with their designated college liaison in The HUB regarding billing and financial aid, particularly for early completion, longer-than- standard completion, or integrated undergraduate and master\\u2019s degree programs. International students Immigration status for students in F-1 and J-1 non-immigrant status is tied to making normal progress toward completing degree requirements. Therefore, F-1 and J-1 students who are considering completing their degree requirements early, anticipating longer-than-standard completion, or moving from an undergraduate to a graduate student classification (integrated undergraduate-graduate study) should consult with their designated advisor in the Office of International Education (OIE) to ensure compliance with immigration regulations. 3.2 Full-time Status All MCDS students are expected to enroll full-time (at present, there is no option to pursue the degree as a part-time student). To be considered a full-time student, a student must be registered for, and complete, a minimum of 36 units in every Fall and Spring semester. All international students are required by US Federal law to maintain full-time status. Students can have no more than one (1) remote course counting toward the 36 units used to satisfy full-time enrollment. Failure to maintain full-time status will result in loss of a student visa (and, therefore, \\u201cpermit of stay\\u201d). All students having a Stafford Loan are required to maintain full-time status. 3.3 MCDS Degree Enrollment Process and Related Information 3.3.1 Duration of the degree program The MCDS degree must be completed within five (5) years from the time that the student matriculates into the program. https://www.cmu.edu/policies/student-and-student-life/masters-students-statute-of- limitations.html As outlined in the Master\\u2019s Students Statute of Limitations (see link above), students will complete all requirements for the master\\u2019s degree within a maximum of seven years from original matriculation as a master\\u2019s student, or less if required by a more restrictive department, school or college policy. Once this time-to-degree limit has lapsed, the person may resume work towards a master\\u2019s degree only if newly admitted to a currently offered master\\u2019s degree program under criteria determined by that program. Under extraordinary circumstances, such as leave of absence, military or public service, family or parental leave, or temporary disability, a school or college may, upon the relevant department's recommendation and with the written approval of the dean (or designate), defer the lapse for a period commensurate with the duration of that interruption. Students who are pursuing a master\\u2019s degree as part-time students for all semesters of their program, as approved by their program, may also appeal to their program or department for extension of the time to degree limit. 3.3.2 Residency requirements There is no formal residence requirement. However, most courses in the program are taught on campus without an option for distance education. Students enrolled in in- person course sections (IPR) are expected to attend all class meetings in person. 3.3.3 Degree Certification: Course requirements and related policies/protocols \\uf0b7 In order to receive the MCDS degree, students must have a Quality Point Average (QPA) of 3.0. Completion of the degree is based on completing courses in the core curriculum, the MCDS seminar, electives and capstone project. \\uf0b7 The capstone project consists of students working at CMU on a research project, or on an industry-sponsored project. \\uf0b7 The student must complete 144 eligible units of study. This includes eight core and concentration courses, two 12-unit seminar courses and two 12-unit capstone courses. All students complete a common MCDS core in their first two semesters, consisting of five courses. All students must also complete at least one concentration, consisting of three courses in one of three areas: Analytics, Systems, or Human-Centered Data Science. The remaining elective course can be any course with number 600 or above chosen from the SCS course catalog. Any additional non-prerequisite units taken beyond the 144 units are also considered electives. \\uf0b7 To maintain full-time enrollment status, a student must enroll in a minimum of 36 course units per semester. A student may not take more than 60 units per semester, without permission from their academic advisor. Students must maintain full-time enrollment status (minimum of 36 units) in their final semester. 3.3.4 Prerequisite Core Course All MCDS students are expected to pass the 11-637 Foundations of Computational Data Science course by the end of their first semester. Each student must pass 11-637 with a grade of \\u201cB\\u201d or better. 3.3.5 Plan of study The degree consists of two timing options based on the length of time the student spends working on the degree. The student chooses their timing at the start of the degree program (for visa requirements). Changes in timing are possible with the approval of the Director of the degree program and successful visa extension application with CMU\\u2019s Office of International Education. Note that all degree options consist of the same amount of coursework: \\uf0b7 Professional Preparation Track \\u2013 a 16-month degree consisting of study for Fall and Spring semesters, a summer internship, and Fall semester of study. Each semester consists of a minimum of 48 units of study. This timing is typical for most students. The student graduates in December. \\uf0b7 Research Preparation Track \\u2013 a 20-month degree consisting of study for Fall and Spring semesters, a summer internship, and a second year of Fall and Spring study. Each semester consists of a minimum of 36 units of study. This timing is designed for students interested in extending their time at CMU for developing applied research skills in preparation for further graduate study or research- oriented employment. Note that the per-semester course load is lower, but the total cost is higher since four semesters of tuition are paid. This timing is also recommended for students interested in pursuing a PhD after graduation. The student graduates in May. 3.3.6.1 MCDS Curriculum All MCDS students must complete 144 units of graduate study which satisfy the following curriculum: \\uf0b7 11-637 - Foundations of Computational Data Science \\uf0b7 Four (4) additional MCDS Core Courses (10-601 Introduction to Machine Learning; 05-839 Interactive Data Science; 15-619 Cloud Computing; 11-631 Data Science Seminar; 48 units) \\uf0b7 Three courses (3) from one area of concentration curriculum (36 units) \\uf0b7 Three (3) MCDS Capstone courses (11-634, 11-635 and 11-632) (36 units) \\uf0b7 One (1) Elective: any graduate level course 600 and above in the School of Computer Science (12 units) 3.3.6.2 Common MCDS Core Courses All MCDS students are required to complete four common core courses in their first two semesters: \\uf0b7 10-601 - Machine Learning \\uf0b7 15-619 - Cloud Computing \\uf0b7 05-839 - Interactive Data Science \\uf0b7 11-631 - Data Science Seminar 3.3.6.3 Areas of Concentration In addition to the common MCDS core, all students must complete at least one area of concentration, which consists of three courses in Analytics, Systems, or Human- Centered Data Science. Students consult with their academic advisor and choose one or more areas of concentration during their first semester, in preparation for enrolling in Spring classes. \\uf0b7 Analytics concentration: o One (1) Machine Learning course o One (1) Software Systems course o One (1) big data course \\uf0b7 Systems concentration: o 15-513 Introduction to Computer Systems (elective, prerequisite for many advanced Systems courses) o Three (3) systems project courses \\uf0b7 Human-Centered Data Science concentration: o One (1) Methods course o Two (2) HCI courses A detailed list of courses satisfying each concentration is contained in the MCDS Program FAQ. 3.3.6.4 MCDS Capstone Courses All MCDS students complete three Capstone courses: \\uf0b7 11-634 - Capstone Planning Seminar (12 units) \\uf0b7 11-635 - Capstone Research (12 units) \\uf0b7 11-632 - Data Science Capstone (12 units) MCDS Program Learning Outcomes \\uf0b7 Design, implement and evaluate the use of analytic algorithms on sample datasets. \\uf0b7 Explain how a machine-learning model is developed for and evaluated on real world datasets. \\uf0b7 Design and execute experimental data collection and present resulting analyses using appropriate user experience (UX) techniques including interactive data visualizations. \\uf0b7 Apply and customize analytics, systems and human-centered data science techniques to application-specific data science requirements and objectives. \\uf0b7 Identify tradeoffs among data science techniques (analytics, systems and/or human-centered) and contrast design alternatives, within the context of specific data science application domains. \\uf0b7 Survey, interpret and comparatively criticize state of the art research talks and papers, with emphasis on constructive improvements. \\uf0b7 Organize, execute, report on, and present a real world data science project in collaboration with other researchers/programmers. Depending on the concentration, additional learning outcomes are emphasized: Analytics. Students electing to complete the Analytics concentration will also learn to: \\uf0b7 Design, implement and evaluate a software system and machine-learning model on real world datasets at real world scale. \\uf0b7 Analyze and document data science requirements in different application domains and survey as well as critique state of the art solutions for those requirements. Systems. Students electing to complete the Systems concentration will also learn to: \\uf0b7 Implement and evaluate complex, scalable data science systems, with emphasis on providing experimental evidence for design decisions. \\uf0b7 Anticipate and avert structural and/or implementation problems with systems design, especially with scaling and tail distributions. HCDS. Students electing to complete the Human-Centered Data Science (HCDS) concentration will also learn to: \\uf0b7 Design, implement and evaluate a user experience prototype to allow for clear understanding of data science solutions. \\uf0b7 Apply social and behavioral research methods to data science problems to understand the human aspects of data collection and analysis. Throughout their coursework, students will take introductory courses on all those topics, practice them in advanced courses and seminars and demonstrate all learned skills in their Capstone project and internship. Students are encouraged to choose elective courses in the curriculum according to their professional goals and mastery of the main subjects. 3.3.10 Capstone project The capstone project consists of students working in a team or individually on a project. The capstone project integrates the educational experience of the student. A capstone project is typically a CMU research project, or an industry sponsored project; occasionally students define capstone topics through communication with the faculty. Students interested in defining their own capstone topics should discuss with the MCDS faculty as early as possible. The capstone project is a great opportunity for a student (or student team) to \\u201cshow off\\u201d their unique skills and accomplishments. Capstone projects have been instrumental in the hiring decisions for several employers. 3.3.11 Elective courses Electives can be any graduate level course (numbered 600 or above) in the School of Computer Science. Students use their elective courses to enhance study in an area of interest or to explore new areas of interest. 3.3.12 Undergraduate courses Undergraduate courses are taken to address an area of weakness in the student\\u2019s prior preparation. Undergraduate courses (numbered less than or equal to 599) may be taken pass/fail or for credit but do not count toward the 144 units of eligible study: The course and course grade will appear on the student\\u2019s transcript, and the course grade will factor as part of the student\\u2019s QPA. 3.3.13 Independent study course Independent study courses allow students to cover study of a particular area of interest, and are used when no formal course is available in a given subject area. Students who are interested in continuing to a Ph.D. degree often enroll in Independent Study, since it offers the opportunity to perform research directly with a faculty member. Independent study courses are considered electives. Each independent study course must be advised and approved by at least one faculty member. Agreement to supervise an independent study course is purely voluntary on the part of the faculty member. It is the duty of the student, therefore, to negotiate the terms and conditions of the independent study with the pertinent faculty members of CMU who will be supervising the study. These individuals are referred to as \\u201cindependent study supervisors.\\u201d Once the student finds someone who agrees to supervise such a course, he/she must: 1. Students wishing to take an independent study must request approval from their academic advisor and complete proposal before the first day of classes in a given semester: 2. Enter into an agreement with the independent study supervisor that includes course expectations, including deliverables. 3. Secure the \\u201cIndependent Study Contract Form\\u201d from the MCDS administrator. 4. Complete the form, provide a brief description of the work to be done, including deliverables and how they will be graded. 5. Secure signatures of both the student and the supervisor. Return the form to the MCDS administrator in order to obtain approval for the independent study from the Director. Independent study contracts must be submitted no later than on the last day of the first week of classes in a given semester. 3.3.14 Double counting courses No course may be used to complete two MCDS degree requirements, nor may a course satisfy requirements in two degree programs. 3.3.15 Courses outside of the School of Computer Science Elective courses in other Schools at Carnegie Mellon may be taken with prior permission of the Director. 3.3.16 Grades All courses offered by the SCS CMU are graded on the 4.3 grading standard http://www.cmu.edu/policies/documents/Grades.html. MCDS students must maintain a 3.0 overall average each semester to remain in good standing. A student must obtain a B- or better grade in all courses, which count towards core requirements. If a student receives a C- or better, that course may count as an elective towards the degree requirements. All courses must receive a letter grade; courses taken pass/fail do not count towards the MCDS degree. Enrollment Services is the only University office that can provide an official letter of enrollment, official transcript and enrollment verification. Enrollment verification can be requested online through The HUB at: https://www.cmu.edu/hub/registrar/student- records/verifications/ 3.3.17 Student Review, Academic Probation and Academic Actions The MCDS program conducts an academic progress review at the conclusion of each semester in order to monitor individual student progress towards graduation regarding the fulfillment of curricular requirements, course grades, and academic integrity. Should a student\\u2019s effort fall below the acceptable level of academic performance and/or fail to meet standards and policies established by Carnegie Mellon University, the student may be dismissed from the program. Infractions After each academic progress review, each student will receive a letter indicating the result of the review and their standing in the program. If applicable, the letter will also note the following infractions by the student in the given semester: \\uf0b7 Cumulative QPA is below 3.0, resulting in the student being put on Academic Probation (see below) \\uf0b7 Cumulative QPA is below 2.6, resulting in academic probation or possible dismissal (see below) \\uf0b7 Academic Integrity Violation (AIV) deemed an infraction by the MCDS committee (see 3.7.2) Minimum QPA and Academic Probation Students must maintain a cumulative QPA of 3.0 to remain in good standing with the program. Should a student\\u2019s overall QPA drop below 3.0 during any given semester, he/she will be placed on academic probation for the following semester. In probation cases, the student will be required to \\uf0b7 enroll in courses as advised by the academic advisor, \\uf0b7 improve his/her grades to no less than an overall 3.0 QPA in the following semester, and \\uf0b7 meet any other goal set by the advisor during that period (e.g., fulfilling a core course requirement). If a student\\u2019s cumulative QPA drops below 2.6, the student will be considered at risk of being unable to complete the program and will be considered for dismissal. He/she will be required to meet the program director to discuss his/her situation. Only if, after that meeting, the MCDS program committee ascertains that the student is likely to complete the remaining program requirements in the allotted time, the student will be allowed to continue his/her studies in MCDS, and dismissed otherwise. If the student is allowed to continue their studies, they will be placed on academic probation for the following semester and is subject to the requirements above. Dismissal A student may be dismissed from the program for any of the following cases: \\uf0b7 If the student has been put on academic probation and failed to meet the remedial requirements set by the advisor in the following semester, or committed an Academic Integrity Violation deemed an infraction by the MCDS committee while on academic probation \\uf0b7 If the student has a cumulative QPA of 2.6 or lower and the MCDS program committee does not ascertain that the student is likely to complete the remaining program requirements in the allotted time \\uf0b7 If the student has committed two Academic Integrity Violation deemed infractions by the MCDS committee (see 3.7.2) \\uf0b7 If the student has committed an Academic Integrity Violation infraction where the violation is deemed to be sufficiently egregious as determined by the MCDS program committee \\uf0b7 If the student has been found to infringe a University Policy, where such infringement is deemed grounds for dismissal Students who realize that one of these situations may apply to them are strongly encouraged to meet with the academic advisor to discuss a plan to mitigate the situation. Students who find they are struggling in the program will have the best chances of success if they communicate early and often with the academic advisor. 3.3.18 Incomplete grades Carnegie Mellon University students are expected to complete a course during the academic semester in which the course was taken. However, if the instructor agrees, a grade of \\u201cI\\u201d (incomplete) may be given when a student has been unable to complete the work of a course. However, the work completed up to that date must be of passing quality and the grade of incomplete provides no undue advantage to that student over other students. By awarding an \\u201cI\\u201d grade, an instructor must specify the requirements for the completion of the work and designate a default letter grade in the event that the student fails to complete the remaining work. Students must complete the required course work by no later than the end of the following academic semester or sooner if required by the instructor. The instructor must record the permanent course grade by the last day of the examination period of the following semester, or the Registrar will automatically assign the default grade. If further work has not been completed after one semester and a default grade is rendered, the default grade will become the grade of record. 3.3.19 Change of grades and missing grades If a grade has been assigned in error, it can be changed to a different permanent grade. The procedure for changing a grade is as follows: \\uf0b7 Discuss the matter with the course instructor; provide evidence that the grade issued was not the grade earned. \\uf0b7 If the instructor agrees, the student should contact the program administrator to process a Change of Grade Form in order to correct the grade that was issued in error. Generally, the instructor is the final authority for a course grade. \\uf0b7 If a grade has not been assigned, please notify the course instructor for the completion of a Missing Grade Form. 3.3.20 Qualifying examinations and procedures (or equivalent) None required. 3.3.21 Thesis/dissertation None required. 3.3.22 On transfer to another program If the requirements for the MCDS degree have not been completed when a student leaves to pursue another academic program, the degree will not be awarded. Completion of the MCDS degree does not guarantee admission into any doctoral degree program at Carnegie Mellon University. The courses that will be completed as part of the MCDS may serve to enhance one\\u2019s application to these programs but will in no way insure admittance. 3.3.23 Intellectual property policy The MCDS degree program adheres to Carnegie Mellon University policy on intellectual property: http://www.cmu.edu/policies/documents/IntellProp.html 3.3.24 Teaching requirements None required. However, students are encouraged to apply for teaching assistant positions in courses where they have excelled. 3.3.25 Language proficiency requirements None required. However, non-native English speakers are encouraged to take advantage of the various support functions provided by the Intercultural Communication Center (ICC) and the Global Communication Center (GCC). 3.3.26 Academic Integrity and Policies on Plagiarism and Cheating The university considers any form of cheating or plagiarism to be a serious violation of student ethics. The student is required to understand and rigorously follow only the permitted forms of collaboration as defined by the instructor in every class. The work you submit must be your own, unless you have clearly attributed it to others. You must not use the work of others without proper citation. And, you must not use resources, including other persons, except as authorized by the course or project for which you are submitting the work. Such conduct might be accepted or commonplace elsewhere, but it is not here. Be careful. Be warned. Failure to abide by these rules, even just once, can result in your permanent separation from the University without refund of money paid. Note that the policy requires the student to be informed and understand the academic integrity rules for every assignment or exam in a course. The MCDS program strives to produce graduates with the highest standards of academic integrity. Academic Integrity Violations are taken very seriously and the MCDS program has a zero tolerance policy for multiple Academic Integrity Violations. A single violation is grounds for dismissal from the graduate program if deemed sufficiently egregious as determined by the MCDS program committee. If a student commits a second violation, the expected penalty is dismissal from the graduate program (see also academic progress review at section 3.3.17). Please review the University Policy on Academic Integrity: https://www.cmu.edu/policies/student-and-student-life/academic-integrity.html The policy includes the University expectations around academic integrity and provides definitions of cheating, plagiarism, and unauthorized assistance. A review of the University\\u2019s Academic Disciplinary Actions procedures https://www.cmu.edu/student-affairs/theword/academic-discipline/index.html is also recommended. These procedures outline the process for investigating, reporting, and adjudicating violations of the University Policy on Academic Integrity. The procedures also outline the appeal process. 3.3.27 Process for Appealing Final Grades https://www.cmu.edu/student-affairs/theword/academic/appeal-of-grades-and-academic-actions.html Final grades will be changed only in exceptional circumstances and only with the approval of the instructor and the department, unit or program. Grading is a matter of sound discretion of the instructor and final grades are rarely changed without the consent of the instructor who assigned the grade. The following circumstances are the unusual exceptions that may warrant a grade appeal: (a) the final grade assigned for a course is based on manifest error (e.g. a clear error such as arithmetic error in computing a grade or failure to grade one of the answers on an exam), or (b) the faculty or staff member who assigned the grade did so in violation of a University policy. 3.3.27 Teaching Assistants The MCDS degree does not have a teaching requirement. However, some students may wish to be a teaching assistant. MCDS students may petition for approval to TA up to one course per semester provided that they are in good academic standing (Overall QPA 3.0 or above). As required by the English Fluency in Higher Education Act of 1990, graduate students are required to have a certain level of fluency in English before they can instruct in Pennsylvania. Through this Act, all institutions of higher education in the state are required to evaluate and certify the English fluency of all instructional personnel, including teaching assistants and interns. The full university policy can be reviewed at: http://www.cmu.edu/policies/faculty/evaluation-certification-english-fluency- instructors.html. In addition to administering the International Teaching Assistant (ITA) Test (mandatory screening test for any non-native speaker of English), Language Support in the Student Academic Success Center helps teaching assistants who are non-native English speakers develop fluency and cultural understanding to teach successfully at Carnegie Mellon. Visit the website for additional information: https://www.cmu.edu/student-success/ 3.3.28 Internship Requirement and Search for Full Time Employment An internship is required for the degree program. In some cases, when a student has prior work experience, the Director of the degree program may waive this requirement. Students start searching for internships generally in the Fall and Spring semesters. Once the student returns from an internship in the Fall, they should immediately begin the search for full time employment. Extensive resources are available at http://www.cmu.edu/career/ including a resume submission system, a list of employers, on campus interviews and mock interviews, and many other resources. The Technical Opportunities Conference (TOC) http://engineering.cmu.edu/companies/toc/ occurs every September. This conference is one of the main recruiting events each year. All international students are required to apply for Curricular Practical Training (CPT). CPT is employment that is an integral part of an established curriculum and is directly related to the student\\u2019s major area of study. Please visit the Office of International Education (OIE) link below to learn more about the CPT process: http://www.cmu.edu/oie/forstu/jobs.html 3.4 Enrollment Verification Enrollment Services is the only University office that can provide an official letter of enrollment, official transcript and enrollment verification. Enrollment verification can be requested online through The HUB via this link: http://www.cmu.edu/hub/transcripts/verifications/enrollment.html 3.5 University Policies on Grades and Grading 3.6.1 University Policy on Grades This policy offers details concerning university grading principles for students taking courses and covers the specifics of assigning and changing grades, grading options, drop/withdrawals and course repeats. It also defines the undergraduate and graduate grading standards. You can review the university grading policies here: https://www.cmu.edu/policies/student-and-student-life/grading.html 3.6.2 University Policy on Grades for Transfer Courses Carnegie Mellon University offers students the opportunity to take courses for credit through a cross-registration program (see Pittsburgh Council on Higher Education (PCHE) and Cross-registration below) and through the receipt of transfer credit from other accredited institutions. The Carnegie Mellon University transcript will include information on such courses as follows: Carnegie Mellon courses and courses taken through the university\\u2019s cross-registration program will have grades recorded on the transcript and be factored into the QPA. All other courses will be recorded on this transcript indicating where the course was taken, but without grades. Such courses will not be taken into account for academic actions, honors or QPA calculations. (Note: suspended students may take courses elsewhere; however, they may receive transfer credit only if their college\\u2019s and department\\u2019s policies allow this.) You can review the university policy here: https://www.cmu.edu/policies/student-and-student-life/transfer-credit-evaluation-and- assignment.html 3.6 Academic Integrity 3.7.1 Expectations Regarding Proper Conduct In the midst of self-exploration, the high demands of a challenging academic environment can create situations where some students have difficulty exercising good judgment. Academic challenges can provide many opportunities for high standards to evolve if students actively reflect on these challenges and if the community supports discussions to aid in this process. It is the responsibility of the entire community to establish and maintain the integrity of our university. Carnegie Mellon University educates its students to become professionals who will serve society with integrity. The university also creates and disseminates new knowledge and expressions of knowledge in ways that benefit society. Carnegie Mellon strives to serve the changing needs of society through the three primary goals outlined in its mission statement: to create and disseminate knowledge and art through research and artistic expression, teaching and learning and transfer to society, to serve students by teaching them leadership and problem-solving skills, and the values of quality, ethical behavior, responsibility to society and commitments to work, to pursue the advantages provided by a diverse community, open to the exchange of ideas, where discovery and artistic creativity can flourish. In any presentation, creative, artistic or research, it is the ethical responsibility of each student to identify the conceptual sources of the work submitted. Failure to do so is dishonest and is the basis for a charge of cheating or plagiarism, which is subject to disciplinary action. Please review the University Policy on Academic Integrity (https://www.cmu.edu/policies/student-and-student-life/academic-integrity.html). The policy includes the University expectations around academic integrity and provides definitions of cheating, plagiarism, and unauthorized assistance. A review of the University\\u2019s Academic Disciplinary Actions procedures (https://www.cmu.edu/student-affairs/theword/academic-discipline/index.html) is also recommended. These procedures outline the process for investigating, reporting, and adjudicating violations of the University Policy on Academic Integrity. The procedures also outline the appeal process. 3.7.2 Protocol for Academic Integrity Violations The university has a very clear and specific protocol for responding to alleged violations of academic integrity. Carnegie Mellon's Academic Disciplinary Actions Overview for Graduate Students describes procedures and the appeal process for disciplinary actions against graduate students in cases of alleged academic integrity violation. For more information on disciplinary actions please see: https://www.cmu.edu/student- affairs/theword/acad_standards/creative/disciplinary.html Further documentation on how to respond to an allegation of a violation as a graduate student: https://www.cmu.edu/student-affairs/ocsi/academic-integrity/grads.html Important Note: MCDS implements the above policy\\u2019s option of \\u201cconven[ing] a disciplinary hearing according to the procedures of the department/program\\u201d. We have adopted the following hearing procedure and \\u201ctwo strikes\\u201d rule: \\uf0b7 If an instructor determines that an academic integrity violation has occurred, both the instructor and students are given the opportunity to explain the situation to the MCDS program committee. A written hearing by email suffices for this purpose. \\uf0b7 The program committee then reviews all information and decides whether the violation is deemed an infraction (see 3.3.17), and which secondary actions are to be taken on the program level. \\uf0b7 Two-Strike-Rule: MCDS may dismiss students upon a first AIV infraction. A second infraction will always lead to the offending student being dismissed from the program, with no exceptions. The MCDS program reserves the right to withdraw a degree even though it has been granted should there be discovery that the work upon which it was based or the academic records in support of it had been falsified. In such a case, the degree will be withdrawn promptly upon discovery of the falsification. The complete reference to this university policy is available at: https://www.cmu.edu/policies/student-and-student- life/withdrawal-of-a-degree.html 4 Academic Policies 4.1 MCDS Academic Policies 4.1.1 Duration of Study MCDS students enrolled for full-time studies are normally expected to complete the degree in three semesters (16 months). This includes a summer internship. 4.1.2 Double-Dipping A Masters student who uses courses taken as part of another degree program (at Carnegie Mellon or elsewhere) toward their program requirements cannot use those same courses toward any other M.S. degree offered by the School of Computer Science without prior approval. 4.1.3 Pass/Fail and Audit Grades Pass/fail and audit grades are not permitted for courses used to satisfy a degree requirement. Graduate students who are required to take additional undergraduate courses to build up the core foundations of computer science may not elect the pass/fail or audit option for these courses. 4.1.4 Transfer Credit An equivalent graduate course previously completed at Carnegie Mellon, or another institution, may be permitted to satisfy one of the MCDS course requirements, with permission from the Director. Students must petition for transfer credit by providing the Program Director with the prior course syllabus and other details that may be required by the Director in order to make a decision. See the section on \\u201cDefinition of transfer credit versus course exemption\\u201d. NOTE: In all cases, credit may only be transferred from another degree program for courses deemed \\u201cfree electives\\u201d - i.e., courses which were not used to satisfy a core requirement or total units requirement in a prior degree program. All MS students are required to take a minimum of 96 units of coursework at CMU. 4.1.5 External Internships and Job Interviewing MCDS students are expected to attain an external internship during the summer. International students must coordinate carefully with the University, due to visa restrictions. International students are required to consult with the Office of International Education for eligibility before seeking an internship/co-op or signing an offer contract. We caution all students to be aware of potential intellectual property (IP) implications with internships, and to review any IP agreements with their academic advisor before signing them. It is possible to lose ownership of your own inventions if they occur during an external internship. While it is necessary for students to travel off-campus for job interviews, it is not acceptable for a student to miss a course requirement or a capstone project commitment due to interview travel. Students should work proactively with prospective employers to arrange interview travel in a way that minimizes the impact on their final semester course work. 4.1.6 Transferring into the MCDS Program Direct transfers into the MCDS program are not permitted. Students who are currently enrolled at Carnegie Mellon who wish to transfer into the MCDS program must do so by applying to the MCDS program via the normal admissions process. As specified in Sec. 4.1.4 in this document, some transfer credit and/or exemption from MCDS requirements may be possible on a case-by-case basis. 4.1.7 Transferring Out of the MCDS Program The MCDS program does not prevent students from transferring to another degree program. Each degree program has its own rules about whether and when transfers into the program are permitted. A student that is interested in transferring out of the MCDS degree program should consult the handbook and Program Director of the desired degree program to learn whether transfers are permitted, and if so, how and when to request such a transfer. 4.1.8 Statute of Limitations https://www.cmu.edu/policies/student-and-student-life/masters-students-statute-of- limitations.html As outlined in the Master\\u2019s Students Statute of Limitations (link above), students will complete all requirements for the master\\u2019s degree within a maximum of seven years from original matriculation as a master\\u2019s student, or less if required by a more restrictive department, school or college policy. Once this time-to-degree limit has lapsed, the person may resume work towards a master\\u2019s degree only if newly admitted to a currently offered master\\u2019s degree program under criteria determined by that program. Under extraordinary circumstances, such as leave of absence, military or public service, family or parental leave, or temporary disability, a school or college may, upon the relevant department's recommendation and with the written approval of the dean (or designate), defer the lapse for a period commensurate with the duration of that interruption. Students who are pursuing a master\\u2019s degree as part-time students for all semesters of their program, as approved by their program, may also appeal to their program or department for extension of the time to degree limit. 4.2 LTI Academic Policies 4.2.1 \\u201cGrandfather\\u201d policy A student can graduate under the policies in effect at the time that the student entered the program; or, at the student's choice, the student can graduate under policies that are adopted after the student entered the program. In unusual cases, the Director may approve exceptions to the program requirements. 4.2.2 Course Drop/Add/withdrawal procedures Students taking undergraduate and Master\\u2019s level courses must follow the procedures and deadlines for adding, dropping, or withdrawing from courses as identified on the academic calendar. Information can be found at: https://www.cmu.edu/hub/registrar/course-changes/index.html. There is a separate calendar for masters level courses. 4.2.3 Courses with restricted enrollment MCDS students have priority for the program core courses. The MCDS program administrators cannot intercede with other departments to secure seats for its students in other courses as all departments reserve seats for their accepted students. Usually all remaining open seats are assigned on a first- come, first-served basis. Students can, on occasion, contact the assigned course instructor in order to plead his/her case for admission to the course. Admission may be granted at the discretion of the instructor. The policy of the department offering the course(s) is always followed. 4.2.4 Definition of transfer credit versus course exemption The LTI may grant transfer credit or issue an exemption for equivalent graduate courses previously completed at another institution. This decision rests with the Director of the particular program. If a student is exempt from a required course due to prior courses or experience, the student can replace that course with an open elective. The student does not receive credit for the external course but can take any course that could normally count toward the degree in its place. If a student receives credit for prior coursework completed at CMU or elsewhere, the student receives that many units of credit, and the total amount of required coursework is reduced by that amount. 4.2.5 External Employment/Consulting Since the MCDS program places heavy demands on student time, external employment and/or consulting are strongly discouraged. Exceptional students who wish to consult should discuss this with their advisor. International students must also have approval in advance from the Office of International Education (OIE) for any outside employment. 4.2.6 Leave of Absence A student in good standing may be granted a LOA of at most 1 year, upon written request to the Program Director and with consent of the student's advisor. It is the responsibility of the student on LOA to contact the program administrator to apply for a return to the program. 4.2.7 Withdrawal from Program Students may voluntarily withdraw from the MCDS program. If a student decides to withdraw, or is considering a withdrawal, she/he should contact the program administrator to schedule an advising meeting as soon as possible. The university\\u2019s general withdrawal policy can be found here: https://www.cmu.edu/hub/registrar/leaves-and-withdrawals/ 4.2.8 Satisfactory Progress If a student does not make satisfactory progress each semester toward completing the degree, the LTI may remove the student from the program. See section on \\u201cEnd of Semester Evaluation\\u201d. In particular, students in the three-semester program who fail one of their first-semester MCDS core required courses are strongly encouraged to consider switching to the four-semester program. 4.2.9 Winter and Summer Breaks Students supported by research projects or working in an on-campus internship are expected to remain on campus working during breaks in classes. A two-week vacation is typically allowed in the summer for the students who are working on campus (not pursuing an external internship). Supported students should arrange their winter break time with their supervisor. 4.3 CMU Academic Policies 4.3.1 Assistance for Individuals with Disabilities http://www.cmu.edu/education-office/disability-resources/ The Office of Disability Resources at Carnegie Mellon University has a continued mission to provide physical, digital, and programmatic access to ensure that students with disabilities have equal access to their educational experience. We work to ensure that qualified individuals receive reasonable accommodations as guaranteed by the Americans with Disabilities Act (ADA) and Section 504 of the Rehabilitation Act of 1973. Students who would like to receive accommodations can begin the process through Disability Resources' secure online portal (https://rainier.accessiblelearning.com/cmu/) or email access@andrew.cmu.edu to begin the interactive accommodation process. Students with physical, sensory, cognitive, or emotional disabilities are encouraged to self-identify with the Office of Disability Resources and request needed accommodations. Any questions about the process can be directed to access@andrew.cmu.edu, or call (412) 268-6121. 4.3.2 Summary of Graduate Student Appeal and Grievance Procedures Graduate students will find the Summary of Graduate Student Appeal and Grievance Procedures on the Graduate Education Resource webpage. This document summarizes processes available to graduate students who seek review of academic and non-academic issues. Generally, graduate students are expected to seek informal resolution of all concerns within the applicable department, unit or program before invoking formal processes. When an informal resolution cannot be reached, however, a graduate student who seeks further review of the matter is to follow the formal procedures outlined here. These appeal and grievance procedures shall apply to students in all graduate programs of the University. Students should refer to the department specific information in this handbook for department and college information about the administration and academic policies of the program. You can review a summary of the university\\u2019s graduate student\\u2019s appeal and grievance procedures here: https://www.cmu.edu/graduate/policies/appeal-grievance-procedures.html 4.3.3 Safeguarding Educational Equity: Sexual Harassment and Sexual Assault The University prohibits sex-based discrimination, sexual harassment, sexual assault, dating/ domestic violence and stalking. The University also prohibits retaliation against individuals who bring forward such concerns or allegations in good faith. The University\\u2019s Sexual Misconduct Policy is available at https://www.cmu.edu/policies/administrative-and-governance/sexual- misconduct/index.html. The University\\u2019s Policy Against Retaliation is available at: https://www.cmu.edu/policies/administrative-and-governance/whistleblower.html If you have been impacted by any of these issues, you are encouraged to make contact with any of the following resources: \\uf0b7 Office of Title IX Initiatives: http://www.cmu.edu/title-ix/, 412-268-7125, tix@cmu.edu \\uf0b7 University Police: https://www.cmu.edu/police/, 412-268-2323 \\uf0b7 Additional resources and information can be found at: https://www.cmu.edu/title-ix/resources-and-information/index.html 4.3.4 Consensual Intimate Relationship Policy Regarding Undergraduate Students https://www.cmu.edu/policies/student-and-student-life/consensual-relationships.html This policy addresses the circumstances in which romantic, sexual or amorous relationships/interactions with undergraduate students, even if consensual, are inappropriate and prohibited. The purpose of this policy is to assure healthy professional relationships. This policy is not intended to discourage consensual intimate relationships unless there is a conflicting professional relationship in which one party has authority over the other as in the policy. 4.3.5 Gestational and Parental Accommodations https://www.cmu.edu/graduate/programs-services/maternity-accommodation- protocol.html Students whose anticipated delivery date is during the course of the semester may consider taking time away from their coursework and/or research responsibilities. Any student who gives birth to a child while engaged in coursework or research is eligible to take either a short-term absence or formal leave of absence. Students are encouraged to consult with relevant university faculty and staff as soon as possible as they begin making plans regarding time away. \\uf0b7 Students must contact the Office of the Dean of Student Affairs to register for Maternity Accommodations. Students will complete an information form and meet with a member of the Dean\\u2019s Office staff to determine resources and procedures appropriate for the individual student. o Planning for the student\\u2019s discussion with appropriate academic contact(s) (advisor, associate dean, etc.) may be reviewed during this meeting. \\uf0b7 Students in course work should consider options for the semester of the anticipated birth such as working with their course instructors to receive incomplete grades, electing to drop to part-time status or taking a full semester leave of absence. \\uf0b7 Students engaged in research must work with their faculty to develop plans for the research for the time they are away and for resuming upon return. \\uf0b7 Master's students who receive an academic stipend funded by Carnegie Mellon are eligible to continue to receive stipend funding for up to six (6) weeks during a Short-Term Maternity Accommodation or a Formal Leave of Absence. Continued academic stipend funding may be extended by two (2) weeks, for a total of eight (8) weeks, if an absence longer than six weeks is medically necessary. To receive this support students must be registered with the Office of the Dean of Students. 4.3.6 Change of Address MCDS students are responsible for notifying MCDS and the HUB of all address changes in a timely manner. Students will be held responsible for any failure to receive official college notices due to not having a correct address on file; F-1 students may jeopardize their status if address information is not kept current. Students can change their address using SIO, which is available via the HUB website: http://www.cmu.edu/hub/index.html 5 Financial Issues 5.1 Tuition 5.1.1 Tuition payments To understand your invoice, payment options, etc., visit: http://www.cmu.edu/hub. The MCDS program sets tuition each year for all students in the program. The student must handle tuition problems by contacting The Hub. The MCDS Program Administrators cannot solve tuition problems. 5.1.2 Sponsored Students A sponsored student is one who has another party (such as an embassy or company) who has agreed to pay the student\\u2019s tuition. Please read the \\u201cSponsored Students\\u201d section. 5.1.3 Employer Reimbursement Process If you have an employer reimbursement plan, write your employer\\u2019s name and address on the bill (or provide CMU with a letter of support) and pay at least one-third of the tuition charge when returning the bill. You must pay previous semester balances before you can enroll for the next semester. 5.1.4 Carnegie Mellon employee reimbursement procedure Contact the Benefits Office for specific information on tuition benefits. You must complete a Tuition Remission Form each semester in order to receive these benefits. To receive a Tuition Remission Form, visit the Human Resources website at: http://www.cmu.edu/hr. 5.1.5 Financial aid, tuition waivers, Scholarships The MCDS degree program does not provide financial aid to graduate students, nor does it provide tuition waivers or scholarships. For complete financial aid information see: https://www.cmu.edu/sfs/financial-aid/index.html . The Financial Aid Office of Carnegie Mellon will provide assistance in completing the necessary paperwork to apply for Stafford loans. Graduate students should consult the graduate student financial aid information found on The HUB website: https://www.cmu.edu/sfs/financial-aid/graduate/index.html . Students will find the Graduate Financial Aid Guide, information about funding options and how to apply for financial aid and other helpful links. Graduate students who find themselves in need of immediate funds for emergency situations should contact the Office of the Dean of Student Affairs (see Appendix A), www.cmu.edu/student-affairs/index.html, to inquire about an Emergency Student Loan. U.S. citizens and permanent residents may complete the Free Application for Federal Student Aid (FAFSA) online at https://fafsa.ed.gov Students may obtain information regarding their loans through the William D. Ford Direct Loan Program, including deferment forms and payment information at http://www.dlssonline.com/index.asp Information about the federal student aid programs may be found at www.studentaid.ed.gov 5.1.6 External fellowships The MCDS program will accept students with external fellowships. 5.1.7 Grade Reports Grade reports are mailed to individual students by the university at the conclusion of each semester. See the official calendar for mailing dates. 5.1.8 Late Graduation On occasion, a student and/or his/her employer may request that the student attend Carnegie Mellon for an additional semester in order to complete a program that will be beneficial to both the student and the employer. Under such circumstances late graduation can be arranged. Student is to discuss his/ her situation with the Director. 5.1.9 Employment Eligibility Verification If you are receiving a stipend, are a TA, or are planning to have a position with CMU then Employment Eligibility Verification is required. Form I-9 must be completed within 3 business days of beginning work for any type of compensation (stipend or employment). Additional details are highlighted below. To ensure compliance with federal law, Carnegie Mellon University maintains the Employment Eligibility Verification (I-9) Policy covering the university\\u2019s I-9 and E-Verify requirements: \\uf0b7 Every individual receiving a stipend from CMU or employed by CMU must comply with the I-9 Policy by completing the Form I-9 within three business days following the first day of stipend start date/employment. \\uf0b7 Individuals who expect to work on a federally funded project are further responsible for submitting an E-Verify Processing Request Form to the Office of Human Resources if required. \\uf0b7 For more information, please see CMU\\u2019s Guidance for Completing the Form I-9 and E-Verify Requirements at CM or visit the Human Resources Service website to learn more about Form I-9 and E-Verify and to schedule an appointment to complete the Form I-9. 5.2 Conference Travel Funding Students funded by a research project may receive travel funding according to policies set by the individual projects. Students who have no project funding may be provided with partial funding, with a larger amount available for travel to present a refereed conference paper. There is an LTI form that must be filled out in advance. An additional conference travel funding opportunity is provided by GSA and the Provost\\u2019s Office for students, student work groups or groups to attend a conference, whether as a participant or as a presenter. The process is managed by the Graduate Education Office. Students can find more information about the application process and deadlines at: https://www.cmu.edu/graduate/professional-development/index.html 5.3 Expenses The program will reimburse any expenses incurred on behalf of the MCDS program if approved by the Director. The student must apply for approval of expenses before they are incurred. Verification of purchase and/or expenses along with receipts is to be presented to the program administrator for reimbursement. Reimbursement requests must be filed within three months of the calendar date when it was incurred. If the reimbursement request concerns pre-approved conference or workshop travel expenses of an MCDS students, then it must be filed within three months of the calendar date of the event\\u2019s last day. Reimbursement requests received after this period will not be processed. The University does not reimburse for taxes. 5.4 Health Insurance https://www.cmu.edu/health-services/student-insurance/plans.html Carnegie Mellon has a Student Health Insurance policy requiring full-time, degree- seeking students to carry adequate medical insurance. Students must either purchase the plan offered by the University or an application for a waiver can be made if the student is \\u201cenrolled as the dependent, partner/spouse or principal in an employer or government-sponsored insurance plan\\u201d. It is the responsibility of each student to make arrangements with Student Health Services to either pay for their insurance at the beginning of the semester or elect a payment plan over the course of the academic year. More information is available at the Student Health Services Web site https://www.cmu.edu/health-services/student-insurance/plans.html. 5.5 Emergency Loans Graduate students who find themselves in need of immediate funds for emergency situations should contact the Office of the Dean of Students (see Appendix A), www.cmu.edu/student-affairs/index.html, to inquire about the types of emergency funding available to enrolled students. 6 Additional University Resources 6.1 The HUB Student Services Center The HUB is located in Warner Hall, Lower Level. The HUB staff delivers comprehensive service and counsel to students and families regarding financial aid, billing and payment, registration and academic records. The Assistant Directors in The HUB serve as contacts for specific colleges and assist enrolled students with key aspects of the enrollment process. Student can find their assigned HUB Assistant Director on their Student Information Online (SIO) Resource page. Questions that need specialized, in- depth attention can be directed to the student's assigned Assistant Director. For general questions and information, students may email The HUB or call 412-268-8186. thehub@andrew.cmu.edu and http://www.cmu.edu/hub/ 6.2 Student Information Online (SIO) Student Information Online (SIO) is a secure site where students can find important, personalized information, including E-Bills and student account information, financial aid status and eligibility, grades and QPA, and course schedules. Students can update their and their spouse's or domestic partner's contact information, sign up for E-Check & E-Refund, authorize their spouses, domestic partners or other individual to receive a copy of their E-Bill, request verifications, view their housing and meal plan assignments, and much more. Students can log on to SIO by going to www.cmu.edu/hub/sio and entering their Andrew User ID and password. On SIO, students will designate an emergency contact address of a relative or family friend to be contacted in the case of an emergency. If students do not want their name and address published in the campus directory, they must notify the HUB in writing. 6.3 ID Cards Graduate students can obtain their ID card from The HUB once they have been entered into SIO for the semester. These cards identify their holders as members of the campus community. Student cards are deactivated upon the cardholder\\u2019s separation from the university. Affiliate ID Cards are available for spouses and partners of graduate students that allow them to access Carnegie Mellon\\u2019s campus. These cards are available through The HUB to spouses and partners of graduate students who are enrolled for the current academic year in a full-time graduate degree program. The card is valid for one year. For information about domestic partner registration, visit the Office of the Dean of Student Affairs webpage: http://www.studentaffairs.cmu.edu/dean/domestic_partner/. For more information about student and affiliate ID cards (spouse, domestic partners and dependent children), please visit: http://www.cmu.edu/idplus/idcards/cardtypes.html. 6.4 Transcripts Information about and instructions for ordering transcripts are available at: www.cmu.edu/hub/transcripts.html. Transcript questions may be directed to esg- transcripts@andrew.cmu.edu. 6.5 Pittsburgh Council on Higher Education (PCHE) and Cross-registration Carnegie Mellon University offers students the opportunity to take courses for credit through a cross-registration program (see Pittsburgh Council on Higher Education (PCHE) and Cross-registration below) and through the receipt of transfer credit from other accredited institutions. The Carnegie Mellon University transcript will include information on such courses as follows: Carnegie Mellon courses and courses taken through the university's cross-registration program will have grades recorded on the transcript and be factored into the QPA. All other courses will be recorded on this transcript indicating where the course was taken, but without grades. Such courses will not be taken into account for academic actions, honors or QPA calculations. NOTE: Suspended students may take courses elsewhere; however, they may receive transfer credit only if their college's and department's policies allow this. You can review the PCHE cross-registration guidelines here: https://www.cmu.edu/policies/student-and-student-life/cross-college-university- registration.html 6.6 Student Privacy Rights and FERPA This university policy notifies students of their rights under the federal Family Educational Rights and Privacy Act (FERPA). According to FERPA, students have the right to: \\uf0b7 Inspect and review their education records; \\uf0b7 Request an amendment to their education record if they believe they are inaccurate or misleading; \\uf0b7 Request a hearing if their request for an amendment is not resolved to their satisfaction; \\uf0b7 Consent to disclosure of personally identifiable information from their education records, except to the extent that FERPA authorizes disclosure without consent; \\uf0b7 File a complaint with the U.S. Department of Education Family Policy Compliance Office if they believe their rights under FERPA have been violated. For questions about Student Privacy Rights, FERPA or filing a complaint, contact John Papinchak, University Registrar, jp7p@andrew.cmu.edu, in Enrollment Services. You can review the university\\u2019s policy on privacy rights here: https://www.cmu.edu/policies/student-and-student-life/privacy-rights- students.html 6.7 Academic Calendar The Academic Calendar can be found at https://www.cmu.edu/hub/calendar/index.html and provides information on all deadlines including registration dates, class start dates, add/drop deadlines, exam dates and more. 6.8 Professional Development The Career and Professional Development Center (CPDC) at Carnegie Mellon is a centralized career center staffed by a team of seasoned and highly educated professionals who orchestrate the career exploration, experiential learning, and career networking needs of students and alumni. CMU's career and professional development model is grounded in discipline-specific career development, experiential learning, and employer relations shaped by strong connections with the university's seven academic colleges. The center's success is founded upon a solid understanding of career and professional development theory, integration of technology, and an unwavering commitment to providing personalized attention towards meeting the unique individual needs of students, alumni, and employers. The CDPC is located on the Lower Level of the University Center, 412-268-2064 The Office of the Assistant Vice Provost for Graduate Education (AVPGE) offers a robust schedule of professional development opportunities. Some are geared towards a specific population (master\\u2019s students, PhD students at the beginning of their program, graduate students seeking tenure track positions, etc.) and others are open to all graduate students (time management, balancing, staying healthy). A full schedule of programs can be found at: http://www.cmu.edu/graduate/. http://www.studentaffairs.cmu.edu/career/index.html 6.9 University Libraries There are three main libraries at Carnegie Mellon: Hunt Library, Mellon Institute Library and Engineering & Science Library with the combined mission of providing access and help to graduate students in finding the information needed, teaching graduate students to evaluate available information and use reliable sources. The libraries\\u2019 digital resources and services, including off-campus/ wireless access to databases and e- journals, offer online access. There are also two neighboring libraries open to Carnegie Mellon graduate students: Carnegie Library of Pittsburgh and University of Pittsburgh Libraries. Visit the University Libraries website for information about all mentioned library locations and hours, on-line resources and FAQ\\u2019s. More information can be found at: http://www.library.cmu.edu/ 6.10 Computing Services Computing Services is located in Cyert Hall 285. Computing Services develops, maintains and supports the computing infrastructure for Carnegie Mellon students, faculty members and staff members. This includes the campus wired and wireless networks, public computer labs or \\u201cclusters,\\u201d cable television and telephone services, computing related documentation and support through the Help Center. In addition, Computing Services provides standard classroom technologies for over 100 lecture halls, classrooms and seminar rooms across campus. The website contains additional information regarding The Help Center hours, location and contact information, computing cluster hours and location, the Carnegie Mellon web portal, computing security and policies and guidelines. Students can email the Help Center at advisor@andrew.cmu.edu with questions and for assistance. More information http://www.cmu.edu/computing/ 6.11 Family and Dependents Resources The Graduate Student Assembly website maintains a resource page for spouses, domestic partners and families of graduate students, including The Student Parent Association, new mother rooms, and links to resources around campus and the Pittsburgh area. Affiliate ID Cards are available for spouses and domestic partners of graduate students that allow them to access Carnegie Mellon\\u2019s campus. These cards are available through The HUB to spouses and partners of graduate students who are enrolled for the current academic year in a full- time graduate degree program. The card is valid for one year. More information can be found at: http://www.cmu.edu/stugov/gsa/resources/family.html For more information about student and affiliate ID cards, please visit: http://www.cmu.edu/idplus/idcards/cardtypes.html. 6.12 Domestic Partner Registration Carnegie Mellon extends certain benefits to domestic partners of students. Eligible students may elect benefits for their domestic partners through the registration process orchestrated by the Office of the Dean of Student Affairs, located on the 3rd floor of Warner Hall. See the web site for information regarding the benefits available for domestic partners, eligibility for domestic partner benefits, registration instructions and forms. More information can be found at: http://www.studentaffairs.cmu.edu/dean/domestic_partner/index.html 6.13 Housing The University does not currently offer housing to graduate students. The Office of Housing and Dining Services does provide community housing information on a very limited basis to assist graduate students who are seeking housing in the communities surrounding the university, including information on the legal aspects of renting an apartment, moving checklists and the off-campus housing database. More information can be found at: www.cmu.edu/housing/community-housing/index.html 6.14 Dining Dining services and operations are offered through the Office of Housing and Dining Services. The office operates dining locations open around campus in academic buildings, Hunt Library and the University Center. These locations offer flexible hours with options from the early morning through late night. The Dining Service website contains information about dining locations, hours of operation, graduate student dining plans forms, nutritional information, and weekly menus for dining locations. More information can be found at: http://www.cmu.edu/dining/ 6.15 Parking and Transportation Graduate students will find information about parking and availability, parking policies, transportation options and Port Authority Transit usage with a valid university ID on the Parking and Transportation Services site. The Parking and Transportation Services office is located in the lower level of the University Center, LL#8. There is limited parking on campus and the varying permit rates can be found on the website. All parking areas of campus are either by permit, metered or by the hour in the garage. Parking and Transportation Services will ticket any car parked in a permit area without a permit or at an expired meter. The city monitors the metered parking along Margaret Morrison, Frew and Tech Streets and will ticket at expired meters as well. More information can be found at: http://www.cmu.edu/parking/ The University offers shuttle and escort services operated through University Police. The Shuttle Service operates several routes within Oakland, Squirrel Hill and Shadyside areas, as well as to university sites located outside of the main campus. The Escort Service offers vehicle routes within a radius of campus between 6:30 pm-6 am daily. Information regarding up-to-date shuttle and escort schedules, pick-up/drop-off locations, routes and usage policies can be found at: www.cmu.edu/police/shuttleandescort/. SafeWalk provides another option to campus community members walking across and around campus during late-night hours. SafeWalk is a student volunteer organization that provides campus escorts for all members of the Carnegie Mellon community. SafeWalk operates nightly during the regular academic year (except certain holidays and break periods) from 10pm until 2am. Students, faculty and staff may request an escort by calling 412-268-SAFE (8-7233 from a campus phone), by approaching an escort team, or by stopping by the SafeWalk dispatch area in the University Center, Lower Level near the Post Office Package Pick- Up window between 10pm-2am. SafeWalk will escort to locations approximately one mile from campus. Additional SafeWalk information can be found at: www.studentaffairs.cmu.edu/safewalk. 6.16 Copying, Printing and Mailing Services Carnegie Mellon offers community members easy access to FedEx, copy centers, printing and mailing services, and postal services. More information regarding these services, locations and contact information can be found at the provided link. More information can be found at: http://www.cmu.edu/student- affairs/theword/campus_resources/copyprintmail.html 6.17 University Center The University Center is a centerpiece of the campus that provides a space for special events, physical fitness, student organizations and various activities, as well as accommodating retail and dining services. As the campus crossroads, the University Center functions as a place for students to interact, get involved and enjoy new experiences. Visit the University Center website for information about campus eateries, ATMs and PNC Bank, fitness rooms and schedules, retail stores, scheduling University Center space, the public prayer room, student organizations and the Wright-Rogal Chapel. The University Center Information Desk is the location if you want to know about upcoming campus events or have questions about Carnegie Mellon in general, call the Information Desk at 412-268-2107. The Information Desk not only provides information about campus events, but also sells postage stamps, makes copies, sends faxes, distributes campus maps, manages a lost & found, and has information brochures about Pittsburgh and the campus. More information can be found at: http://www.cmu.edu/university-center 6.18 Athletic/Fitness Facilities For the participant or the spectator, Carnegie Mellon offers intercollegiate athletics, intramural sports, physical education classes and club sports. The Athletics Department also offers aerobics classes in the University Center and Skibo Gym as well as occasional workshops and instruction related to fitness and health. The Athletics Office is located in the Skibo Gymnasium. Skibo Gym facilities include courts for basketball, volleyball, badminton, as well as weight- training and aerobic equipment. The University Center\\u2019s recreational facilities include an eight- lane pool, racquetball and squash courts, aerobics room, fitness center and gym for basketball and volleyball. All users must present a current Carnegie Mellon Card to use these facilities. More information can be found at: http://www.cmu.edu/athletics 6.19 CMU Alert CMU Alert sends voice and/or text messages to registered phones in the event of a campus emergency that threatens public safety or during tests of the system in the Spring and Fall semesters. Students can register for CMU Alert through the web site. More information can be found at: http://www.cmu.edu/alert 6.20 Accidents on CMU property Please report all accidents to Charity Anderson (caanders@andrew.cmu.edu) and the MCDS program administrator. You will be asked to complete an accident report. By reporting accidents, the student helps minimize future harm! Serious accidents and accidents taking place elsewhere on campus should be reported to Campus Police, x8-6232 (non-emergency), x8-2323 (emergency). 6.21 Consumer Information Carnegie Mellon University suggests that all current and prospective students be informed consumers. Please see this link for detailed consumer information: https://www.cmu.edu/hub/consumer- information/. Appendix A 2023-2024 Highlighted University Resources for Graduate Students Table of Contents Key Resources for Graduate Student Support 1 Office of Graduate and Postdoctoral Affairs 1 Office of the Dean of Students 1 The Division of Student Affairs 2 Center for Student Diversity & Inclusion 2 Assistance for Individuals with Disabilities 3 Eberly Center for Teaching Excellence & Educational Innovation 3 Graduate Student Assembly 4 Office of International Education (OIE) 4 Veterans and Military Community 5 Carnegie Mellon Ethics Hotline 5 Policy Against Retaliation 5 Key Offices for Academic & Research Support 6 Computing and Information Resources 6 Student Academic Success Center 6 University Libraries 6 Research at CMU 7 Office of Research Integrity & Compliance 7 Key Offices for Health, Wellness & Safety 7 Counseling & Psychological Services 7 Health Services 8 Campus Wellness 8 Religious and Spiritual Life Initiatives (RSLI) 8 University Police 9 Shuttle and Escort Services 9 The WORD 10 7 Key Resources for Graduate Student Support 8 Office of Graduate and Postdoctoral Affairs https://www.cmu.edu/graduate graded@cmu.edu The Office of Graduate and Postdoctoral Affairs provides university-wide support for all graduate students and academic programs, with a focus on supporting graduate student success at Carnegie Mellon. Examples of resources offered through the Office of Graduate and Postdoctoral Affairs include, but are not limited to: \\uf0b7 Website with university resources, contact information for CMU programs and services, possible financial assistance and potential funding opportunities, and various procedural and policy information \\uf0b7 Newsletter to all graduate students with information on activities, resources, and opportunities \\uf0b7 Professional development seminars and workshops, and various programming and events for the graduate student community The Office of Graduate and Postdoctoral Affairs also works directly with the colleges and departments on issues related to graduate students and serve as a resource for developing policy and procedures. The Office of Graduate and Postdoctoral Affairs partners with many other offices and organizations, such as the Graduate Student Assembly, to support the holistic graduate student educational experience. 9 Office of the Dean of Students https://www.cmu.edu/student-affairs/dean/ The Office of the Dean of Students provides central leadership of the meta curricular experience at Carnegie Mellon including the coordination of student support. Graduate students will find the enrollment information for Domestic Partner Registration and Parental Accommodations in the Office of the Dean of Students or on their website. This Office also manages the Student Emergency Support Funding process. There are currently three forms of support funding for enrolled students: emergency student loans, student parental loans, and the Tartan Emergency Support Fund. Inquiring students will be provided with additional information about the various types of funding during a consultation meeting with a member of the Dean of Students team. Tuition costs are not eligible for Student Emergency Support funding. College Liaisons and the Student Support Resources team serve as additional resources for graduate students. College Liaisons are senior members of the Division of Student Affairs who work with departments and colleges addressing student concerns across a wide range of issues. College Liaisons are identified on the Important Contacts list in Student Information Online (SIO). The Student Support Resources team offers an additional level of support for students who are navigating a wide range of life events. Student Support Resources staff members work in partnership with campus and community resources to provide coordination of care and support appropriate to each student\\u2019s situation. 10 The Division of Student Affairs The Division of Student Affairs includes (not an exhaustive list): \\uf0b7 Athletics, Physical Education and Recreation \\uf0b7 Career and Professional Development Center (CPDC) \\uf0b7 Center for Student Diversity and Inclusion \\uf0b7 Cohon University Center \\uf0b7 Counseling & Psychological Services (CaPS) \\uf0b7 Dining Services \\uf0b7 Office of Community Standards and Integrity (OCSI) \\uf0b7 Office of Student Leadership, Involvement, and Civic Engagement (SLICE) \\uf0b7 University Health Services (UHS) \\uf0b7 Wellness Initiatives 11 Center for Student Diversity & Inclusion https://www.cmu.edu/student-diversity/ Diversity and inclusion have a singular place among the values of Carnegie Mellon University. The Center for Student Diversity & Inclusion actively cultivates a strong, diverse and inclusive community capable of living out these values and advancing research, creativity, learning and development that changes the world. The Center offers resources to enhance an inclusive and transformative student experience in dimensions such as access, success, campus climate and intergroup dialogue. Additionally, the Center supports and connects historically underrepresented students and those who are first in their family to attend college in a setting where students\\u2019 differences and talents are appreciated and reinforced, both at the graduate and undergraduate level. Initiatives coordinated by the Center include, but are not limited to: \\uf0b7 First generation/first in family to attend college programs \\uf0b7 LGBTQ+ Initiatives \\uf0b7 Race and ethnically focused programs, including Inter-University Graduate Students of Color Series (SOC) and PhD SOC Network \\uf0b7 Women\\u2019s empowerment programs, including Graduate Women\\u2019s Gatherings (GWGs) 12 Assistance for Individuals with Disabilities https://www.cmu.edu/disability-resources/ The Office of Disability Resources at Carnegie Mellon University has a continued mission to provide physical, digital, and programmatic access to ensure that students with disabilities have equal access to their educational experience. The Office works to ensure that qualified individuals receive reasonable accommodations as guaranteed by the Americans with Disabilities Act (ADA) and Section 504 of the Rehabilitation Act of 1973. Students who would like to receive accommodations can begin the process through Disability Resources' secure online portal or email access@andrew.cmu.edu to begin the interactive accommodation Process. Students with physical, sensory, cognitive, or emotional disabilities are encouraged to self-identify with the Office of Disability Resources and request needed accommodations. Any questions about the process can be directed to access@andrew.cmu.edu, or call (412) 268- 6121. 13 Eberly Center for Teaching Excellence & Educational Innovation https://www.cmu.edu/teaching/ The Eberly Center offers a wide variety of confidential, consultation services and professional development programs to support graduate students as teaching assistants or instructors of record during their time at Carnegie Mellon University and as future faculty members at other institutions. Regardless of one's current or future teaching context and duties, Eberly\\u2019s goal is to disseminate evidence-based teaching strategies in ways that are accessible and actionable. Programs and services include campus-wide Graduate Student Instructor Orientation events and our Future Faculty Program, both of which are designed to help participants be effective and efficient in their teaching roles. The Eberly Center also assists departments in creating and conducting customized programs to meet the specific needs of their graduate student instructors. Specific information about Eberly Center support for graduate students is found at: https://www.cmu.edu/teaching/graduatestudentsupport/ 14 Graduate Student Assembly https://www.cmu.edu/stugov/gsa/ The Graduate Student Assembly (GSA) is the branch of Carnegie Mellon Student Government that represents and advocates for the diverse interests of all graduate students at CMU. GSA is composed of representatives from the different graduate programs and departments who want to improve the graduate student experience at the different levels of the university. GSA is funded by the Student Activities Fee from all graduate students. GSA passes legislation, allocates student activities funding, advocates for legislative action locally and in Washington D.C. on behalf of graduate student issues and needs, and otherwise acts on behalf of all graduate student interests. GSA\\u2019s recent accomplishments are a testament to their making a difference, and steps to implementing the vision laid out by the strategic plan. https://www.cmu.edu/stugov/gsa/about-the-gsa/strategic-plan.html GSA offers an expanding suite of social programming on and off-campus to bring graduate students from different departments together and build a sense of community. GSA is the host of the Graduate Student Lounge on the 3rd floor of the Cohon University Center. GSA also maintains a website of graduate student resources on and off-campus. GSA continues to rely on student feedback to improve the graduate student experience at CMU. Feel free to contact them at gsa@cmu.edu to get involved, stop by their office in the Cohon University Center Room 304 or become a representative for your department. 15 Office of International Education (OIE) https://www.cmu.edu/oie/ Carnegie Mellon hosts international graduate and undergraduate students who come from more than 90 countries. The Office of International Education (OIE) is the liaison to the University for all non-immigrant students and scholars, as well the repository for study abroad opportunities. OIE provides many services including: advising on personal, immigration, study abroad, academic, and social and acculturation issues; presenting programs of interest such as international career workshops, tax workshops, and cross-cultural and immigration workshops; international education and statistics on international students in the United States; posting pertinent information to students through email and the OIE website and conducting orientation and pre- departure programs. 16 Veterans and Military Community https://www.cmu.edu/veterans/ Military veterans are a vital part of the Carnegie Mellon University community. Graduate students can find information on applying for veteran education benefits, campus services, veteran\\u2019s groups at CMU, and non-educational resources through the Veterans and Military Community website. There are also links and connections to veteran resource in the Pittsburgh community. The ROTC and Veteran Affairs Coordinator can be reached at urovaedbenefits@andrew.cmu.edu or 412-268-8747. 17 Carnegie Mellon Ethics Hotline https://www.cmu.edu/hr/resources/ethics-hotline.html The health, safety and well-being of the university community are top priorities at Carnegie Mellon University. CMU provides a hotline that all members of the university community should use to confidentially report suspected unethical activity, violations of university policy, or violations of law. Students, faculty and staff can anonymously file a report by calling 1-844- 587-0793 or visiting https://cmu.ethicspoint.com/. All submissions are reported to appropriate university personnel and handled discreetly. The hotline is NOT an emergency service. For emergencies, call University Police at 412-268-2323. 18 Policy Against Retaliation It is the policy of Carnegie Mellon University to protect from retaliation any individual who makes a good faith report of a suspected violation of any applicable law or regulation, university Policy or procedure, any contractual obligation of the university, and any report made pursuant to the Carnegie Mellon University Code of Business Ethics and Conduct. Additional details regarding the Policy Against Retaliation are available at: https://www.cmu.edu/policies/administrative-and-governance/whistleblower.html 19 Key Offices for Academic & Research Support 20 Computing and Information Resources https://www.cmu.edu/computing/ Computing Services maintains and supports computing resources for the campus community, including the campus wired and wireless networks, printing, computer labs, file storage, email and software catalog. As members of this community, we are all responsible for the security of these shared resources. Be sure to review the Safe Computing (https://www.cmu.edu/computing/safe/) section and the University Computing Policy (https://www.cmu.edu/policies/information-technology/computing.html) Visit the Computing Services website (https://www.cmu.edu/computing/) to learn more. For assistance the Computing Services Help Center is available at 412-268-4357 (HELP) or ithelp@cmu.edu. 21 Student Academic Success Center https://www.cmu.edu/student-success/ The Student Academic Success Center\\u2019s (SASC) work to support success focuses on creating spaces for students to engage in their coursework and approach to learning through many group and individual program options. SASC supports student success by providing academic coaching, subject-specific tutoring, effective communication strategies, accommodations for students with disabilities, and language support for multilingual learners. SASC engages with faculty and staff to improve the coordination and professional development for academic advisors. Visit the SASC website for more information about services offered in areas such as communication and language support; language and cross- cultural support; and learning support. 22 University Libraries https://www.library.cmu.edu/ The University Libraries offers a wide range of information, resources, and services supporting graduate students in coursework, research, teaching, and publishing. The library licenses and purchases books, journals, media, and other needed materials in various formats. Library liaisons, consultants, and information specialists provide in-depth and professional assistance and advice in all-things information, including: \\uf0b7 Locating and obtaining specific resources \\uf0b7 Providing specialized research support \\uf0b7 Advanced training in the use and management of data Sign up for workshops and hands-on topic-specific sessions such as data visualization with Tableau, cleaning data with OpenRefine, and getting started with Zotero. Weekly drop-in hours for Digital Humanities and for Research Data Research Management are scheduled during the academic year. Start at the library home page to find the books, journals, and databases you need; to identify and reach out to the library liaison in your field; to sign up for scheduled workshops; and to connect with consultants in scholarly publishing, research data management, and digital humanities. 23 Research at CMU https://www.cmu.edu/research/ The primary purpose of research at the university is the advancement of knowledge in all fields in which the university is active. Research is regarded as one of the university\\u2019s major contributions to society and as an essential element in education, particularly at the graduate level and in faculty development. Research activities are governed by several university policies. Guidance and more general information are found by visiting the Research at Carnegie Mellon website. 24 Office of Research Integrity & Compliance https://www.cmu.edu/research-compliance/ The Office of Research Integrity & Compliance (ORIC) is designed to support research at Carnegie Mellon University. The staff work with researchers to ensure research is conducted with integrity and in accordance with federal and Pennsylvania regulation. ORIC assists researchers with human subject research, conflicts of interest, responsible conduct of research, export controls, and institutional animal care & use. ORIC also provides consultation, advice, and review of allegations of research misconduct. 25 Key Offices for Health, Wellness & Safety 26 Counseling & Psychological Services https://www.cmu.edu/counseling/ Counseling & Psychological Services (CaPS) affords the opportunity for students to talk privately about academic and personal concerns in a safe, confidential setting. An initial consultation at CaPS can help clarify the nature of the concern, provide immediate support, and explore further options if needed. These may include a referral for counseling within CaPS, to another resource at Carnegie Mellon, or to another resource within the larger Pittsburgh community. CaPS also provides workshops and group sessions on mental health related topics specifically for graduate students on campus. CaPS services are provided at no cost. Appointments can be made in person, or by telephone at 412-268-2922. 27 Health Services https://www.cmu.edu/HealthServices/ University Health Services (UHS) is staffed by physicians, advanced practice clinicians and registered nurses who provide general medical care, allergy injections, first aid, gynecological care, and contraception as well as on-site pharmaceuticals. The CMU Student Insurance Plan covers most visit fees to see the physicians and advanced practice clinicians & nurse visits. Fees for prescription medications, laboratory tests, diagnostic procedures and referral to the emergency room or specialists are the student\\u2019s responsibility and students should review the UHS website and their insurance plan for detailed information about the university health insurance requirement and fees. UHS also has a registered dietician and health promotion specialists on staff to assist students in addressing nutrition, drug and alcohol and other healthy lifestyle issues. In addition to providing direct health care, UHS administers the Student Health Insurance Program. The Student Health Insurance plan offers a high level of coverage in a wide network of health care providers and hospitals. Appointments can be made by visiting UHS\\u2019s website, walk-in, or by telephone, 412-268-2157. 28 Campus Wellness https://www.cmu.edu/wellness/ At Carnegie Mellon, we believe our individual and collective well-being is rooted in healthy connections to each other and to campus resources. The university provides a wide variety of wellness, mindfulness and connectedness initiatives and resources designed to help students thrive inside and outside the classroom. 29 Religious and Spiritual Life Initiatives (RSLI) https://www.cmu.edu/wellbeing/resources/religious-spiritual/index.html Carnegie Mellon is committed to the holistic growth of our students, including creating opportunities for spiritual and religious practice and exploration. RSLI has relationships with local houses of worship from various traditions and many of these groups are members of CMU\\u2019s Council of Religious Advisors. They also offer programs and initiatives that cross traditional religious boundaries in order to increase knowledge of and appreciation for the full diversity of the worldview traditions. RSLI staff are available to support students across the spectrum of religious and spiritual practice and would be more than happy to help you make a connection into a community of faith during your time at CMU. 30 University Police https://www.cmu.edu/police/ x2323 The University Police Department is located at 4551 Filmore Street . The department\\u2019s services include police patrols and call response, criminal investigations, fixed officer and foot officer patrols, event security, and crime prevention and education programming as well as bicycle and laptop registration. Visit the department\\u2019s website for additional information about the staff, emergency phone locations, crime prevention, lost and found, fingerprint services, and annual statistic reports. Carnegie Mellon University publishes an annual campus security and fire safety report describing the university\\u2019s security, alcohol and drug, sexual assault, and fire safety policies. The report also contains statistics about the number and type of crimes committed on the campus and the number and cause of fires in campus residence facilities during the preceding three years. Graduate students can obtain a copy by contacting the University Police Department at x2323. The annual security and fire safety report is also available online at: https://www.cmu.edu/police/annualreports/ 31 Shuttle and Escort Services https://www.cmu.edu/parking/transport/ Parking and Transportation coordinates the Shuttle Service and Escort Service provided for CMU students, faculty, and community. The Shuttle & Escort website has full information about these services, stops, routes, tracking and schedules. 32 The WORD https://www.cmu.edu/student-affairs/theword/ The WORD is Carnegie Mellon University\\u2019s online student handbook and serves as the foundation for the department (and sometimes college) handbook. The WORD contains university-wide academic policy information and resources, community policies and resources, and describes the university level procedures used to review possible violations of these standards. It is designed to provide all students with the tools, guidance, and insights to help you achieve your full potential as a member of the Carnegie Mellon community. Graduate students are encouraged to bookmark this site and refer to it often. University policies can also be found in full text at: https://www.cmu.edu/policies/. \",\n          \"Author (LTI's Professor): Yonantan Bisk; Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs; Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang; Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'} Author (LTI's Professor): Yonantan Bisk; Title: HomeRobot: Open-Vocabulary Mobile Manipulation; Authors: Sriram Yenamandra, A. Ramachandran, Karmesh Yadav, Austin S. Wang, Mukul Khanna, Th\\u00e9ophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander Clegg, John Turner, Z. Kira, M. Savva, Angel X. Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton; Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.; Year: 2023; Venue: Conference on Robot Learning; Citations: 19; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The HomeRobot OVMM benchmark is introduced, where an agent navigates household environments to grasp novel objects and place them on target receptacles, and baselines achieve a 20% success rate in the real world; the experiments identify ways future research work improve performance.'} Author (LTI's Professor): Yonantan Bisk; Title: EXCALIBUR: Encouraging and Evaluating Embodied Exploration; Authors: Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Weihs; Abstract: Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: \\u201cis the small heavy red bowl made from glass?\\u201d or \\u201cis there a silver spoon heavier than the egg?\\u201d. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures and represents the next frontier for embodied AI research.'} Author (LTI's Professor): Yonantan Bisk; Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents; Authors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye; Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.; Year: 2023; Venue: arXiv.org; Citations: 16; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A framework to use the knowledge in LLMs to simplify the control problem, rather than solving it is proposed, which leads to a significant 15% improvement over SOTA for generalization to human goal specifications.'} Author (LTI's Professor): Yonantan Bisk; Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models; Authors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\\u00e3o Silv\\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, L. Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui; Abstract: \\u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Yonantan Bisk; Title: Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis; Authors: Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Varma Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao, Yu Quan Chong, Chen Wang, Katia P. Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Z. Kira, Fei Xia, Yonatan Bisk; Abstract: Building general-purpose robots that can operate seamlessly, in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. Unfortunately, however, most existing robotic systems have been constrained - having been designed for specific tasks, trained on specific datasets, and deployed within specific environments. These systems usually require extensively-labeled data, rely on task-specific models, have numerous generalization issues when deployed in real-world scenarios, and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing an overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository of resources, including papers reviewed in this survey as well as related projects and repositories for developing foundation models for robotics.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable is provided and a taxonomy is established to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics.'} Author (LTI's Professor): Yonantan Bisk; Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models; Authors: Open X-Embodiment Collaboration, A. Padalkar, Acorn Pooley, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Animesh Garg, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, B. Sch\\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Joao Silv'erio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, L. Chen, Lerrel Pinto, Fei-Fei Li, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Max Spero, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart'in-Mart'in, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui; Abstract: Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website $\\\\href{https://robotics-transformer-x.github.io}{\\\\text{robotics-transformer-x.github.io}}$.; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper provides datasets in standardized data formats and models to make it possible to explore the possibility of generalist X-robot policy in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies.'} Author (LTI's Professor): Yonantan Bisk; Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception; Authors: Gyan Tatiya, Jonathan M Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov; Abstract: A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multimodal Object property learning with Self-Attention and Interactive Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from multimodal foundation models and aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems.'} Author (LTI's Professor): Yonantan Bisk; Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation; Authors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk; Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A new task OUTDOOR is introduced, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain are introduced.'} Author (LTI's Professor): Yonantan Bisk; Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models; Authors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\\u00e3o Silv\\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, L. Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui; Abstract: \\u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Yonantan Bisk; Title: SLAP: Spatial-Language Attention Policies; Authors: Priyam Parashar, Vidhi Jain, Xiaohan Zhang, Jay Vakil, Sam Powers, Yonatan Bisk, Chris Paxton; Abstract: Despite great strides in language-guided manipulation, existing work has been constrained to table-top settings. Table-tops allow for perfect and consistent camera angles, properties are that do not hold in mobile manipulation. Task plans that involve moving around the environment must be robust to egocentric views and changes in the plane and angle of grasp. A further challenge is ensuring this is all true while still being able to learn skills efficiently from limited data. We propose Spatial-Language Attention Policies (SLAP) as a solution. SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows an 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations). We see a 4x improvement over baseline in mobile manipulation setting. In addition, we show how SLAPs robustness allows us to execute Task Plans from open-vocabulary instructions using a large language model for multi-step mobile manipulation. For videos, see the website: https://robotslap.github.io; Year: 2023; Venue: Conference on Robot Learning; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The proposed Spatial-Language Attention Policies (SLAP) uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy, which shows an 80% success rate in the real world across eight tasks with a single model, and a 4x improvement over baseline in mobile manipulation setting.'} Author (LTI's Professor): Yonantan Bisk; Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment; Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell; Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\\\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'} Author (LTI's Professor): Yonantan Bisk; Title: SPRING: Studying the Paper and Reasoning to Play Games; Authors: Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Tom M. Mitchell, Yuan-Fang Li; Abstract: Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context\\\"reasoning\\\"induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.; Year: 2023; Venue: ; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories, and show the potential of games as a test bed for LLMs.'} Author (LTI's Professor): Yonantan Bisk; Title: WebArena: A Realistic Web Environment for Building Autonomous Agents; Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig; Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.; Year: 2023; Venue: arXiv.org; Citations: 73; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'} Author (LTI's Professor): Yonantan Bisk; Title: Computational Language Acquisition with Theory of Mind; Authors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig; Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.; Year: 2023; Venue: International Conference on Learning Representations; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"It is found that training speakers with a highly weighted ToM listener component leads to performance gains in the authors' image referential game setting, and suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.\\\"} Author (LTI's Professor): Yonantan Bisk; Title: SPRING: Studying Papers and Reasoning to play Games; Authors: Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Russ Salakhutdinov, A. Azaria, Tom M. Mitchell, Yuanzhi Li; Abstract: None; Year: 2023; Venue: Neural Information Processing Systems; Citations: 2; TLDR: None Author (LTI's Professor): Yonantan Bisk; Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models; Authors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\\u00e3o Silv\\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, K. Majd, Krishan Rana, K. Srinivasan, L. Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, T. Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui; Abstract: \\u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models,; Year: 2023; Venue: arXiv.org; Citations: 49; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A consolidation of pretrained models in domains from NLP to Computer Vision, where large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications.'} Author (LTI's Professor): Yonantan Bisk; Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents; Authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap; Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.; Year: 2023; Venue: arXiv.org; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': 'SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence, is presented and it is found that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.'} Author (LTI's Professor): Jamie Callan; Title: Conversational Search with Random Walks over Entity Graphs; Authors: Gustavo Gon\\u00e7alves, Jo\\u00e3o Magalh\\u00e3es, Jamie Callan; Abstract: The entities that emerge during a conversation can be used to model topics, but not all entities are equally useful for this task. Modeling the conversation with entity graphs and predicting each entity's centrality in the conversation provides additional information that improves the retrieval of answer passages for the current question. Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.; Year: 2023; Venue: International Conference on the Theory of Information Retrieval; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.'} Author (LTI's Professor): Jamie Callan; Title: KALE: Using a K-Sparse Projector for Lexical Expansion; Authors: Lu\\u00eds Borges, Bruno Martins, Jamie Callan; Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.; Year: 2023; Venue: International Conference on the Theory of Information Retrieval; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'KALE is a new lightweight method that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary, which can replace the original lexical vocabulary with gains in accuracy and efficiency.'} Author (LTI's Professor): Jamie Callan; Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms; Authors: Zhen Fan, Luyu Gao, Jamie Callan; Abstract: Lexical exact-match systems perform text retrieval efficiently with sparse matching signals and fast retrieval through inverted lists, but naturally suffer from the mismatch between lexical surface form and implicit term semantics. This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF). Each CSF pairs a lexical surface form with a context source, and is represented by a lexical form weight and a contextualized semantic vector representation. This framework is able to perform sparse lexicon-based retrieval by learning to represent each query and document as a \\\"bag-of-CSFs\\\", simultaneously addressing two key factors in sparse retrieval: vocabulary expansion of surface form and semantic representation of term meaning. At retrieval time, it efficiently matches CSFs through exact-match of learned surface forms, and effectively scores each CSF pair via contextual semantic representations, leading to joint improvement in both term match and term scoring. Multiple experiments show that this approach successfully resolves the main mismatch issues in lexical exact-match retrieval and outperforms state-of-the-art lexical exact-match systems, reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact-match-based system.; Year: 2023; Venue: International Conference on the Theory of Information Retrieval; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF), reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact- match-based system.'} Author (LTI's Professor): Jamie Callan; Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval; Authors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan; Abstract: None; Year: 2023; Venue: European Conference on Information Retrieval; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Jamie Callan; Title: Active Retrieval Augmented Generation; Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig; Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 50; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'} Author (LTI's Professor): Jamie Callan; Title: Multi-Objective Improvement of Android Applications; Authors: Jamie Callan, J. Petke; Abstract: Non-functional properties, such as runtime or memory use, are important to mobile app users and developers, as they affect user experience. Previous work on automated improvement of non-functional properties in mobile apps failed to address the inherent trade-offs between such properties. We propose a practical approach and the first open-source tool, GIDroid (2023), for multi-objective automated improvement of Android apps. In particular, we use Genetic improvement, a search-based technique that navigates the space of software variants to find improved software. We use a simulation-based testing framework to greatly improve the speed of search. GIDroid contains three state-of-the-art multi-objective algorithms, and two new mutation operators, which cache the results of method calls. Genetic improvement relies on testing to validate patches. Previous work showed that tests in open-source Android applications are scarce. We thus wrote tests for 21 versions of 7 Android apps, creating a new benchmark for performance improvements. We used GIDroid to improve versions of mobile apps where developers had previously found improvements to runtime, memory, and bandwidth use. Our technique automatically re-discovers 64% of existing improvements. We then applied our approach to current versions of software in which there were no known improvements. We were able to improve execution time by up to 35%, and memory use by up to 33% in these apps.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a practical approach and the first open-source tool, GIDroid, for multi-objective automated improvement of Android apps, and uses Genetic improvement, a search-based technique that navigates the space of software variants to find improved software.'} Author (LTI's Professor): Justine Cassell; Title: When to generate hedges in peer-tutoring interactions; Authors: Alafate Abulimiti, C. Clavel, Justine Cassell; Abstract: This paper explores the application of machine learning techniques to predict where hedging occurs in peer-tutoring interactions. The study uses a naturalistic face-to-face dataset annotated for natural language turns, conversational strategies, tutoring strategies, and nonverbal behaviors. These elements are processed into a vector representation of the previous turns, which serves as input to several machine learning models, including MLP and LSTM. The results show that embedding layers, capturing the semantic information of the previous turns, significantly improves the model\\u2019s performance. Additionally, the study provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviors, in predicting hedges by using Shapley values for feature explanation. We discover that the eye gaze of both the tutor and the tutee has a significant impact on hedge prediction. We further validate this observation through a follow-up ablation study.; Year: 2023; Venue: SIGDIAL Conferences; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The results show that embedding layers, capturing the semantic information of the previous turns, significantly improves the model\\u2019s performance and provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviors, in predicting hedges by using Shapley values for feature explanation.'} Author (LTI's Professor): Justine Cassell; Title: How About Kind of Generating Hedges using End-to-End Neural Models?; Authors: Alafate Abulimiti, C. Clavel, Justine Cassell; Abstract: Hedging is a strategy for softening the impact of a statement in conversation. In reducing the strength of an expression, it may help to avoid embarrassment (more technically, \\u201cface threat\\u201d) to one\\u2019s listener. For this reason, it is often found in contexts of instruction, such as tutoring. In this work, we develop a model of hedge generation based on i) fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by ii) reranking to select the candidate that best matches the expected hedging strategy within a candidate pool using a hedge classifier. We apply this method to a natural peer-tutoring corpus containing a significant number of disfluencies, repetitions, and repairs. The results show that generation in this noisy environment is feasible with reranking. By conducting an error analysis for both approaches, we reveal the challenges faced by systems attempting to accomplish both social and task-oriented goals in conversation.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work develops a model of hedge generation based on fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by reranking to select the candidate that best matches the expected hedging strategy within a candidate pool using a hedge classifier.'} Author (LTI's Professor): Justine Cassell; Title: Beyond Single-Mindedness: A Figure-Ground Reversal for the Cognitive Sciences; Authors: Mark Dingemanse, Andreas Liesenfeld, Marlou Rasenberg, Saul Albert, F. Ameka, Abeba Birhane, Dimitris Bolis, Justine Cassell, Rebecca Clift, E. Cuffari, H. Jaegher, C. Novaes, N. Enfield, Riccardo Fusaroli, E. Gregoromichelaki, E. Hutchins, Ivana Konvalinka, D. Milton, J. R\\u0105czaszek-Leonardi, V. Reddy, F. Rossano, David Schlangen, J. Seibt, E. Stokoe, L. Suchman, C. Vesper, T. Wheatley, Martina Wiltschko; Abstract: A fundamental fact about human minds is that they are never truly alone: all minds are steeped in situated interaction. That social interaction matters is recognized by any experimentalist who seeks to exclude its influence by studying individuals in isolation. On this view, interaction complicates cognition. Here, we explore the more radical stance that interaction co-constitutes cognition: that we benefit from looking beyond single minds toward cognition as a process involving interacting minds. All around the cognitive sciences, there are approaches that put interaction center stage. Their diverse and pluralistic origins may obscure the fact that collectively, they harbor insights and methods that can respecify foundational assumptions and fuel novel interdisciplinary work. What might the cognitive sciences gain from stronger interactional foundations? This represents, we believe, one of the key questions for the future. Writing as a transdisciplinary collective assembled from across the classic cognitive science hexagon and beyond, we highlight the opportunity for a figure-ground reversal that puts interaction at the heart of cognition. The interactive stance is a way of seeing that deserves to be a key part of the conceptual toolkit of cognitive scientists.; Year: 2023; Venue: Cognitive Sciences; Citations: 18; TLDR: None Author (LTI's Professor): Justine Cassell; Title: \\\"You might think about slightly revising the title\\u201d: Identifying Hedges in Peer-tutoring Interactions; Authors: Yann Raphalen, C. Clavel, Justine Cassell; Abstract: Hedges have an important role in the management of rapport. In peer-tutoring, they are notably used by tutors in dyads experiencing low rapport to tone down the impact of instructions and negative feedback.Pursuing the objective of building a tutoring agent that manages rapport with teenagers in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of a such a hybrid model approach.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A model explainability tool is employed to explore the features that characterize hedges in peer-tutoring conversations, and some novel features, and the benefits of a such a hybrid model approach are identified.'} Author (LTI's Professor): Mona Diab; Title: Author Correction: Arabic natural language processing for Qur\\u2019anic research: a systematic review; Authors: M. Bashir, Aqil M. Azmi, H. Nawaz, W. Zaghouani, Mona T. Diab, Ala I. Al-Fuqaha, Junaid Qadir; Abstract: None; Year: 2023; Venue: Artificial Intelligence Review; Citations: 0; TLDR: None Author (LTI's Professor): Mona Diab; Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology; Authors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues; Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.; Year: 2023; Venue: International Workshop on Spoken Language Translation; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.'} Author (LTI's Professor): Fernando Diaz; Title: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision; Authors: Fernando Diaz; Abstract: Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work addresses the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements.'} Author (LTI's Professor): Fernando Diaz; Title: Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery; Authors: Rebecca Salganik, Fernando Diaz, G. Farnadi; Abstract: As online music platforms grow, music recommender systems play a vital role in helping users navigate and discover content within their vast musical databases. At odds with this larger goal, is the presence of popularity bias, which causes algorithmic systems to favor mainstream content over, potentially more relevant, but niche items. In this work we explore the intrinsic relationship between music discovery and popularity bias. To mitigate this issue we propose a domain-aware, individual fairness-based approach which addresses popularity bias in graph neural network (GNNs) based recommender systems. Our approach uses individual fairness to reflect a ground truth listening experience, i.e., if two songs sound similar, this similarity should be reflected in their representations. In doing so, we facilitate meaningful music discovery that is robust to popularity bias and grounded in the music domain. We apply our BOOST methodology to two discovery based tasks, performing recommendations at both the playlist level and user level. Then, we ground our evaluation in the cold start setting, showing that our approach outperforms existing fairness benchmarks in both performance and recommendation of lesser-known content. Finally, our analysis explains why our proposed methodology is a novel and promising approach to mitigating popularity bias and improving the discovery of new and niche content in music recommender systems.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a domain-aware, individual fairness-based approach which addresses popularity bias in graph neural network (GNNs) based recommender systems and applies the BOOST methodology to two discovery based tasks, performing recommendations at both the playlist level and user level.'} Author (LTI's Professor): Fernando Diaz; Title: Overview of the TREC 2021 Fair Ranking Track; Authors: Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, Sebastian Kohlmeier; Abstract: The TREC Fair Ranking Track aims to provide a platform for participants to develop and evaluate novel retrieval algorithms that can provide a fair exposure to a mixture of demographics or attributes, such as ethnicity, that are represented by relevant documents in response to a search query. For example, particular demographics or attributes can be represented by the documents' topical content or authors. The 2021 Fair Ranking Track adopted a resource allocation task. The task focused on supporting Wikipedia editors who are looking to improve the encyclopedia's coverage of topics under the purview of a WikiProject. WikiProject coordinators and/or Wikipedia editors search for Wikipedia documents that are in need of editing to improve the quality of the article. The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia. The under-representation of particular protected characteristics in Wikipedia can result in systematic biases that can have a negative human, social, and economic impact, particularly for disadvantaged or protected societal groups.; Year: 2023; Venue: Text Retrieval Conference; Citations: 23; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia.'} Author (LTI's Professor): Fernando Diaz; Title: Recall, Robustness, and Lexicographic Evaluation; Authors: Fernando Diaz, Bhaskar Mitra; Abstract: Although originally developed to evaluate sets of items, recall is often used to evaluate rankings of items, including those produced by recommender, retrieval, and other machine learning systems. The application of recall without a formal evaluative motivation has led to criticism of recall as a vague or inappropriate measure. In light of this debate, we reflect on the measurement of recall in rankings from a formal perspective. Our analysis is composed of three tenets: recall, robustness, and lexicographic evaluation. First, we formally define `recall-orientation' as the sensitivity of a metric to a user interested in finding every relevant item. Second, we analyze recall-orientation from the perspective of robustness with respect to possible content consumers and providers, connecting recall to recent conversations about fair ranking. Finally, we extend this conceptual and theoretical treatment of recall by developing a practical preference-based evaluation method based on lexicographic comparison. Through extensive empirical analysis across three recommendation tasks and 17 information retrieval tasks, we establish that our new evaluation method, lexirecall, has convergent validity (i.e., it is correlated with existing recall metrics) and exhibits substantially higher sensitivity in terms of discriminative power and stability in the presence of missing labels. Our conceptual, theoretical, and empirical analysis substantially deepens our understanding of recall and motivates its adoption through connections to robustness and fairness.; Year: 2023; Venue: ; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Through extensive empirical analysis across three recommendation tasks and 17 information retrieval tasks, it is established that the new evaluation method, lexirecall, has convergent validity and exhibits substantially higher sensitivity in terms of discriminative power and stability in the presence of missing labels.'} Author (LTI's Professor): Scott Fahlman; Title: Score: A Rule Engine for the Scone Knowledge Base System; Authors: Jeffrey Chen, S. Fahlman; Abstract: We present Score, a rule engine designed and implemented for the Scone knowledge base system. Scone is a knowledge base system designed for storing and manipulating rich representations of general knowledge in symbolic form. It represents knowledge in the form of nodes and links in a network structure, and it can perform basic inference about the relationships between different elements efficiently. On its own, Scone acts as a sort of\\\"smart memory\\\"that can interface with other software systems. One area of improvement for Scone is how useful it can be in supplying knowledge to an intelligent agent that can use the knowledge to perform actions and update the knowledge base with its observations. We augment the Scone system with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone's knowledge base, potentially improving the capabilities of any planning systems built on top of Scone. Production rule systems consist of\\\"if-then\\\"production rules that try to match their predicates to existing knowledge and fire their actions when their predicates are satisfied. We propose two kinds of production rules, if-added and if-needed rules, that differ in how they are checked and fired to cover multiple use cases. We then implement methods to efficiently check and fire these rules in a large knowledge base. The new rule engine is not meant to be a complex stand-alone planner, so we discuss how it fits into the context of Scone and future work on planning systems.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"The Scone system is augmented with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone's knowledge base, potentially improving the capabilities of any planning systems built on top of Scone.\\\"} Author (LTI's Professor): Daniel Fried; Title: Pragmatic Inference with a CLIP Listener for Contrastive Captioning; Authors: Jiefu Ou, Benno Krojer, Daniel Fried; Abstract: We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off-the-shelf CLIP model to parameterize the listener. Compared with captioner-only pragmatic models, our method benefits from rich vision language alignment representations from CLIP when reasoning over distractors. Like previous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to discriminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which allows us to automatically optimize the captions for informativity - outperforming past methods for discriminative captioning by 11% to 15% accuracy in human evaluations; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images by leveraging an off-the-shelf CLIP model to parameterize the listener.'} Author (LTI's Professor): Daniel Fried; Title: SantaCoder: don't reach for the stars!; Authors: Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Mu\\u00f1oz Ferrandis, Niklas Muennighoff, Mayank Mishra, A. Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, J. Poirier, Hailey Schoelkopf, S. Troshin, Dmitry Abulkhanov, M. Romero, M. Lappert, F. Toni, Bernardo Garc'ia del R'io, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, I. Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, D. Lansky, Huu Nguyen, Danish Contractor, Luisa Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, S. Hughes, Daniel Fried, Arjun Guha, H. D. Vries, Leandro von Werra; Abstract: The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.; Year: 2023; Venue: arXiv.org; Citations: 85; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The current state of the Personally Identifiable Information (PII) redaction pipeline is outlined, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data are outlined.'} Author (LTI's Professor): Daniel Fried; Title: Grounding Language Models to Images for Multimodal Generation; Authors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried; Abstract: We propose an ef\\ufb01cient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and \\ufb01netune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.; Year: 2023; Venue: arXiv.org; Citations: 61; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Daniel Fried; Title: StarCoder: may the source be with you!; Authors: Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, J. Lamy-Poirier, Jo\\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, J. Stillerman, S. Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, N. Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, M. Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, A. Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\\u00f1oz Ferrandis, Sean M. Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, H. D. Vries; Abstract: The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.; Year: 2023; Venue: arXiv.org; Citations: 231; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work performs the most comprehensive evaluation of Code LLMs to date and shows that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model.'} Author (LTI's Professor): Daniel Fried; Title: Grounding Language Models to Images for Multimodal Inputs and Outputs; Authors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried; Abstract: We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.; Year: 2023; Venue: International Conference on Machine Learning; Citations: 33; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Daniel Fried; Title: Generating Images with Multimodal Language Models; Authors: Jing Yu Koh, Daniel Fried, R. Salakhutdinov; Abstract: We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 69; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces, and exhibits a wider range of capabilities compared to prior multimodal language models.'} Author (LTI's Professor): Daniel Fried; Title: WebArena: A Realistic Web Environment for Building Autonomous Agents; Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig; Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.; Year: 2023; Venue: arXiv.org; Citations: 73; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'} Author (LTI's Professor): Alexander Hauptmann; Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data; Authors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann; Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.; Year: 2023; Venue: Defense + Commercial Sensing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition and shows that this model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.'} Author (LTI's Professor): Alexander Hauptmann; Title: Towards Open-Domain Twitter User Profile Inference; Authors: Haoyang Wen, Zhenxin Xiao, E. Hovy, Alexander Hauptmann; Abstract: ,; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Daphne Ippolito; Title: The State of Intent Detection in the Era of Large Autoregressive Language Models; Authors: Tom B. Brown, Benjamin Mann, Nick Ryder, Jared D Subbiah, Prafulla Kaplan, A. Dhariwal, P. Neelakantan, Girish Shyam, Amanda Sastry, Sandhini Askell, Ariel Agarwal, Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott teusz Litwin, Benjamin Gray, Jack Chess, Christopher Clark, Sam Berner, Alec McCandlish, Ilya Radford, Sutskever Dario, Amodei, Matthew Henderson, Ivan Vulic. 2020, Ef-310, Aakanksha Chowdhery, Sharan Narang, J. Devlin, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek, Parker Rao, Yi Barnes, Noam Tay, Vin-316 Shazeer, Emily odkumar Prabhakaran, Nan Reif, Ben Du, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, H. Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, L. Fedus, Denny Zhou, Daphne Ippolito, D. Luan, Hyeontaek Lim, Barret Zoph, A. Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Oleksandr Polozov, K. Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta; Abstract: In-context learning (ICL) using large pre-001 trained autoregressive language models (LLMs, 002 e.g. GPT-3) has demonstrated effective clas-003 sification performance at a variety of natural 004 language tasks. Using LLMs for intent detec-005 tion is challenging due to the large label space 006 and limited context window, such that it is diffi-007 cult to fit a sufficient number of examples in the 008 prompt to allow the use of in-context learning. 009 In this paper, dense retrieval is used to bypass 010 this limitation, giving the model only a par-011 tial view of the full label space. We show that 012 retriever-augmented large language models are 013 an effective way to tackle intent detection, by-014 passing context window limitations effectively 015 through the retrieval mechanism. Comparing 016 the LLaMA and OPT model families at differ-017 ent scales, we set new state of the art perfor-018 mance in the few-shot setting with zero training 019 for two of the three intent classification datasets 020 that we consider, while achieving competitive 021 results on the third one. This work demon-022 strates that the Retriever+ICL framework is a 023 strong zero-training competitor to fine-tuned in-024 tent detection approaches. In addition, a small 025 study on the number of examples provided at 026 different model scales is done, showing that 027 larger models are needed to make effective use 028 of more examples in-prompt. 029; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The Retriever+ICL framework is a strong zero-training competitor to fine-tuned in-024 tent detection approaches, and is set new state of the art in the few-shot setting with zero training.'} Author (LTI's Professor): Daphne Ippolito; Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System; Authors: Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, Yun William Yu; Abstract: Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model\\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).; Year: 2023; Venue: International Conference on Natural Language Generation; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling) are presented, which has implications for detecting generated text.'} Author (LTI's Professor): Daphne Ippolito; Title: A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity; Authors: S. Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David M. Mimno, Daphne Ippolito; Abstract: Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.; Year: 2023; Venue: arXiv.org; Citations: 35; TLDR: {'model': 'tldr@v2.0.0', 'text': 'These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which are hoped to help support more informed data-centric decisions in LM development.'} Author (LTI's Professor): Daphne Ippolito; Title: This paper is included in the Proceedings of the 32nd USENIX Security; Authors: \\u2217. NicholasCarlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, F. Tram\\u00e8r, Borja Balle, Daphne Ippolito, Eric Wallace, Google, DeepMind, Princeton, Uc Berkeley; Abstract: Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted signi\\ufb01cant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-\\ufb01lter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that diffusion models memorize individual images from their training data and emit them at generation time, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.'} Author (LTI's Professor): Daphne Ippolito; Title: Extracting Training Data from Diffusion Models; Authors: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tram\\u00e8r, B. Balle, Daphne Ippolito, Eric Wallace; Abstract: Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.; Year: 2023; Venue: USENIX Security Symposium; Citations: 231; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.'} Author (LTI's Professor): Daphne Ippolito; Title: Report of the 1st Workshop on Generative AI and Law; Authors: A. F. Cooper, Katherine Lee, James Grimmelmann, Daphne Ippolito, Christopher Callison-Burch, Christopher A. Choquette-Choo, Niloofar Mireshghallah, Miles Brundage, David Mimno, M. Choksi, J. Balkin, Nicholas Carlini, Christopher De Sa, Jonathan Frankle, Deep Ganguli, Bryant Gipson, A. Guadamuz, Swee Leng Harris, Abigail Z. Jacobs, Elizabeth Joh, Gautam Kamath, M. Lemley, Cass Matthews, C. McLeavey, Corynne Mcsherry, Milad Nasr, Paul Ohm, Adam Roberts, Tom Rubin, Pamela Samuelson, Ludwig Schubert, Kristen Vaccaro, Luis Villa, Felix Wu, Elana Zeide; Abstract: This report presents the takeaways of the inaugural Workshop on Generative AI and Law (GenLaw), held in July 2023. A cross-disciplinary group of practitioners and scholars from computer science and law convened to discuss the technical, doctrinal, and policy challenges presented by law for Generative AI, and by Generative AI for law, with an emphasis on U.S. law in particular. We begin the report with a high-level statement about why Generative AI is both immensely significant and immensely challenging for law. To meet these challenges, we conclude that there is an essential need for 1) a shared knowledge base that provides a common conceptual language for experts across disciplines; 2) clarification of the distinctive technical capabilities of generative-AI systems, as compared and contrasted to other computer and AI systems; 3) a logical taxonomy of the legal issues these systems raise; and, 4) a concrete research agenda to promote collaboration and knowledge-sharing on emerging issues at the intersection of Generative AI and law. In this report, we synthesize the key takeaways from the GenLaw workshop that begin to address these needs. All of the listed authors contributed to the workshop upon which this report is based, but they and their organizations do not necessarily endorse all of the specific claims in this report.; Year: 2023; Venue: Social Science Research Network; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'There is an essential need for a shared knowledge base that provides a common conceptual language for experts across disciplines to meet the challenges presented by law for Generative AI, and by GenerativeAI for law, with an emphasis on U.S. law in particular.'} Author (LTI's Professor): Daphne Ippolito; Title: What are Adapters Really Efficient At?; Authors: Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Mitesh M. Khapra, Pratyush Kumar, V. Rudra, Murthy Anoop, Kunchukuttan. 2022, Naama-677, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir R. Radev, Al-683 ham, Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Jonas Pfeiffer, Aishwarya Kamath, Andreas R\\u00fcckl\\u00e9, Kyunghyun Cho, Iryna Gurevych, Clifton Poth, Aishwarya, Ivan Kamath, Sebastian Vuli\\u00b4c, Kyunghyun Ruder, Gregor Geigle, Max Glockner, Jonas Beck, Nils Pfeiffer, Reimers Iryna, Victor Sanh, Colin Raffel, Lintang Bach, Zaid Sutawika, Antoine Alyafeai, Arnaud Chaffin, Arun Stiegler, Manan Raja, Dey Saiful, Canwen Bari, Urmish Xu, Thakker, Shanya Sharma, Eliza Szczechla, Taewoon, Gunjan Kim, Nihal Chhablani, Nayak, Debajyoti, Jonathan Datta, Mike Tian-Jian Chang, Han Jiang, Matteo Wang, S. Manica, Zheng Xin Shen, Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Tripathi Neeraj, Jos Rozen, Abheesht Sharma, A. Santilli, Thibault F\\u00e9vry, Jason Alan Fries, Maarten Sap, Hannah Rashkin, Derek Chen, Ronan, Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Adam R. Brown, Adam Santoro, Adri\\u00e0 Gupta, Agnieszka Garriga-Alonso, Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par-765, Allen Nie, Aman Hussain, Amanda Askell, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, A. Santilli, Andreas Stuhlm\\u00fcller, Andrew M. Dai, Andrew La, Andrew Lampinen, Angela Zou, Angelica Jiang, Anh Chen, Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-773, Arul Menezes, Arun Kirubarajan, Asher Mul-774, Ashish lokandov, Austin Sabharwal, Herrick, Avia, A. Efrat, Ayla Erdem, B. Karaka\\u00b8s, Ryan Roberts, B. S. Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan \\u00d6zyurt, Behnam Hedayatnia, Behnam, Benjamin Neyshabur, Benno Inden, Berk Stein, Ek-779 mekci, Bill Yuchen, Blake Lin, Cameron Howald, Cameron Diao, Catherine Dour, Cedrick Stinson, Ar-781 C\\u00e9sar, Chandan Ferri Ram\\u00edrez, Charles Singh, Christopher D. Manning, Christopher Potts, Cindy 785 Ramirez, Clara Rivera, Clemencia Siro, Colin Raf-786, Courtney Ashcraft, Cristina Garbacea, Dan Sileo, Daniel H Garrette, Dan Hendrycks, Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\\u00ed Gonz\\u00e1lez, Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, D. Gilboa, David Dohan, D. Drakard, David Ju-792, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Tam, m\\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-807 L\\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijase-808 vic, Hannah Kim, Harsh Mehta, H. Bogar, Henry Shevlin, Hinrich Sch\\u00fctze, H. Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jae-813 hoon Lee, J. Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Ko-815 co\\u00b4n, Jana Thompson, Jared Kaplan, Jarema Radom, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, K. Markert, Kaustubh D. Dhole, Kevin Gim-827 pel, Kevin Omondi, K. Mathewson, Kristen Chi-828 afullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-829 Donell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras Ochando, Louis-Philippe Morency, Luca Moschella, Maarten \\u00b8Senel, Maarten Bosma, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Jose Maru, Marie Ram\\u00edrez Quintana, Tolkiehn Mario, Martha Giulianelli, Martin Lewis, L. PotthastMatthew, Matthew L. Leavitt, M\\u00e1ty\\u00e1s Schu-840 bert Hagen, Medina Orduna, Melody Baitemirova, Arnaud Melvin, Michael A McElrath, Michael Yee, Michael Co-842 hen, Michael Gu, M. Ivanitskiy, Michael Star-843 ritt, M. Strube, Michele Sw\\u02dbedrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mo Suzgun, Monica Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T. Varma, Nanyun Peng, tish Shirish Keskar, Niveditha Iyer, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio, Moreno Casares, Parth Doshi, Jason Wei, Maarten Bosma, Vincent Y. Zhao, Adams Wei Guu, Brian Yu, Nan Lester, An-921 Du, M. Dai, Quoc V. Le, Finetuned, Adina Williams, Nikita Nangia, Samuel R. Bowman, Thomas Wolf, Lysandre Debut, Clement Chaumond, Anthony Delangue, Pier-339 Moi, Tim ric Cistac, R\\u00e9mi Rault, Morgan Louf, Funtow-900 Joe, Sam Davison, Patrick Shleifer, von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Sylvain Gugger, Mariama Drame, Yinfei Yang, Yuan Zhang, Chris Tar, Hailey Schoelkopf, Niklas Muen-954, Alham Fikri, David Ifeoluwa Adelani, M Saiful Bari, Lintang Sutawika, Zhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo, Jonathan May; Abstract: Adapters have been positioned as a parameter-001 efficient fine-tuning (PEFT) approach, whereby 002 a minimal number of parameters are added to 003 the model and fine-tuned. However, adapters 004 have not been sufficiently analyzed to under-005 stand if PEFT translates to benefits in train-006 ing/deployment efficiency and maintainabil-007 ity/extensibility. Through extensive experi-008 ments on many adapters, tasks, and languages 009 in supervised and cross-lingual zero-shot set-010 tings, we clearly show that for Natural Lan-011 guage Understanding tasks, the parameter ef-012 ficiency in adapters does not translate to effi-013 ciency gains compared to full fine-tuning of 014 models. More precisely, adapters are relatively 015 expensive to train and have slightly higher de-016 ployment latency. Furthermore, the maintain-017 ability /extensibility benefits of adapters can be 018 achieved with simpler approaches like multi-019 task training via full fine-tuning, which also 020 provide relatively faster training times. We, 021 therefore, recommend that for moderately sized 022 models practitioners should rely on full fine-023 tuning or multi-task training rather than using 024 adapters. 025; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is recommended that for moderately sized models practitioners should rely on full fine-023 tuning or multi-task training rather than using adapters, as adapters are relatively expensive to train and have slightly higher de-016 ployment latency.'} Author (LTI's Professor): Daphne Ippolito; Title: Are aligned neural networks adversarially aligned?; Authors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tram\\u00e8r, Ludwig Schmidt; Abstract: Large language models are now tuned to align with the goals of their creators, namely to be\\\"helpful and harmless.\\\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 75; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models, and conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.'} Author (LTI's Professor): Daphne Ippolito; Title: Effective Prompt Extraction from Language Models; Authors: Yiming Zhang, Daphne Ippolito; Abstract: The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction experiments on real systems such as Bing Chat and ChatGPT suggest that system prompts can be revealed by an adversary despite existing defenses in place.; Year: 2023; Venue: ; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper presents a framework for systematically measuring the effectiveness of prompt extraction attacks and determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination.'} Author (LTI's Professor): Daphne Ippolito; Title: Scalable Extraction of Training Data from (Production) Language Models; Authors: Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. F. Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tram\\u00e8r, Katherine Lee; Abstract: This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.; Year: 2023; Venue: arXiv.org; Citations: 47; TLDR: {'model': 'tldr@v2.0.0', 'text': 'In order to attack the aligned ChatGPT, a new divergence attack is developed that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly.'} Author (LTI's Professor): Lori Levin; Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient; Authors: W. Gaetz, C. Dockstader, P. Furlong, S. Amaral, A. Vossough, E. Schwartz, T. Roberts, Lori S. Levin; Abstract: None; Year: 2023; Venue: Brain Research; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Lori Levin; Title: Identifying Health-Related Quality of Life Domains after Upper Extremity Transplantation.; Authors: D. Tulsky, Pamela A. Kisala, Callie E Tyner, J. Slotkin, C. Kaufman, C. Dearth, A. Horan, S. Talbot, J. Shores, K. Azari, C. Cetrulo, G. Brandacher, C. Cooney, David E Victorson, M. Dooley, Lori S. Levin, Cdr Scott M Tintle; Abstract: None; Year: 2023; Venue: Archives of Physical Medicine and Rehabilitation; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This study identified key constructs for use in evaluation of the potentially substantial physical, medical, social, and emotional effects of UET, including physical functioning and medical complications, positive and negative emotional functioning, and social participation, relationships, and independence.'} Author (LTI's Professor): Lori Levin; Title: Assessment of quality of life after upper extremity transplantation: Framework for patient-reported outcome scale domains; Authors: Callie E Tyner, J. Slotkin, Pamela A. Kisala, Lori S. Levin, Scott M. Tintle, D. Tulsky; Abstract: Upper extremity transplantation offers the promise of restored function and regained quality of life (QOL) for individuals who have sustained hand or arm amputation. However, a major challenge for this procedure becoming an accessible treatment option for patients is the lack of standard measures to document benefits to QOL. Patient-reported outcomes (PRO) measures are well-suited for this kind of intervention, where the perspective of the patient is central to defining treatment success. To date, qualitative work with experts, clinicians, and patients has been used to identify the most important domains of QOL for PRO item development. Specifically, our group\\u2019s qualitative work has identified several domains of QOL that are unique to individuals who have received upper extremity transplants, which are distinct from topics covered by existing PRO measures. These include emotional and social aspects of upper extremity transplant, such as Expectations and Perceived Outcomes, Integration and Assimilation of Transplant, Fitting in, and Post-Surgical Challenges and Complications. The broad topic of Satisfaction with Transplant was subdivided into three subtopics: Function, Sensation, and Aesthetics. Satisfaction with Sensation was also identified as a unique domain not evaluated by existing PRO measures. This report operationalizes these eight QOL domains by presenting scoping definitions. This manuscript describes the work that has been completed for domain characterization as an early step toward developing standardized PRO measures to evaluate these important outcomes specific to upper extremity transplantation.; Year: 2023; Venue: Frontiers in Psychology; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Qualitative work with experts, clinicians, and patients has identified several domains of QOL that are unique to individuals who have received upper extremity transplants, which are distinct from topics covered by existing PRO measures.'} Author (LTI's Professor): Lori Levin; Title: What is needed to ensure long-term sustainability for the field of vascularized composite allotransplantation?; Authors: Yoshiko Toyoda, Lori S. Levin; Abstract: The field of vascularized composite allotransplantation (VCA) has demonstrated remarkable advances since its inception with some excellent long-term results in a variety of graft types. However, unlike solid organ transplantation, it has yet to become mainstream. We therefore discuss strategies on ensuring long-term sustainability by addressing continued clinical developments of VCA to improve the risk-to-benefit balance, importance of public support, improved policy and financial support, and need for a bridge to the future of transplant surgery. There has been headway on all fronts and collaboration among the VCA centers for centralization of data and incorporation of patient voices will be essential for continued progress.; Year: 2023; Venue: Current Opinion in Organ Transplantation; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Strategies on ensuring long-term sustainability by addressing continued clinical developments of VCA to improve the risk-to-benefit balance, importance of public support, improved policy and financial support, and need for a bridge to the future of transplant surgery are discussed.'} Author (LTI's Professor): Lei Li; Title: Can Language Models Understand Physical Concepts?; Authors: Lei Li, Jingjing Xu, Qingxiu Dong, Ce Zheng, Qi Liu, Lingpeng Kong, Xu Sun; Abstract: Language models~(LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is not yet clear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85\\\\% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up the parameters of LMs 134x. Our dataset is available at \\\\url{https://github.com/TobiasLee/VEC}; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A distillation method is proposed to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up the parameters of LMs 134x, and indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge.'} Author (LTI's Professor): Lei Li; Title: Silkie: Preference Distillation for Large Visual Language Models; Authors: Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong; Abstract: This paper explores preference distillation for large vision language models (LVLMs), improving their ability to generate helpful and faithful responses anchoring the visual context. We first build a vision-language feedback (VLFeedback) dataset utilizing AI annotation. Specifically, responses are generated by models sampled from 12 LVLMs, conditioned on multi-modal instructions sourced from various datasets. We adopt GPT-4V to assess the generated outputs regarding helpfulness, visual faithfulness, and ethical considerations. Furthermore, the preference supervision is distilled into Qwen-VL-Chat through the direct preference optimization (DPO) method. The resulting model Silkie, achieves 6.9% and 9.5% relative improvement on the MME benchmark regarding the perception and cognition capabilities, respectively. Silkie also demonstrates reduced hallucination by setting a new state-of-the-art score of 3.02 on the MMHal-Bench benchmark. Further analysis shows that DPO with our VLFeedback dataset mainly boosts the fine-grained perception and complex cognition abilities of LVLMs, leading to more comprehensive improvements compared to human-annotated preference datasets.; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper explores preference distillation for large vision language models (LVLMs), improving their ability to generate helpful and faithful responses anchoring the visual context, leading to more comprehensive improvements compared to human-annotated preference datasets.'} Author (LTI's Professor): Lei Li; Title: Large Language Models are not Fair Evaluators; Authors: Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui; Abstract: In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the\\\"win/tie/lose\\\"outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\\\url{https://github.com/i-Eval/FairEval} to facilitate future research.; Year: 2023; Venue: arXiv.org; Citations: 165; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a calibration framework with three simple yet effective strategies that successfully mitigates evaluation bias, resulting in closer alignment with human judgments.'} Author (LTI's Professor): Lei Li; Title: M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning; Authors: Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, Qi Liu; Abstract: Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese. We have open-sourced the dataset to encourage further research.; Year: 2023; Venue: arXiv.org; Citations: 47; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Ying-VLM, a VLM model trained on the M$^3$IT dataset, is developed, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese.'} Author (LTI's Professor): Lei Li; Title: Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning; Authors: Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun; Abstract: In-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mechanism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow computation layers' processing; (2) the consolidated information in label words serves as a reference for LLMs' final predictions. Based on these insights, we introduce an anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL. The promising applications of our findings again validate the uncovered ICL working mechanism and pave the way for future studies.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 35; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL are introduced.'} Author (LTI's Professor): Lei Li; Title: Communication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter; Authors: Yi Liu, Xiaohan Bi, Lei Li, Sishuo Chen, Wenkai Yang, Xu Sun; Abstract: Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as a promising paradigm for institutions with limited language resources. This approach allows multiple institutions to act as clients and train a unified model through model synchronization, rather than collecting sensitive data for centralized training. This significantly reduces the cost of corpus collection and preserves data privacy. However, as pre-trained language models (PLMs) continue to increase in size, the communication cost for transmitting parameters during synchronization has become a training speed bottleneck. In this paper, we propose a communication-efficient Fed-MNMT framework that addresses this issue by keeping PLMs frozen and only transferring lightweight adapter modules between clients. Since different language pairs exhibit substantial discrepancies in data distributions, adapter parameters of clients may conflict with each other. To tackle this, we explore various clustering strategies to group parameters for integration and mitigate the negative effects of conflicting parameters. Experimental results demonstrate that our framework reduces communication cost by over 98% while achieving similar or even better performance compared to competitive baselines. Further analysis reveals that clustering strategies effectively solve the problem of linguistic discrepancy and pruning adapter modules further improves communication efficiency.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a communication-efficient Fed-MNMT framework that addresses the problem of linguistic discrepancy by keeping PLMs frozen and only transferring lightweight adapter modules between clients, and explores various clustering strategies to group parameters for integration and mitigate the negative effects of conflicting parameters.'} Author (LTI's Professor): Lei Li; Title: A Survey for In-context Learning; Authors: Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui; Abstract: With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few training examples. It has been a new trend exploring ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress, challenges, and future work in ICL. We \\ufb01rst present a formal de\\ufb01nition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques of ICL, including training strategies, prompting strategies, and so on. Finally, we present the challenges of ICL and provide potential directions for further research. We hope our work can encourage more research on uncovering how ICL works and improving ICL in future work. 1; Year: 2023; Venue: arXiv.org; Citations: 167; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The progress, challenges, and future work in ICL are summarized and a formal definition of ICL is presented and its correlation to related studies are clarified and potential directions for further research are provided.'} Author (LTI's Professor): Lei Li; Title: ImageNetVC: Zero-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories; Authors: Heming Xia, Qingxiu Dong, Lei Li, Jingjing Xu, Ziwei Qin, Zhifang Sui; Abstract: Recently, Pretrained Language Models (PLMs) have been serving as general-purpose interfaces, posing a significant demand for comprehensive visual knowledge. However, it remains unclear how well current PLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge. To investigate this, we propose I MAGE N ET VC, a fine-grained, human-annotated dataset specifically designed for zero-shot visual common-sense evaluation across 1,000 ImageNet categories. Utilizing I MAGE N ET VC, we delve into the fundamental visual commonsense knowledge of both unimodal PLMs and VaLMs, un-covering the scaling law and the influence of the backbone model on VaLMs. Furthermore, we investigate the factors affecting the visual commonsense knowledge of large-scale models, providing insights into the development of language models enriched with visual commonsense knowledge. Our code and dataset are available at https://github.com/hemingkx/ ImageNetVC .; Year: 2023; Venue: arXiv.org; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes I MAGE N ET VC, a fine-grained, human-annotated dataset specifically designed for zero-shot visual common-sense evaluation across 1,000 ImageNet categories, and delves into the fundamental visual commonsense knowledge of both unimodal PLMs and VaLMs, un-covering the scaling law and the influence of the backbone model on VaL Ms.'} Author (LTI's Professor): Lei Li; Title: Can We Edit Factual Knowledge by In-Context Learning?; Authors: Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, Baobao Chang; Abstract: Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or out-dated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/Zce1112zslx/IKE.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 44; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge.'} Author (LTI's Professor): Teruko Mitamura; Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA; Authors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg; Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.; Year: 2023; Venue: Workshop on Document-grounded Dialogue and Conversational Question Answering; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.'} Author (LTI's Professor): Teruko Mitamura; Title: ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules; Authors: Zhi-Qi Cheng, Qianwen Dai, Siyao Li, Jingdong Sun, T. Mitamura, A. Hauptmann; Abstract: Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy. We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrating its superiority over existing methods. Our proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model. Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.1; Year: 2023; Venue: IEEE International Conference on Computer Vision; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model and offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.'} Author (LTI's Professor): Teruko Mitamura; Title: Hierarchical Event Grounding; Authors: Jiefu Ou, Adithya Pratapa, Rishubh Gupta, T. Mitamura; Abstract: Event grounding aims at linking mention references in text corpora to events from a knowledge base (KB). Previous work on this task focused primarily on linking to a single KB event, thereby overlooking the hierarchical aspects of events. Events in documents are typically described at various levels of spatio-temporal granularity. These hierarchical relations are utilized in downstream tasks of narrative understanding and schema construction. In this work, we present an extension to the event grounding task that requires tackling hierarchical event structures from the KB. Our proposed task involves linking a mention reference to a set of event labels from a subevent hierarchy in the KB. We propose a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss. On an automatically created multilingual dataset from Wikipedia and Wikidata, our experiments demonstrate the effectiveness of the hierarchical loss against retrieve and re-rank baselines. Furthermore, we demonstrate the systems' ability to aid hierarchical discovery among unseen events. Code is available at https://github.com/JefferyO/Hierarchical-Event-Grounding; Year: 2023; Venue: AAAI Conference on Artificial Intelligence; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents an extension to the event grounding task that requires tackling hierarchical event structures from the KB, and proposes a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework; Authors: P. Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard J. Chen, Zihao Deng, Nicholas Allen, R. Auerbach, Faisal Mahmood, R. Salakhutdinov, Louis-Philippe Morency; Abstract: The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task, and introduces two new estimators for these PID statistics that scale to high-dimensional distributions.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Quantifying & Modeling Feature Interactions: An Information Decomposition Framework; Authors: P. Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard J. Chen, Zihao Deng, Faisal Mahmood, R. Salakhutdinov, Louis-Philippe Morency; Abstract: The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different signals. Despite these empirical advances, there remain fundamental research questions: how can we quantify the nature of interactions that exist among input features? Subsequently, how can we capture these interactions using suitable data-driven methods? To answer this question, we propose an information-theoretic approach to quantify the degree of redundancy , uniqueness , and synergy across input features, which we term the PID statistics of a multimodal distribution. Using 2 newly proposed estimators that scale to high-dimensional distributions, we demonstrate their usefulness in quantifying the interactions within multimodal datasets, the nature of interactions captured by multimodal models, and principled approaches for model selection. We conduct extensive experiments on both synthetic datasets where the PID statistics are known and on large-scale multimodal benchmarks where PID estimation was previously impossible. Finally, to demonstrate the real-world applicability of our approach, we present three case studies in pathology, mood prediction, and robotic perception where our framework accurately recommends strong multimodal models for each application.; Year: 2023; Venue: arXiv.org; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy across input features, which is term the PID statistics of a multimodal distribution.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities; Authors: Alex Wilf, Sihyun Shawn Lee, P. Liang, Louis-Philippe Morency; Abstract: Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory\\\"Simulation Theory\\\"to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs' ToM capabilities.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"SimToM is introduced, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking that shows substantial improvement over existing methods and suggests perspective- taking as a promising direction for future research into improving LLMs' ToM capabilities.\\\"} Author (LTI's Professor): Louis-Philippe Morency; Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings; Authors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman; Abstract: None; Year: 2023; Venue: Journal of Vision; Citations: 0; TLDR: None Author (LTI's Professor): Louis-Philippe Morency; Title: Tutorial on Multimodal Machine Learning: Principles, Challenges, and Open Questions; Authors: P. Liang, Louis-Philippe Morency; Abstract: Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents capable of understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in healthcare and robotics, multimodality has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this tutorial is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. Building upon a new edition of our survey paper on multimodal ML and academic courses at CMU, this tutorial will cover three topics: (1) what is multimodal: the principles in learning from heterogeneous, connected, and interacting data, (2) why is it hard: a taxonomy of six core technical challenges faced in multimodal ML but understudied in unimodal ML, and (3) what is next: major directions for future research as identified by our taxonomy.; Year: 2023; Venue: ICMI Companion; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This tutorial is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning by synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Continual Learning for Personalized Co-Speech Gesture Generation; Authors: Chaitanya Ahuja, Pratik Joshi, Ryo Ishii, Louis-Philippe Morency; Abstract: Co-speech gestures are a key channel of human communication, making them important for personalized chat agents to generate. In the past, gesture generation models assumed that data for each speaker is available all at once, and in large amounts. However in practical scenarios, speaker data comes sequentially and in small amounts as the agent personalizes with more speakers, akin to a continual learning paradigm. While more recent works have shown progress in adapting to low-resource data, they catastrophically forget the gesture styles of initial speakers they were trained on. Also, prior generative continual learning works are not multimodal, making this space less studied. In this paper, we explore this new paradigm and propose C-DiffGAN: an approach that continually learns new speaker gesture styles with only a few minutes of per-speaker data, while retaining previously learnt styles. Inspired by prior continual learning works, C-DiffGAN encourages knowledge retention by 1) generating reminiscences of previous low-resource speaker data, then 2) crossmodally aligning to them to mitigate catastrophic forgetting. We quantitatively demonstrate improved performance and reduced forgetting over strong baselines through standard continual learning measures, reinforced by a qualitative user study that shows that our method produces more natural, style-preserving gestures. Code and videos can be found at https://chahuja.com/cdiffgan; Year: 2023; Venue: IEEE International Conference on Computer Vision; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper quantitatively demonstrate improved performance and reduced forgetting over strong baselines through standard continual learning measures, reinforced by a qualitative user study that shows that the proposed C-DiffGAN method produces more natural, style-preserving gestures.'} Author (LTI's Professor): Louis-Philippe Morency; Title: MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning; Authors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov; Abstract: Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'MultiZoo is released, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas that provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation.'} Author (LTI's Professor): Louis-Philippe Morency; Title: MMOE: Mixture of Multimodal Interaction Experts; Authors: Haofei Yu, P. Liang, R. Salakhutdinov, Louis-Philippe Morency; Abstract: Multimodal machine learning, which studies the information and interactions across various input modalities, has made significant advancements in understanding the relationship between images and descriptive text. However, this is just a portion of the potential multimodal interactions seen in the real world and does not include new interactions between conflicting utterances and gestures in predicting sarcasm, for example. Notably, the current methods for capturing shared information often do not extend well to these more nuanced interactions, sometimes performing as low as 50% in binary classification. In this paper, we address this problem via a new approach called MMOE, which stands for a mixture of multimodal interaction experts. Our method automatically classifies data points from unlabeled multimodal datasets by their interaction type and employs specialized models for each specific interaction. Based on our experiments, this approach improves performance on these challenging interactions by more than 10%, leading to an overall increase of 2% for tasks like sarcasm prediction. As a result, interaction quantification provides new insights for dataset analysis and yields simple approaches that obtain state-of-the-art performance.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This method automatically classifies data points from unlabeled multimodal datasets by their interaction type and employs specialized models for each specific interaction, leading to an overall increase of 2% for tasks like sarcasm prediction.'} Author (LTI's Professor): Louis-Philippe Morency; Title: SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations; Authors: Victoria Lin, Louis-Philippe Morency; Abstract: Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents SenteCon, a method for introducing human interpretability in deep language representations that outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth.; Authors: H. Swartz, Lauren M. Bylsma, Jay Fournier, J. Girard, C. Spotts, J. Cohn, Louis-Philippe Morency; Abstract: None; Year: 2023; Venue: Journal of Affective Disorders; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Preliminary evidence supporting the efficacy of both brief IPT and CBT, delivered by either TH or IP, for depression showed that working alliance is preserved in TH, and delivery via TH may improve therapy adherence.'} Author (LTI's Professor): Louis-Philippe Morency; Title: MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things; Authors: Shentong Mo, P. Liang, Russ Salakhutdinov, Louis-Philippe Morency; Abstract: The Internet of Things (IoT), the network integrating billions of smart physical devices embedded with sensors, software, and communication technologies for the purpose of connecting and exchanging data with other devices and systems, is a critical and rapidly expanding component of our modern world. The IoT ecosystem provides a rich source of real-world modalities such as motion, thermal, geolocation, imaging, depth, sensors, video, and audio for prediction tasks involving the pose, gaze, activities, and gestures of humans as well as the touch, contact, pose, 3D of physical objects. Machine learning presents a rich opportunity to automatically process IoT data at scale, enabling efficient inference for impact in understanding human wellbeing, controlling physical devices, and interconnecting smart cities. To develop machine learning technologies for IoT, this paper proposes MultiIoT, the most expansive IoT benchmark to date, encompassing over 1.15 million samples from 12 modalities and 8 tasks. MultiIoT introduces unique challenges involving (1) learning from many sensory modalities, (2) fine-grained interactions across long temporal ranges, and (3) extreme heterogeneity due to unique structure and noise topologies in real-world sensors. We also release a set of strong modeling baselines, spanning modality and task-specific methods to multisensory and multitask models to encourage future research in multisensory representation learning for IoT.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes MultiIoT, the most expansive IoT benchmark to date, encompassing over 1.15 million samples from 12 modalities and 8 tasks, and releases a set of strong modeling baselines, spanning modality and task-specific methods to multisensory and multitask models to encourage future research in mult isensory representation learning for IoT.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Neural Mixed Effects for Nonlinear Personalized Predictions; Authors: T. W\\u00f6rtwein, Nicholas Allen, Lisa B. Sheeber, R. Auerbach, J. Cohn, Louis-Philippe Morency; Abstract: Personalized prediction is a machine learning approach that predicts a person\\u2019s future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a neural network in a scalable manner1. NME combines the efficiency of neural network optimization with nonlinear mixed effects modeling. Empirically, we observe that NME improves performance across six unimodal and multimodal datasets, including a smartphone dataset to predict daily mood and a mother-adolescent dataset to predict affective state sequences where half the mothers experience symptoms of depression. Furthermore, we evaluate NME for two model architectures, including for neural conditional random fields (CRF) to predict affective state sequences where the CRF learns nonlinear person-specific temporal transitions between affective states. Analysis of these person-specific transitions on the mother-adolescent dataset shows interpretable trends related to the mother\\u2019s depression symptoms.; Year: 2023; Venue: International Conference on Multimodal Interaction; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'NME combines the efficiency of neural network optimization with nonlinear mixed effects modeling and improves performance across six unimodal and multimodal datasets, including a smartphone dataset to predict daily mood and a mother-adolescent datasets to predict affective state sequences where half the mothers experience symptoms of depression.'} Author (LTI's Professor): Louis-Philippe Morency; Title: MultiZoo and MultiBench: A Standardized Toolkit for Multimodal Deep Learning; Authors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov; Abstract: None; Year: 2023; Venue: Journal of machine learning research; Citations: 0; TLDR: None Author (LTI's Professor): Louis-Philippe Morency; Title: SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior; Authors: Maneesh Bilalpur, Saurabh Hinduja, Laura Cariola, Lisa B. Sheeber, Nicholas B Allen, Louis-Philippe Morency, Jeffrey F. Cohn; Abstract: Depression strongly impacts parents\\u2019 behavior. Does parents\\u2019 depression strongly affect the behavior of their children as well? To investigate this question, we compared dyadic interactions between 73 depressed and 75 non-depressed mothers and their adolescent child. Families were of low income and 84% were white. Child behavior was measured from audio-video recordings using manual annotation of verbal and nonverbal behavior by expert coders and by multimodal computational measures of facial expression, face and head dynamics, prosody, speech behavior, and linguistics. For both sets of measures, we used Support Vector Machines. For computational measures, we investigated the relative contribution of single versus multiple modalities using a novel approach to SHapley Additive exPlanations (SHAP). Computational measures outperformed manual ratings by human experts. Among individual computational measures, prosody was the most informative. SHAP reduction resulted in a four-fold decrease in the number of features and highest performance (77% accuracy; positive and negative agreements at 75% and 76%, respectively). These findings suggest that maternal depression strongly impacts the behavior of adolescent children; differences are most revealed in prosody; multimodal features together with SHAP reduction are most powerful.; Year: 2023; Venue: International Conference on Multimodal Interaction; Citations: 0; TLDR: None Author (LTI's Professor): Louis-Philippe Morency; Title: What are Adapters Really Efficient At?; Authors: Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Mitesh M. Khapra, Pratyush Kumar, V. Rudra, Murthy Anoop, Kunchukuttan. 2022, Naama-677, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir R. Radev, Al-683 ham, Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Jonas Pfeiffer, Aishwarya Kamath, Andreas R\\u00fcckl\\u00e9, Kyunghyun Cho, Iryna Gurevych, Clifton Poth, Aishwarya, Ivan Kamath, Sebastian Vuli\\u00b4c, Kyunghyun Ruder, Gregor Geigle, Max Glockner, Jonas Beck, Nils Pfeiffer, Reimers Iryna, Victor Sanh, Colin Raffel, Lintang Bach, Zaid Sutawika, Antoine Alyafeai, Arnaud Chaffin, Arun Stiegler, Manan Raja, Dey Saiful, Canwen Bari, Urmish Xu, Thakker, Shanya Sharma, Eliza Szczechla, Taewoon, Gunjan Kim, Nihal Chhablani, Nayak, Debajyoti, Jonathan Datta, Mike Tian-Jian Chang, Han Jiang, Matteo Wang, S. Manica, Zheng Xin Shen, Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Tripathi Neeraj, Jos Rozen, Abheesht Sharma, A. Santilli, Thibault F\\u00e9vry, Jason Alan Fries, Maarten Sap, Hannah Rashkin, Derek Chen, Ronan, Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Adam R. Brown, Adam Santoro, Adri\\u00e0 Gupta, Agnieszka Garriga-Alonso, Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par-765, Allen Nie, Aman Hussain, Amanda Askell, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, A. Santilli, Andreas Stuhlm\\u00fcller, Andrew M. Dai, Andrew La, Andrew Lampinen, Angela Zou, Angelica Jiang, Anh Chen, Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-773, Arul Menezes, Arun Kirubarajan, Asher Mul-774, Ashish lokandov, Austin Sabharwal, Herrick, Avia, A. Efrat, Ayla Erdem, B. Karaka\\u00b8s, Ryan Roberts, B. S. Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan \\u00d6zyurt, Behnam Hedayatnia, Behnam, Benjamin Neyshabur, Benno Inden, Berk Stein, Ek-779 mekci, Bill Yuchen, Blake Lin, Cameron Howald, Cameron Diao, Catherine Dour, Cedrick Stinson, Ar-781 C\\u00e9sar, Chandan Ferri Ram\\u00edrez, Charles Singh, Christopher D. Manning, Christopher Potts, Cindy 785 Ramirez, Clara Rivera, Clemencia Siro, Colin Raf-786, Courtney Ashcraft, Cristina Garbacea, Dan Sileo, Daniel H Garrette, Dan Hendrycks, Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\\u00ed Gonz\\u00e1lez, Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, D. Gilboa, David Dohan, D. Drakard, David Ju-792, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Tam, m\\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-807 L\\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijase-808 vic, Hannah Kim, Harsh Mehta, H. Bogar, Henry Shevlin, Hinrich Sch\\u00fctze, H. Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jae-813 hoon Lee, J. Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Ko-815 co\\u00b4n, Jana Thompson, Jared Kaplan, Jarema Radom, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, K. Markert, Kaustubh D. Dhole, Kevin Gim-827 pel, Kevin Omondi, K. Mathewson, Kristen Chi-828 afullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-829 Donell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras Ochando, Louis-Philippe Morency, Luca Moschella, Maarten \\u00b8Senel, Maarten Bosma, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Jose Maru, Marie Ram\\u00edrez Quintana, Tolkiehn Mario, Martha Giulianelli, Martin Lewis, L. PotthastMatthew, Matthew L. Leavitt, M\\u00e1ty\\u00e1s Schu-840 bert Hagen, Medina Orduna, Melody Baitemirova, Arnaud Melvin, Michael A McElrath, Michael Yee, Michael Co-842 hen, Michael Gu, M. Ivanitskiy, Michael Star-843 ritt, M. Strube, Michele Sw\\u02dbedrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mo Suzgun, Monica Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T. Varma, Nanyun Peng, tish Shirish Keskar, Niveditha Iyer, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio, Moreno Casares, Parth Doshi, Jason Wei, Maarten Bosma, Vincent Y. Zhao, Adams Wei Guu, Brian Yu, Nan Lester, An-921 Du, M. Dai, Quoc V. Le, Finetuned, Adina Williams, Nikita Nangia, Samuel R. Bowman, Thomas Wolf, Lysandre Debut, Clement Chaumond, Anthony Delangue, Pier-339 Moi, Tim ric Cistac, R\\u00e9mi Rault, Morgan Louf, Funtow-900 Joe, Sam Davison, Patrick Shleifer, von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Sylvain Gugger, Mariama Drame, Yinfei Yang, Yuan Zhang, Chris Tar, Hailey Schoelkopf, Niklas Muen-954, Alham Fikri, David Ifeoluwa Adelani, M Saiful Bari, Lintang Sutawika, Zhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo, Jonathan May; Abstract: Adapters have been positioned as a parameter-001 efficient fine-tuning (PEFT) approach, whereby 002 a minimal number of parameters are added to 003 the model and fine-tuned. However, adapters 004 have not been sufficiently analyzed to under-005 stand if PEFT translates to benefits in train-006 ing/deployment efficiency and maintainabil-007 ity/extensibility. Through extensive experi-008 ments on many adapters, tasks, and languages 009 in supervised and cross-lingual zero-shot set-010 tings, we clearly show that for Natural Lan-011 guage Understanding tasks, the parameter ef-012 ficiency in adapters does not translate to effi-013 ciency gains compared to full fine-tuning of 014 models. More precisely, adapters are relatively 015 expensive to train and have slightly higher de-016 ployment latency. Furthermore, the maintain-017 ability /extensibility benefits of adapters can be 018 achieved with simpler approaches like multi-019 task training via full fine-tuning, which also 020 provide relatively faster training times. We, 021 therefore, recommend that for moderately sized 022 models practitioners should rely on full fine-023 tuning or multi-task training rather than using 024 adapters. 025; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is recommended that for moderately sized models practitioners should rely on full fine-023 tuning or multi-task training rather than using adapters, as adapters are relatively expensive to train and have slightly higher de-016 ployment latency.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Text-Transport: Toward Learning Causal Effects of Natural Language; Authors: Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael; Abstract: As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper introduces Text-Transport, a method for estimation of causal effects from natural language under any text distribution that transports causal effects between domains, bypassing the need for strong assumptions in the target domain.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos; Authors: Dong Won Lee, Chaitanya Ahuja, P. Liang, Sanika Natu, Louis-Philippe Morency; Abstract: Many educational videos use slide presentations, a sequence of visual pages that contain text and figures accompanied by spoken language, which are constructed and presented carefully in order to optimally transfer knowledge to students. Previous studies in multimedia and psychology attribute the effectiveness of lecture presentations to their multimodal nature. As a step toward developing vision-language models to aid in student learning as intelligent teacher assistants, we introduce the Lecture Presentations Multimodal (LPM) Dataset as a large-scale benchmark testing the capabilities of vision-and-language models in multimodal understanding of educational videos. Our dataset contains aligned slides and spoken language, for 180+ hours of video and 9000+ slides, with 10 lecturers from various subjects (e.g., computer science, dentistry, biology). We introduce three research tasks, (1) figure-to-text retrieval, (2) text-to-figure retrieval, and (3) generation of slide explanations, which are grounded in multimedia learning and psychology principles to test a vision-language model\\u2019s understanding of multimodal content. We provide manual annotations to help implement these tasks and establish baselines on them. Comparing baselines and human student performances, we find that state-of-the-art vision-language models (zero-shot and fine-tuned) struggle in (1) weak crossmodal alignment between slides and spoken text, (2) learning novel visual mediums, (3) technical language, and (4) long-range sequences. We introduce PolyViLT, a novel multimodal transformer trained with a multi-instance learning loss that is more effective than current approaches for retrieval. We conclude by shedding light on the challenges and opportunities in multimodal understanding of educational presentation videos.; Year: 2023; Venue: IEEE International Conference on Computer Vision; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'PolyViLT is introduced, a novel multimodal transformer trained with a multi-instance learning loss that is more effective than current approaches for retrieval and sheds light on the challenges and opportunities in multimodal understanding of educational presentation videos.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Counterfactual Augmentation for Multimodal Learning Under Presentation Bias; Authors: Victoria Lin, Louis-Philippe Morency, D. Dimitriadis, Srinagesh Sharma; Abstract: In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a presentation bias in the labels that compromises the ability to train new models. In this paper, we propose counterfactual augmentation, a novel causal method for correcting presentation bias using generated counterfactual labels. Our empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods. Model analyses further indicate that the generated counterfactuals align closely with true counterfactuals in an oracle setting.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods, and model analyses indicate that the generatedcounterfactuals align closely with true counterfactUALs in an oracle setting.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Multimodal Feature Selection for Detecting Mothers' Depression in Dyadic Interactions with their Adolescent Offspring; Authors: Maneesh Bilalpur, Saurabh Hinduja, Laura A. Cariola, Lisa B. Sheeber, Nick Alien, L\\u00e1szl\\u00f3 A. Jeni, Louis-Philippe Morency, J. Cohn; Abstract: Depression is the most common psychological disorder, a leading cause of disability world-wide, and a major contributor to inter-generational transmission of psychopathol-ogy within families. To contribute to our understanding of depression within families and to inform modality selection and feature reduction, it is critical to identify interpretable features in developmentally appropriate contexts. Mothers with and without depression were studied. Depression was defined as history of treatment for depression and elevations in current or recent symptoms. We explored two multimodal feature selection strategies in dyadic interaction tasks of mothers with their adolescent children for depression detection. Modalities included face and head dynamics, facial action units, speech-related behavior, and verbal features. The initial feature space was vast and inter-correlated (collinear). To reduce dimension-ality and gain insight into the relative contribution of each modality and feature, we explored feature selection strategies using Variance Inflation Factor (VIF) and Shapley values. On an average collinearity correction through VIF resulted in about 4 times feature reduction across unimodal and multimodal features. Collinearity correction was also found to be an optimal intermediate step prior to Shapley analysis. Shapley feature selection following VIF yielded best performance. The top 15 features obtained through Shapley achieved 78 % accuracy. The most informative features came from all four modalities sampled, which supports the importance of multimodal feature selection.; Year: 2023; Venue: IEEE International Conference on Automatic Face & Gesture Recognition; Citations: 2; TLDR: None Author (LTI's Professor): Louis-Philippe Morency; Title: Difference-Masking: Choosing What to Mask in Continued Pretraining; Authors: Alex Wilf, Syeda Nahida Akter, Leena Mathur, P. Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency; Abstract: The self-supervised objective of masking-and-predicting has led to promising performance gains on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition that deciding what to mask can substantially improve learning outcomes. We investigate this in continued pretraining setting in which pretrained models continue to pretrain on domain-specific data before performing some downstream task. We introduce Difference-Masking, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language-only and multimodal video tasks.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Difference-Masking is introduced, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Expanding the Role of Affective Phenomena in Multimodal Interaction Research; Authors: Leena Mathur, Maja J Matari'c, Louis-Philippe Morency; Abstract: In recent decades, the field of affective computing has made substantial progress in advancing the ability of AI systems to recognize and express affective phenomena, such as affect and emotions, during human-human and human-machine interactions. This paper describes our examination of research at the intersection of multimodal interaction and affective computing, with the objective of observing trends and identifying understudied areas. We examined over 16,000 papers from selected conferences in multimodal interaction, affective computing, and natural language processing: ACM International Conference on Multimodal Interaction, AAAC International Conference on Affective Computing and Intelligent Interaction, Annual Meeting of the Association for Computational Linguistics, and Conference on Empirical Methods in Natural Language Processing. We identified 910 affect-related papers and present our analysis of the role of affective phenomena in these papers. We find that this body of research has primarily focused on enabling machines to recognize or express affect and emotion; there has been limited research on how affect and emotion predictions might, in turn, be used by AI systems to enhance machine understanding of human social behaviors and cognitive states. Based on our analysis, we discuss directions to expand the role of affective phenomena in multimodal interaction research.; Year: 2023; Venue: International Conference on Multimodal Interaction; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An examination of research at the intersection of multimodal interaction and affective computing, with the objective of observing trends and identifying understudied areas, finds that this body of research has primarily focused on enabling machines to recognize or express affect and emotion.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification; Authors: P. Liang, Yun Cheng, R. Salakhutdinov, Louis-Philippe Morency; Abstract: In order to perform multimodal fusion of heterogeneous signals, we need to understand their interactions: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how humans annotate two categorizations of multimodal interactions: (1) partial labels, where different annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator annotates the label given the first modality before asking them to explicitly reason about how their answer changes when given the second. We further propose an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions, uniqueness: the extent to which one modality enables a prediction that the other does not, and synergy: the extent to which both modalities enable one to make a prediction that one would not otherwise make using individual modalities. Through experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate and efficient method for quantifying multimodal interactions.; Year: 2023; Venue: International Conference on Multimodal Interaction; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A comparative study of how humans annotate two categorizations of multimodal interactions is performed and a method to automatically convert annotations of partial and counterfactual labels to information decomposition is proposed, yielding an accurate and efficient method for quantifying multimodals interactions.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Intensive Longitudinal Assessment of Adolescents to Predict Suicidal Thoughts and Behaviors.; Authors: R. Auerbach, Ranqing Lan, H. Galfalvy, Kira L. Alqueza, J. Cohn, Ryann Crowley, Katherine Durham, Karla Joyce, Lauren E. Kahn, Rahil A. Kamath, Louis-Philippe Morency, G. Porta, A. Srinivasan, Jamie Zelazny, D. Brent, Nicholas Allen; Abstract: None; Year: 2023; Venue: Journal of the American Academy of Child and Adolescent Psychiatry; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"Intensive longitudinal assessment through personal smartphones offers a feasible method to assess variability in adolescents' emotional experiences and suicide risk.\\\"} Author (LTI's Professor): Louis-Philippe Morency; Title: Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications; Authors: P. Liang, Chun Kai Ling, Yun Cheng, A. Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, R. Salakhutdinov; Abstract: In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We validate these estimated bounds and show how they accurately track true interactions. Finally, two semi-supervised multimodal applications are explored based on these theoretical results: (1) analyzing the relationship between multimodal performance and estimated interactions, and (2) self-supervised learning that embraces disagreement between modalities beyond agreement as is typically done.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings and validate these estimated bounds and show how they accurately track true interactions.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Representation Learning for Interpersonal and Multimodal Behavior Dynamics: A Multiview Extension of Latent Change Score Models; Authors: A. Vail, J. Girard, Lauren M. Bylsma, Jay Fournier, Holly A. Swartz, Jeffrey F. Cohn, Louis-Philippe Morency; Abstract: Characterizing the dynamics of behavior across multiple modalities and individuals is a vital component of computational behavior analysis. This is especially important in certain applications, such as psychotherapy, where individualized tracking of behavior patterns can provide valuable information about the patient\\u2019s mental state. Conventional methods that rely on aggregate statistics and correlational metrics may not always suffice, as they are often unable to capture causal relationships or evaluate the true probability of identified patterns. To address these challenges, we present a novel approach to learning multimodal and interpersonal representations of behavior dynamics during one-on-one interaction. Our approach is enabled by the introduction of a multiview extension of latent change score models, which facilitates the concurrent capture of both inter-modal and interpersonal behavior dynamics and the identification of directional relationships between them. A core advantage of our approach is its high level of interpretability while simultaneously achieving strong predictive performance. We evaluate our approach within the domain of therapist-client interactions, with the objective of gaining a deeper understanding about the collaborative relationship between the two, a crucial element of the therapeutic process. Our results demonstrate improved performance over conventional approaches that rely upon summary statistics or correlational metrics. Furthermore, since our multiview approach includes the explicit modeling of uncertainty, it naturally lends itself to integration with probabilistic classifiers, such as Gaussian process models. We demonstrate that this integration leads to even further improved performance, all the while maintaining highly interpretable qualities. Our analysis provides compelling motivation for further exploration of stochastic systems within computational models of behavior.; Year: 2023; Venue: International Conference on Multimodal Interaction; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents a novel approach to learning multimodal and interpersonal representations of behavior dynamics during one-on-one interaction enabled by the introduction of a multiview extension of latent change score models that demonstrates improved performance over conventional approaches that rely upon summary statistics or correlational metrics.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Comparative Knowledge Distillation; Authors: Alex Wilf, Alex Tianyi Xu, P. Liang, A. Obolenskiy, Daniel Fried, Louis-Philippe Morency; Abstract: In the era of large scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally heavy teacher models to lightweight, efficient student models while preserving performance. Traditional KD paradigms, however, assume readily available access to teacher models for frequent inference -- a notion increasingly at odds with the realities of costly, often proprietary, large scale models. Addressing this gap, our paper considers how to minimize the dependency on teacher model inferences in KD in a setting we term Few Teacher Inference Knowledge Distillation (FTI KD). We observe that prevalent KD techniques and state of the art data augmentation strategies fall short in this constrained setting. Drawing inspiration from educational principles that emphasize learning through comparison, we propose Comparative Knowledge Distillation (CKD), which encourages student models to understand the nuanced differences in a teacher model's interpretations of samples. Critically, CKD provides additional learning signals to the student without making additional teacher calls. We also extend the principle of CKD to groups of samples, enabling even more efficient learning from limited teacher calls. Empirical evaluation across varied experimental settings indicates that CKD consistently outperforms state of the art data augmentation and KD techniques.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"Drawing inspiration from educational principles that emphasize learning through comparison, CKD is proposed, which encourages student models to understand the nuanced differences in a teacher model's interpretations of samples, and consistently outperforms state of the art data augmentation and KD techniques.\\\"} Author (LTI's Professor): Louis-Philippe Morency; Title: MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models; Authors: P. Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, R. Salakhutdinov; Abstract: The nature of human and computer interactions are inherently multimodal, which has led to substantial interest in building interpretable, interactive, and reliable multimodal interfaces. However, modern multimodal models and interfaces are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize their internal workings in order to empower stakeholders to visualize model behavior, perform model debugging, and promote trust in these models? Our paper proposes MultiViz, a method for analyzing the behavior of multimodal models via 4 stages: (1) unimodal importance, (2) cross-modal interactions, (3) multimodal representations and (4) multimodal prediction. MultiViz includes modular visualization tools for each stage before combining outputs from all stages through an interactive and human-in-the-loop API. Through user studies with 21 participants on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available at https://github.com/pliang279/MultiViz, will be regularly updated with new visualization tools and metrics, and welcomes input from the community1.; Year: 2023; Venue: CHI Extended Abstracts; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that the complementary stages in MultiViz together enable users to simulate model predictions, assign interpretable concepts to features, perform error analysis on model misclassifications, and use insights from error analysis to debug models.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models; Authors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang; Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE, and formulate the underlying data-generating process as a hierarchical latent variable model, and shows that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy; Authors: P. Liang, Zihao Deng, Martin Q. Ma, James Y. Zou, Louis-Philippe Morency, R. Salakhutdinov; Abstract: In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks; Year: 2023; Venue: Neural Information Processing Systems; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'FactorCL is a new multimodal representation learning method to go beyond multi-view redundancy and captures both shared and unique information and achieves state-of-the-art results on six benchmarks.'} Author (LTI's Professor): Louis-Philippe Morency; Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents; Authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap; Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.; Year: 2023; Venue: arXiv.org; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': 'SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence, is presented and it is found that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.'} Author (LTI's Professor): Louis-Philippe Morency; Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions; Authors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, P. Liang, Louis-Philippe Morency; Abstract: Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper empirically shows that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced, and argues that the few-shot de-biasing approach is highly feasible and practical.'} Author (LTI's Professor): David Mortensen; Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages; Authors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig; Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\\u2019s resource level is the most important feature in determining ChatGPT\\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.; Year: 2023; Venue: Conference on Machine Translation; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work presents the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark, and reveals that a language\\u2019s resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests thatChatGPT is especially disadvantaged for LRLs and African languages.\\\"} Author (LTI's Professor): David Mortensen; Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models; Authors: Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov; Abstract: Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work conducts a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages and shows evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results.\\\"} Author (LTI's Professor): David Mortensen; Title: African Substrates Rather Than European Lexifiers to Augment African-diaspora Creole Translation; Authors: David R. Mortensen; Abstract: Machine translation (MT) model training is difficult for low-resource languages. This is especially true for African-diaspora Creole languages because of data scarcity. Cross-lingual data augmentation methods with knowledge transfer from related high-resource languages are a common technique to overcome this disadvantage. For instance, practitioners may transfer knowledge from a language in the same language family as the low-resource language of interest. Africandiaspora Creole languages are low-resource and simultaneously have relationships with multiple language groups. These languages, such as Haitian Creole and Jamaican Patois, are typically lexified by colonial European languages, but they are structurally similar to African languages. We explore the advantages of transferring knowledge from the European lexifier language versus the phylogenetic and typological relatives of the African substrate languages. We analysed Haitian and Jamaican MT: both controlling tightly for data properties across compared transfer languages and later allowing use of all data we collected. Our inquiry demonstrates a significant advantage in using African transfer languages in some settings.; Year: 2023; Venue: AfricaNLP; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This inquiry demonstrates a significant advantage in using African transfer languages in some settings with the advantages of transferring knowledge from the European lexifier language versus the phylogenetic and typological relatives of the African substrate languages.'} Author (LTI's Professor): David Mortensen; Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing; Authors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin; Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.; Year: 2023; Venue: Special Interest Group on Computational Morphology and Phonology Workshop; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'In this submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), approaches to data augmentation and modeling across seven low-resource languages are explored and token classification models are found to be the best performing.'} Author (LTI's Professor): David Mortensen; Title: Multilingual TTS Accent Impressions for Accented ASR; Authors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo; Abstract: None; Year: 2023; Venue: International Conference on Text, Speech and Dialogue; Citations: 0; TLDR: None Author (LTI's Professor): David Mortensen; Title: Construction Grammar Provides Unique Insight into Neural Language Models; Authors: Leonie Weissweiler, Taiqi He, Naoki Otani, David R. Mortensen, L. Levin, Hinrich Sch\\u00fctze; Abstract: Construction Grammar (CxG) has recently been used as the basis for probing studies that have investigated the performance of large pretrained language models (PLMs) with respect to the structure and meaning of constructions. In this position paper, we make suggestions for the continuation and augmentation of this line of research. We look at probing methodology that was not designed with CxG in mind, as well as probing methodology that was designed for specific constructions. We analyse selected previous work in detail, and provide our view of the most important challenges and research questions that this promising new field faces.; Year: 2023; Venue: CXGSNLP; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The view of the most important challenges and research questions that this promising new field of research faces is provided, and an analysis of selected previous work in detail is provided.'} Author (LTI's Professor): David Mortensen; Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation; Authors: David R. Mortensen, Ela Gulsen, Taiqi He, Nathaniel R. Robinson, Jonathan D. Amith, Lindia Tjuatja, L. Levin; Abstract: Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation convention\\u00e2\\u20ac\\u201dGeneralized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.; Year: 2023; Venue: Special Interest Group on Computational Morphology and Phonology Workshop; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An annotation convention is proposed that combines all of these positive properties using an Item-and-Process (IP) framework, and its linguistic adequacy is demonstrated, and it is compared with two other interlinear glossed text annotation schemes.'} Author (LTI's Professor): David Mortensen; Title: Transformed Protoform Reconstruction; Authors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen; Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The Meloni et al (2021) model is updated with the state-of-the-art seq2seq model: the Transformer, which outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognate spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties.'} Author (LTI's Professor): David Mortensen; Title: PWESuite: Phonetic Word Embeddings and Tasks They Facilitate; Authors: Vil\\u00e9m Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel R. Robinson, Mrinmaya Sachan, David R. Mortensen; Abstract: Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Three methods that use articulatory features to build phonetically informed word embeddings are developed that address the inconsistent evaluation of existing phonetic word embedding methods and contribute a task suite to fairly evaluate past, current, and future methods.'} Author (LTI's Professor): Graham Neubig; Title: DiffusER: Diffusion via Edit-based Reconstruction; Authors: Machel Reid, V. Hellendoorn, Graham Neubig; Abstract: In text generation, models that generate text from scratch one token at a time are currently the dominant paradigm. Despite being performant, these models lack the ability to revise existing text, which limits their usability in many practical scenarios. We look to address this, with DIFFUSER (Diffusion via Edit-based Reconstruction), a new edit-based generative model for text based on denoising diffusion models \\u2013 a class of models that use a Markov chain of denoising steps to incrementally generate data. DIFFUSER is not only a strong generative model in general, rivalling autoregressive models on several tasks spanning machine translation, summarization, and style transfer; it can also perform other varieties of generation that standard autoregressive models are not well-suited for. For instance, we demonstrate that DIFFUSER makes it possible for a user to condition generation on a prototype, or an incomplete sequence, and continue revising based on previous edit steps.; Year: 2023; Venue: International Conference on Learning Representations; Citations: 14; TLDR: {'model': 'tldr@v2.0.0', 'text': 'DIFFUSER (Diffusion via Edit-based Reconstruction), a new edit-based generative model for text based on denoising diffusion models \\u2013 a class of models that use a Markov chain of denoising steps to incrementally generate data.'} Author (LTI's Professor): Graham Neubig; Title: Cross-Modal Fine-Tuning: Align then Refine; Authors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar; Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.; Year: 2023; Venue: International Conference on Machine Learning; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work proposes ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities and highlights the importance of data alignment via a series of ablation studies and demonstrates ORCA's utility in data-limited regimes.\\\"} Author (LTI's Professor): Graham Neubig; Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages; Authors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig; Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\\u2019s resource level is the most important feature in determining ChatGPT\\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.; Year: 2023; Venue: Conference on Machine Translation; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work presents the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark, and reveals that a language\\u2019s resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests thatChatGPT is especially disadvantaged for LRLs and African languages.\\\"} Author (LTI's Professor): Graham Neubig; Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing; Authors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, FAHIM FAISAL, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig; Abstract: Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces GlobalBench, an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages and tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world.'} Author (LTI's Professor): Graham Neubig; Title: Learning Performance-Improving Code Edits; Authors: Aman Madaan, Alex Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, A. Yazdanbakhsh; Abstract: The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.; Year: 2023; Venue: arXiv.org; Citations: 28; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper investigates the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits, and hypothesizes that language models can suggest such edits in ways that would be impractical for static analysis alone.'} Author (LTI's Professor): Graham Neubig; Title: CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code; Authors: Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig; Abstract: Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 26; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is found that generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed, than all existing metrics.'} Author (LTI's Professor): Graham Neubig; Title: Divergences between Language Models and Human Brains; Authors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe; Abstract: Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories.'} Author (LTI's Professor): Graham Neubig; Title: Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction; Authors: Manuel Mager, R. Bhatnagar, Graham Neubig, Ngoc Thang Vu, Katharina Kann; Abstract: Neural models have drastically advanced state of the art for machine translation (MT) between high-resource languages. Traditionally, these models rely on large amounts of training data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of parallel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open questions, product of an increased interest of the NLP community in these languages.; Year: 2023; Venue: AMERICASNLP; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for high- resource languages between high-resource languages.'} Author (LTI's Professor): Graham Neubig; Title: User-Centric Evaluation of OCR Systems for Kwak\\u2019wala; Authors: Shruti Rijhwani, Daisy Rosenblum, Michayla King, Antonios Anastasopoulos, Graham Neubig; Abstract: There has been recent interest in improving optical character recognition (OCR) for endangered languages, particularly because a large number of documents and books in these languages are not in machine-readable formats. The performance of OCR systems is typically evaluated using automatic metrics such as character and word error rates. While error rates are useful for the comparison of different models and systems, they do not measure whether and how the transcriptions produced from OCR tools are useful to downstream users. In this paper, we present a human-centric evaluation of OCR systems, focusing on the Kwak'wala language as a case study. With a user study, we show that utilizing OCR reduces the time spent in the manual transcription of culturally valuable documents -- a task that is often undertaken by endangered language community members and researchers -- by over 50%. Our results demonstrate the potential benefits that OCR tools can have on downstream language documentation and revitalization efforts.; Year: 2023; Venue: COMPUTEL; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This paper presents a human-centric evaluation of OCR systems, focusing on the Kwak'wala language as a case study, and shows that utilizing OCR reduces the time spent in the manual transcription of culturally valuable documents by over 50%.\\\"} Author (LTI's Professor): Graham Neubig; Title: EXCALIBUR: Encouraging and Evaluating Embodied Exploration; Authors: Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Weihs; Abstract: Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: \\u201cis the small heavy red bowl made from glass?\\u201d or \\u201cis there a silver spoon heavier than the egg?\\u201d. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures and represents the next frontier for embodied AI research.'} Author (LTI's Professor): Graham Neubig; Title: Multi-Dimensional Evaluation of Text Summarization with In-Context Learning; Authors: Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, Chunting Zhou; Abstract: Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 14; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency.'} Author (LTI's Professor): Graham Neubig; Title: A Gold Standard Dataset for the Reviewer Assignment Problem; Authors: Ivan Stelmakh, J. Wieting, Graham Neubig, Nihar B. Shah; Abstract: Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\\\"similarity score\\\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information.; Year: 2023; Venue: arXiv.org; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel dataset of similarity scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously is collected and used to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders.'} Author (LTI's Professor): Graham Neubig; Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing; Authors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin; Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.; Year: 2023; Venue: Special Interest Group on Computational Morphology and Phonology Workshop; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'In this submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), approaches to data augmentation and modeling across seven low-resource languages are explored and token classification models are found to be the best performing.'} Author (LTI's Professor): Graham Neubig; Title: Alignment for Honesty; Authors: Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, Pengfei Liu; Abstract: Recent research has made significant strides in applying alignment techniques to enhance the helpfulness and harmlessness of large language models (LLMs) in accordance with human intentions. In this paper, we argue for the importance of alignment for honesty, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning the limits of an LLM's knowledge, which is far from straightforward. This challenge demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. In this paper, we address these challenges by first establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM's honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source a wealth of resources to facilitate future research at https://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned models, training and evaluation datasets for honesty alignment, concept glossary, as well as all relevant source code.; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This paper argues for the importance of alignment for honesty, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative, by establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius.\\\"} Author (LTI's Professor): Graham Neubig; Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation; Authors: Patrick Fernandes, Aman Madaan, Emmy Liu, Ant\\u00f3nio Farinhas, Pedro Henrique Martins, Amanda Bertsch, Jos\\u00e9 G. C. de Souza, Shuyan Zhou, Tongshuang Sherry Wu, Graham Neubig, Andr\\u00e9 F. T. Martins; Abstract: Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.; Year: 2023; Venue: arXiv.org; Citations: 23; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An overview of the recent research that has leveraged human feedback to improve natural language generation and the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention is provided.'} Author (LTI's Professor): Graham Neubig; Title: Program-Aided Reasoners (better) Know What They Know; Authors: Anubha Kabra, Sanketh Rangreji, Yash Mathur, Aman Madaan, Emmy Liu, Graham Neubig; Abstract: Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to\\\"know what they know\\\", which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper compares the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models and demonstrates that, in the majority of cases, program-aided reasoners better know what they know thanText-based counterparts.'} Author (LTI's Professor): Graham Neubig; Title: Learning to Filter Context for Retrieval-Augmented Generation; Authors: Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, Graham Neubig; Abstract: On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.; Year: 2023; Venue: arXiv.org; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'FILCO is proposed, a method that improves the quality of the context provided to the generator by identifying useful context based on lexical and information-theoretic approaches, and training context filtering models that can filter retrieved contexts at test time.'} Author (LTI's Professor): Graham Neubig; Title: FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios; Authors: Ethan Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu; Abstract: The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .; Year: 2023; Venue: arXiv.org; Citations: 48; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT), and demonstrates the efficacy of the proposed method.'} Author (LTI's Professor): Graham Neubig; Title: Active Retrieval Augmented Generation; Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig; Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 50; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'} Author (LTI's Professor): Graham Neubig; Title: Large Language Models Enable Few-Shot Clustering; Authors: Vijay Viswanathan, Kiril Gashteovski, Carolin (Haas) Lawrence, Tongshuang Sherry Wu, Graham Neubig; Abstract: Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.; Year: 2023; Venue: arXiv.org; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is found that incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters.'} Author (LTI's Professor): Graham Neubig; Title: Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach; Authors: Masahiro Kaneko, Graham Neubig, Naoaki Okazaki; Abstract: Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.; Year: 2023; Venue: arXiv.org; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue and shows that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.'} Author (LTI's Professor): Graham Neubig; Title: Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting; Authors: Emmy Liu, Aditi Chaudhary, Graham Neubig; Abstract: Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute accuracy, but also holds potential benefits for non-idiomatic sentences.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'To improve translation of natural idioms, this work introduces two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models.'} Author (LTI's Professor): Graham Neubig; Title: Why do Nearest Neighbor Language Models Work?; Authors: Frank F. Xu, Uri Alon, Graham Neubig; Abstract: Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.; Year: 2023; Venue: International Conference on Machine Learning; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper identifies three main reasons why k-nearest neighbor language models (kNN-LM) perform better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution.'} Author (LTI's Professor): Graham Neubig; Title: Syntax and Semantics Meet in the \\u201cMiddle\\u201d: Probing the Syntax-Semantics Interface of LMs Through Agentivity; Authors: Lindia Tjuatja, Emmy Liu, L. Levin, Graham Neubig; Abstract: Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms\\u2014i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.; Year: 2023; Venue: STARSEM; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far and suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.'} Author (LTI's Professor): Graham Neubig; Title: DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions; Authors: Vijay Viswanathan, Luyu Gao, Tongshuang Sherry Wu, Pengfei Liu, Graham Neubig; Abstract: Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We operationalize the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval algorithms on our test set and present a superior bi-encoder retriever for text-based dataset recommendation. This system, trained on the DataFinder Dataset, finds more relevant search results than existing third-party dataset search engines. To encourage progress on dataset recommendation, we release our dataset and models to the public.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work operationalizes the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs, and builds the DataFinder Dataset, a larger automatically-constructed training set and a smaller expert-annotated evaluation set.'} Author (LTI's Professor): Graham Neubig; Title: Multi-lingual and Multi-cultural Figurative Language Understanding; Authors: Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, Graham Neubig; Abstract: Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, \\\\datasetname, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 11; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work assesses multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings, and reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region.\\\"} Author (LTI's Professor): Graham Neubig; Title: It\\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk; Authors: Amanda Bertsch, Alex Xie, Graham Neubig, Matthew R. Gormley; Abstract: Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area.; Year: 2023; Venue: BIGPICTURE; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical.'} Author (LTI's Professor): Graham Neubig; Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input; Authors: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley; Abstract: Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .; Year: 2023; Venue: Neural Information Processing Systems; Citations: 42; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores.'} Author (LTI's Professor): Graham Neubig; Title: WebArena: A Realistic Web Environment for Building Autonomous Agents; Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig; Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.; Year: 2023; Venue: arXiv.org; Citations: 73; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'} Author (LTI's Professor): Graham Neubig; Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions; Authors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig; Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment.'} Author (LTI's Professor): Graham Neubig; Title: Computational Language Acquisition with Theory of Mind; Authors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig; Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.; Year: 2023; Venue: International Conference on Learning Representations; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"It is found that training speakers with a highly weighted ToM listener component leads to performance gains in the authors' image referential game setting, and suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.\\\"} Author (LTI's Professor): Graham Neubig; Title: DeMuX: Data-efficient Multilingual Learning; Authors: Simran Khanuja, Srinivas Gowriraj, L. Dery, Graham Neubig; Abstract: We consider the task of optimally fine-tuning pre-trained multilingual models, given small amounts of unlabelled target data and an annotation budget. In this paper, we introduce DEMUX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for complex tasks. Our code is released here: https://github.com/simran-khanuja/demux.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'DEMUX is introduced, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set, to optimally fine-tuning pre-trained multilingual models.'} Author (LTI's Professor): Graham Neubig; Title: Improving Factuality of Abstractive Summarization via Contrastive Reward Learning; Authors: Ethan Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig; Abstract: Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries. Code and human evaluation results will be publicly available at \\\\url{https://github.com/EthanC111/factuality_summarization}.; Year: 2023; Venue: TRUSTNLP; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations, suggesting that further advances in learning and evaluation algorithms can feed directly into providing morefactuality summaries.'} Author (LTI's Professor): Graham Neubig; Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents; Authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap; Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.; Year: 2023; Venue: arXiv.org; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': 'SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence, is presented and it is found that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.'} Author (LTI's Professor): Graham Neubig; Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation; Authors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat; Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.; Year: 2023; Venue: Conference on Machine Translation; Citations: 22; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations, and finds that it improves performance compared to just prompting for scores.'} Author (LTI's Professor): Eric Nyberg; Title: GameQA: Gamified Mobile App Platform for Building Multiple-Domain Question-Answering Datasets; Authors: Njall Skarphedinsson, Breki Gudmundsson, Steinar Smari, M. L\\u00e1rusd\\u00f3ttir, H. Einarsson, Abuzar Khan, Eric Nyberg, H. Loftsson; Abstract: The methods used to create many of the well-known Question-Answering (QA) datasets are hard to replicate for low-resource languages. A commonality amongst these methods is hiring annotators to source answers from the internet by querying a single answer source, such as Wikipedia. Applying these methods for low-resource languages can be problematic since there is no single large answer source for these languages. Consequently, this can result in a high ratio of unanswered questions, since the amount of information in any single source is limited. To address this problem, we developed a novel crowd-sourcing platform to gather multiple-domain QA data for low-resource languages. Our platform, which consists of a mobile app and a web API, gamifies the data collection process. We successfully released the app for Icelandic (a low-resource language with about 350,000 native speakers) to build a dataset which rivals large QA datasets for high-resource languages both in terms of size and ratio of answered questions. We have made the platform open source with instructions on how to localize and deploy it to gather data for other low-resource languages.; Year: 2023; Venue: Conference of the European Chapter of the Association for Computational Linguistics; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work developed a novel crowd-sourcing platform to gather multiple-domain QA data for low-resource languages and successfully released the app for Icelandic to build a dataset which rivals large QA datasets for high- resource languages both in terms of size and ratio of answered questions.'} Author (LTI's Professor): Eric Nyberg; Title: InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers; Authors: Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayan Kundu, R. Ramanathan, Eric Nyberg; Abstract: We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available. https://github.com/searchivarius/inpars_light/; Year: 2023; Venue: arXiv.org; Citations: 11; TLDR: {'model': 'tldr@v2.0.0', 'text': 'InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25.'} Author (LTI's Professor): Eric Nyberg; Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA; Authors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg; Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.; Year: 2023; Venue: Workshop on Document-grounded Dialogue and Conversational Question Answering; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.'} Author (LTI's Professor): Eric Nyberg; Title: Chain-of-Skills: A Configurable Model for Open-Domain Question Answering; Authors: Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, Jianfeng Gao; Abstract: The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transfer- ability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a modular retriever where individual modules correspond to key skills that can be reused across datasets and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.'} Author (LTI's Professor): Bhiksha Raj; Title: FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding; Authors: Thanh-Dat Truong, Ngan T. H. Le, B. Raj, J. Cothren, Khoa Luu; Abstract: Although Domain Adaptation in Semantic Scene Segmentation has shown impressive improvement in recent years, the fairness concerns in the domain adaptation have yet to be well defined and addressed. In addition, fairness is one of the most critical aspects when deploying the segmentation models into human-related real-world applications, e.g., autonomous driving, as any unfair predictions could influence human safety. In this paper, we propose a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In particular, from the proposed formulated fairness objective, a new adaptation framework will be introduced based on the fair treatment of class distributions. Moreover, to generally model the context of structural dependency, a new conditional structural constraint is introduced to impose the consistency of predicted segmentation. Thanks to the proposed Conditional Structure Network, the self-attention mechanism has sufficiently modeled the structural information of segmentation. Through the ablation studies, the proposed method has shown the performance improvement of the segmentation models and promoted fairness in the model predictions. The experimental results on the two standard benchmarks, i.e., SYNTHIA $\\\\rightarrow$ Cityscapes and GTA5 $\\\\rightarrow$ Cityscapes, have shown that our method achieved State-of-the-Art (SOTA) performance11The implementation of FREDOM is available at https://github.com/uark-cviu/FREDOM; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation, where a new adaptation framework will be introduced based on the fair treatment of class distributions to generally model the context of structural dependency.'} Author (LTI's Professor): Bhiksha Raj; Title: UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation; Authors: Pha Nguyen, Kha Gia Quach, J. Gauch, S. Khan, B. Raj, Khoa Luu; Abstract: Multiple Object Tracking (MOT) aims to find bounding boxes and identities of targeted objects in consecutive video frames. While fully-supervised MOT methods have achieved high accuracy on existing datasets, they cannot generalize well on a newly obtained dataset or a new unseen domain. In this work, we first address the MOT problem from the cross-domain point of view, imitating the process of new data acquisition in practice. Then, a new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects. It can also learn and update itself from the target data feedback. The intensive experiments are designed on four challenging settings, including MOTSynth to MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then prove the adaptability of the proposed self-supervised learning strategy. The experiments also show superior performance on tracking metrics MOTA and IDF1, compared to fully supervised, unsupervised, and self-supervised state-of-the-art methods.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects, and it can also learn and update itself from the target data feedback.'} Author (LTI's Professor): Bhiksha Raj; Title: SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning; Authors: Hao Chen, R. Tao, Yue Fan, Yidong Wang, Jindong Wang, B. Schiele, Xingxu Xie, B. Raj, M. Savvides; Abstract: The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.; Year: 2023; Venue: International Conference on Learning Representations; Citations: 34; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper revisits the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrates the inherent quantity-quality trade-off problem of pseudo-labels with thresholding, which may prohibit learning.'} Author (LTI's Professor): Bhiksha Raj; Title: Fixed Inter-Neuron Covariability Induces Adversarial Robustness; Authors: Muhammad A Shah, B. Raj; Abstract: The vulnerability to adversarial perturbations is a major flaw of Deep Neural Networks (DNNs) that raises question about their reliability when in real-world scenarios. On the other hand, human perception, which DNNs are supposed to emulate, is highly robust to such perturbations, indicating that there may be certain features of the human perception that make it robust but are not represented in the current class of DNNs. One such feature is that the activity of biological neurons is correlated and the structure of this correlation tends to be rather rigid over long spans of times, even if it hampers performance and learning. We hypothesize that integrating such constraints on the activations of a DNN would improve its adversarial robustness, and, to test this hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern. When evaluated on image and sound recognition tasks, the models with a SCA layer achieved high accuracy, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks \\\\textit{without being trained on adversarially perturbed data; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The SCA layer is developed, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks.'} Author (LTI's Professor): Bhiksha Raj; Title: Understanding political polarization using language models: A dataset and method; Authors: Samiran Gode, Supreeth Bare, B. Raj, H. Yoo; Abstract: Our paper aims to analyze political polarization in US political system using language models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates' views on the economy, healthcare, education, and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model\\u2010based method that helps analyze how polarized a candidate is. Our data are divided into two parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, and so forth. We further split this data into four phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization, we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer\\u2010based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background. The code and data for the project will be available here: \\u201chttps://github.com/samirangode/Understanding_Polarization\\u201d; Year: 2023; Venue: The AI Magazine; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model\\u2010based method that helps analyze how polarized a candidate is.'} Author (LTI's Professor): Bhiksha Raj; Title: Prolonged school closure during the pandemic time in successive waves of COVID-19- vulnerability of children to sexual abuses \\u2013 A case study in Tamil Nadu, India; Authors: Kandaswamy Paramasivan, B. Raj, Nandan Sudarasanam, R. Subburaj; Abstract: None; Year: 2023; Venue: Heliyon; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Considering that the median delay in filing CSA complaints was above 30 days in the mild and post-intervention periods, the upsurge of cases in the more relaxed phases indicates increased occurrences of CSA during strict lockdowns.'} Author (LTI's Professor): Bhiksha Raj; Title: BASS: Block-wise Adaptation for Speech Summarization; Authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj; Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.; Year: 2023; Venue: Interspeech; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'} Author (LTI's Professor): Bhiksha Raj; Title: Understanding Political Polarisation using Language Models: A dataset and method; Authors: Samiran Gode, Supreeth Bare, B. Raj, H. Yoo; Abstract: Our paper aims to analyze political polarization in US political system using Language Models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates views on the economy, healthcare, education and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is. Our data is divided into 2 parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, etc. We further split this data into 4 phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is are used to understand the polarization.'} Author (LTI's Professor): Bhiksha Raj; Title: An Approach to Ontological Learning from Weak Labels; Authors: Ankit Shah, Larry Tang, Po Hao Chou, Yilun Zheng, Ziqian Ge, B. Raj; Abstract: Ontologies encompass a formal representation of knowledge through the definition of concepts or properties of a domain, and the relationships between those concepts. In this work, we seek to investigate whether using this ontological information will improve learning from weakly labeled data, which are easier to collect since it requires only the presence or absence of an event to be known. We use the AudioSet ontology and dataset, which contains audio clips weakly labeled with the ontology concepts and the ontology providing the \\\"Is A\\\" relations between the concepts. We first re-implemented the model proposed by [1] with modifications to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts. We find that the baseline Twin Neural Network (TNN) does not perform better by incorporating ontology information in the weak and multi-label scenario, but that the GCN does capture the ontology knowledge better for weak, multi-labeled data. We also investigate how different modules can tolerate noises introduced from weak labels and better incorporate ontology information. Our best TNN-GCN model achieves mAP=0.45 and AUC=0.87 for lower-level concepts and mAP=0.72 and AUC=0.86 for higher-level concepts, which is an improvement over the baseline TNN but about the same as our models that do not use ontology information.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work re-implements the model proposed by [1] with modifications to fit the multi-label scenario and expands on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts.'} Author (LTI's Professor): Bhiksha Raj; Title: Rethinking Voice-Face Correlation: A Geometry View; Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, B. Raj; Abstract: Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.; Year: 2023; Venue: ACM Multimedia; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction and finds significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium.'} Author (LTI's Professor): Bhiksha Raj; Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement; Authors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj; Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \\u2013 such as spectral tilt, spectral flux, shimmer, etc. \\u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance.'} Author (LTI's Professor): Bhiksha Raj; Title: Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms; Authors: Joseph Konan, Ojas Bhargave, Shikhar Agnihotri, Hojeong Lee, Ankit Shah, Shuo Han, YUNYANG ZENG, Amanda Shu, Haohui Liu, Xuankai Chang, Hamza Khalid, Minseon Gwak, Kawon Lee, Minjeong Kim, B. Raj; Abstract: In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assistants, and telecommunication.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A multi-task learning framework forVoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement and outperforms both industry performance and state-of-the-art methods for speech Enhancement on VoIP applications is proposed.'} Author (LTI's Professor): Bhiksha Raj; Title: Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms; Authors: Ankit Shah, Shuyi Chen, Kejun Zhou, Yue Chen, B. Raj; Abstract: General-purpose embedding is highly desirable for few-shot even zero-shot learning in many application scenarios, including audio tasks. In order to understand representations better, we conducted a thorough error analysis and visualization of HEAR 2021 submission results. Inspired by the analysis, this work experiments with different front-end audio preprocessing methods, including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT), and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover a more holistic simulation of the frequency information received by the human auditory system. We tested the models on the suite of HEAR 2021 tasks, which encompass a broad category of tasks. Preliminary results show (1) the proposed BECR can incur a more dispersed embedding on the test set, (2) BECR improves the PaSST model without extra computation complexity, and (3) STFT preprocessing outperforms CQT in all tasks we tested. Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Experiments with different front-end audio preprocessing methods are experiments, and a Batch Embedding Covariance Regularization (BECR) term is proposed to uncover a more holistic simulation of the frequency information received by the human auditory system.'} Author (LTI's Professor): Bhiksha Raj; Title: Training on Foveated Images Improves Robustness to Adversarial Attacks; Authors: Muhammad A Shah, B. Raj; Abstract: Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks -- subtle, perceptually indistinguishable perturbations of inputs that change the response of the model. In the context of vision, we hypothesize that an important contributor to the robustness of human visual perception is constant exposure to low-fidelity visual stimuli in our peripheral vision. To investigate this hypothesis, we develop \\\\RBlur, an image transform that simulates the loss in fidelity of peripheral vision by blurring the image and reducing its color saturation based on the distance from a given fixation point. We show that compared to DNNs trained on the original images, DNNs trained on images transformed by \\\\RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\\\\% higher accuracy on perturbed data.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'DNNs trained on images transformed by \\\\\\\\RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\\\\\\\\% higher accuracy on perturbed data.'} Author (LTI's Professor): Bhiksha Raj; Title: Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations; Authors: Hao Chen, Ankit Shah, Jindong Wang, R. Tao, Yidong Wang, Xingxu Xie, Masashi Sugiyama, Rita Singh, B. Raj; Abstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \\\\textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Imprecise label learning (ILL) is introduced, a framework for the unification of learning with various imprecise label configurations, marking the first unified framework with robust and effective performance across various challenging settings.'} Author (LTI's Professor): Bhiksha Raj; Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features; Authors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj; Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.; Year: 2023; Venue: Interspeech; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes an analysis pipeline to help explore the voice-face relationship in a fine-grained manner, i.s. phonemes v. facial anthropometric measurements (AM), and indicates that AMs are more predictable from vowels compared to consonants, particularly with plosives.'} Author (LTI's Professor): Bhiksha Raj; Title: Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments; Authors: Thanh-Dat Truong, Hoang-Quan Nguyen, B. Raj, Khoa Luu; Abstract: Continual semantic segmentation aims to learn new classes while maintaining the information from the previous classes. Although prior studies have shown impressive progress in recent years, the fairness concern in the continual semantic segmentation needs to be better addressed. Meanwhile, fairness is one of the most vital factors in deploying the deep learning model, especially in human-related or safety applications. In this paper, we present a novel Fairness Continual Learning approach to the semantic segmentation problem. In particular, under the fairness objective, a new fairness continual learning framework is proposed based on class distributions. Then, a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning, i.e., catastrophic forgetting and background shift. Our proposed loss has also been proven as a novel, generalized learning paradigm of knowledge distillation commonly used in continual learning. Moreover, the proposed Conditional Structural Consistency loss further regularized the structural constraint of the predicted segmentation. Our proposed approach has achieved State-of-the-Art performance on three standard scene understanding benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairness of the segmentation model.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel Fairness Continual Learning approach to the semantic segmentation problem is presented, in particular, a new fairness continual learning framework is proposed based on class distributions and a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning.'} Author (LTI's Professor): Bhiksha Raj; Title: PaintSeg: Training-free Segmentation via Painting; Authors: Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang, B. Raj; Abstract: The paper introduces PaintSeg, a new unsupervised method for segmenting objects without any training. We propose an adversarial masked contrastive painting (AMCP) process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models. During the painting process, inpainting and outpainting are alternated, with the former masking the foreground and filling in the background, and the latter masking the background while recovering the missing part of the foreground object. Inpainting and outpainting, also referred to as I-step and O-step, allow our method to gradually advance the target segmentation mask toward the ground truth without supervision or training. PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points. Our experimental results demonstrate that PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks, providing a training-free solution suitable for unsupervised segmentation.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An adversarial masked contrastive painting process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models, providing a training-free solution suitable for unsupervised segmentation.'} Author (LTI's Professor): Bhiksha Raj; Title: TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement; Authors: YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj; Abstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work shows that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility and uses data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from the method.'} Author (LTI's Professor): Bhiksha Raj; Title: Improving sound event detection with ontologies; Authors: B. Raj; Abstract: Sound event recognition is the task of identifying and categorizing sounds in audio data. Automated algorithms for sound event recognition depend on having explicit models for individual sound event types to be recognized, which are trained on data tagged explicitly for those classes. The approach is data hungryand is fundamentally limited by the number of classes for which such data may be obtained. It also ignores the relationship between sounds being modeled. In this work, we attempt to address these deficiencies through the use of a human-generated sound ontology which represents sibling and parent\\u2013child relations between sound classes. We incorporate the relationships in the ontology through the design of an appropriate \\u201closs\\u201d function (the objective function optimized to train sound-classifier models) that incorporates the relationships in the ontology, and through appropriate model update rules which utilize data from a class to update parameters (of both ontological siblings and parents). Through experiments run on the \\u201cAudioset\\u201d (a popular, large-scale dataset of 600 sound categories), we find that better-performing models can be trained for sound classes with a given dataset, and that the amount of new data required to train models for a novel sound class can be significantly reduced.; Year: 2023; Venue: Journal of the Acoustical Society of America; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Through experiments run on the \\u201cAudioset\\u201d, it is found that better-performing models can be trained for sound classes with a given dataset, and that the amount of new data required to train models for a novel sound class can be significantly reduced.'} Author (LTI's Professor): Bhiksha Raj; Title: Synergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session; Authors: L. Heller, Benjamin Elizalde, B. Raj, Soham Deshmukh; Abstract: Machine Listening, as usually formalized, attempts to perform a task that is, from our perspective, fundamentally human-performable, and performed by humans. Current automated models of Machine Listening vary from purely data-driven approaches to approaches imitating human systems. In recent years, the most promising approaches have been hybrid in that they have used data-driven approaches informed by models of the perceptual, cognitive, and semantic processes of the human system. Not only does the guidance provided by models of human perception and domain knowledge enable better, and more generalizable Machine Listening, in the converse, the lessons learned from these models may be used to verify or improve our models of human perception themselves. This paper summarizes advances in the development of such hybrid approaches, ranging from Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds. The research described herein was presented in a special session on\\\"Synergy between human and machine approaches to sound/scene recognition and processing\\\"at the 2023 ICASSP meeting.; Year: 2023; Venue: arXiv.org; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Advances in the development of hybrid approaches to Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds are summarized.'} Author (LTI's Professor): Carolyn Ros\\u00e9; Title: High school students\\u2019 data modeling practices and processes: from modeling unstructured data to evaluating automated decisions; Authors: Shiyan Jiang, Hengtao Tang, Can Tatar, C. Ros\\u00e9, J. Chao; Abstract: ABSTRACT It\\u2019s critical to foster artificial intelligence (AI) literacy for high school students, the first generation to grow up surrounded by AI, to understand working mechanism of data-driven AI technologies and critically evaluate automated decisions from predictive models. While efforts have been made to engage youth in understanding AI through developing machine learning models, few provided in-depth insights into the nuanced learning processes. In this study, we examined high school students\\u2019 data modeling practices and processes. Twenty-eight students developed machine learning models with text data for classifying negative and positive reviews of ice cream stores. We identified nine data modeling practices that describe students\\u2019 processes of model exploration, development, and testing and two themes about evaluating automated decisions from data technologies. The results provide implications for designing accessible data modeling experiences for students to understand data justice as well as the role and responsibility of data modelers in creating AI technologies.; Year: 2023; Venue: Journal of Educational Media; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It\\u2019s critical to foster artificial intelligence literacy for high school students to understand working mechanism of data-driven AI technologies and critically evaluate automated decisions from predictive models.'} Author (LTI's Professor): Carolyn Ros\\u00e9; Title: Linguistic representations for fewer-shot relation extraction across domains; Authors: Sireesh Gururaja, Ritam Dutt, Ting-gen Liao, C. Ros\\u00e9; Abstract: Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolds on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic representations enhance generalizability by providing features that function as cross-domain pivots. We focus on the task of relation extraction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer-based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains. We find that while the inclusion of these graphs results in significantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work explores the impact of linguistic representations on cross-domain performance in a few-shot transfer setting, and investigates whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains.'} Author (LTI's Professor): Carolyn Ros\\u00e9; Title: Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning; Authors: Armineh Nourbakhsh, Sameena Shah, C. Ros\\u00e9; Abstract: In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast. Instead of a data augmentation approach, CounterComp is based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels. Our proposed auxiliary metric learning loss improves the performance of three state of the art models on four recently released datasets. We also show how the approach can improve OOD performance on unseen domains, as well as unseen compositions. Lastly, we demonstrate how the method can lead to better compositional attention patterns during training.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels.'} Author (LTI's Professor): Carolyn Ros\\u00e9; Title: Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models; Authors: James Fiacco, David Adamson, C. Ros\\u00e9; Abstract: By aligning the functional components derived from the activations of transformer models trained for AES with external knowledge such as human-understandable feature groups, the proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for performing such analyses on further neural AES systems. The analysis focuses on models trained to score essays based on organization, main idea, support, and language. The findings provide insights into the models\\u2019 decision-making processes, biases, and limitations, contributing to the development of more transparent and reliable AES systems.; Year: 2023; Venue: Workshop on Innovative Use of NLP for Building Educational Applications; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for performing such analyses on further neural AES systems.'} Author (LTI's Professor): Carolyn Ros\\u00e9; Title: Exploring Artificial Intelligence in English Language Arts with StoryQ; Authors: J. Chao, Rebecca Ellis, Shiyan Jiang, C. Ros\\u00e9, W. Finzer, Can Tatar, James Fiacco, Kenia Wiedemann; Abstract: Exploring Artificial Intelligence (AI) in English Language Arts (ELA) with StoryQ is a 10-hour curriculum module designed for high school ELA classes. The module introduces students to fundamental AI concepts and essential machine learning workflow using StoryQ, a web-based GUI environment for Grades 6-12 learners. In this module, students work with unstructured text data and learn to train, test, and improve text classification models such as intent recognition, clickbait filter, and sentiment analysis. As they interact with machine-learning language models deeply, students also gain a nuanced understanding of language and how to wield it, not just as a data structure, but as a tool in our human-human encounters as well. The current version contains eight lessons, all delivered through a full-featured online learning and teaching platform. Computers and Internet access are required to implement the module. The module was piloted in an ELA class in the Spring of 2022, and the student learning outcomes were positive. The module is currently undergoing revision and will be further tested and improved in Fall 2022.; Year: 2023; Venue: AAAI Conference on Artificial Intelligence; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Alexander Rudnicky; Title: Advancing Regular Language Reasoning in Linear Recurrent Neural Networks; Authors: Ting-Han Fan, Ta-Chung Chi, Alexander I. Rudnicky; Abstract: In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work theoretically analyze some existing LRNNs and proposes a new LRNN equipped with a block-diagonal and input-dependent transition matrix that is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.'} Author (LTI's Professor): Alexander Rudnicky; Title: Structured Dialogue Discourse Parsing; Authors: Ta-Chung Chi, Alexander I. Rudnicky; Abstract: Dialogue discourse parsing aims to uncover the internal structure of a multi-participant conversation by finding all the discourse links and corresponding relations. Previous work either treats this task as a series of independent multiple-choice problems, in which the link existence and relations are decoded separately, or the encoding is restricted to only local interaction, ignoring the holistic structural information. In contrast, we propose a principled method that improves upon previous work from two perspectives: encoding and decoding. From the encoding side, we perform structured encoding on the adjacency matrix followed by the matrix-tree learning algorithm, where all discourse links and relations in the dialogue are jointly optimized based on latent tree-level distribution. From the decoding side, we perform structured inference using the modified Chiu-Liu-Edmonds algorithm, which explicitly generates the labeled multi-root non-projective spanning tree that best captures the discourse structure. In addition, unlike in previous work, we do not rely on hand-crafted features; this improves the model\\u2019s robustness. Experiments show that our method achieves new state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).; Year: 2023; Venue: SIGDIAL Conferences; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a principled method that improves upon previous work from two perspectives: encoding and decoding and achieves new state-of-the-art results, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).'} Author (LTI's Professor): Alexander Rudnicky; Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech; Authors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky; Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.; Year: 2023; Venue: AAAI Conference on Artificial Intelligence; Citations: 17; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces the MQTTS system, a Text-to-Speech system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality, and shows that MqTTS outperforms existing TTS systems in several objective and subjective measures.'} Author (LTI's Professor): Alexander Rudnicky; Title: Tartan: an LLM Driven SocialBot; Authors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky; Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users; Year: 2023; Venue: ; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work has laid the groundwork for looking more closely at using LLMs in a conversational system and analyzes common patterns in conversations the authors' bot has had with users.\\\"} Author (LTI's Professor): Alexander Rudnicky; Title: Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4; Authors: Mario Rodr'iguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, Jo\\u00e3o Sedoc, L. F. D\\u2019Haro, Alexander I. Rudnicky; Abstract: The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics\\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.; Year: 2023; Venue: DSTC; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The datasets and baselines provided to participants are described and the submission and result details of the two proposed subtasks are discussed, which promote robust and multilingual automatic evaluation metrics.'} Author (LTI's Professor): Alexander Rudnicky; Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking; Authors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, Jo\\u00e3o Magalh\\u00e3es; Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.; Year: 2023; Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents a method for performing zero-shot Dialogue State Tracking by casting the task as a learning-to-ask-questions framework that outperforms template-based question generation and shows that QG methods need to be aligned with the same grammatical person used in the dialogue.'} Author (LTI's Professor): Maarten Sap; Title: FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions; Authors: Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, R. L. Bras, Gunhee Kim, Yejin Choi, Maarten Sap; Abstract: Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'FANToM is introduced, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering that is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain- of-thought reasoning or fine-tuning.'} Author (LTI's Professor): Maarten Sap; Title: Modeling Empathic Similarity in Personal Narratives; Authors: Jocelyn Shen, Maarten Sap, Pedro Colon-Hernandez, Hae Won Park, C. Breazeal; Abstract: The most meaningful connections between people are often fostered through expression of shared vulnerability and emotional experiences in personal narratives. We introduce a new task of identifying similarity in personal stories based on empathic resonance, i.e., the extent to which two people empathize with each others' experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP. Using insights from social psychology, we craft a framework that operationalizes empathic similarity in terms of three key features of stories: main events, emotional trajectories, and overall morals or takeaways. We create EmpathicStories, a dataset of 1,500 personal stories annotated with our empathic similarity features, and 2,000 pairs of stories annotated with empathic similarity scores. Using our dataset, we fine-tune a model to compute empathic similarity of story pairs, and show that this outperforms semantic similarity models on automated correlation and retrieval metrics. Through a user study with 150 participants, we also assess the effect our model has on retrieving stories that users empathize with, compared to naive semantic similarity-based retrieval, and find that participants empathized significantly more with stories retrieved by our model. Our work has strong implications for the use of empathy-aware models to foster human connection and empathy between people.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"A new task of identifying similarity in personal stories based on empathic resonance is introduced, i.e., the extent to which two people empathize with each others' experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP.\\\"} Author (LTI's Professor): Maarten Sap; Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements; Authors: Xuhui Zhou, Haojie Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap; Abstract: Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance\\\"your English is very good\\\"may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'COBRA frames are introduced, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context, and the importance and feasibility of contextualized NLP by modeling social factors are highlighted.'} Author (LTI's Professor): Maarten Sap; Title: Riveter: Measuring Power and Social Dynamics Between Entities; Authors: Maria Antoniak, Anjalie Field, Jimin Mun, Melanie Walsh, Lauren F. Klein, Maarten Sap; Abstract: Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research by organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions.'} Author (LTI's Professor): Maarten Sap; Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting; Authors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap; Abstract: None; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: None Author (LTI's Professor): Maarten Sap; Title: What are Adapters Really Efficient At?; Authors: Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Mitesh M. Khapra, Pratyush Kumar, V. Rudra, Murthy Anoop, Kunchukuttan. 2022, Naama-677, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir R. Radev, Al-683 ham, Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Jonas Pfeiffer, Aishwarya Kamath, Andreas R\\u00fcckl\\u00e9, Kyunghyun Cho, Iryna Gurevych, Clifton Poth, Aishwarya, Ivan Kamath, Sebastian Vuli\\u00b4c, Kyunghyun Ruder, Gregor Geigle, Max Glockner, Jonas Beck, Nils Pfeiffer, Reimers Iryna, Victor Sanh, Colin Raffel, Lintang Bach, Zaid Sutawika, Antoine Alyafeai, Arnaud Chaffin, Arun Stiegler, Manan Raja, Dey Saiful, Canwen Bari, Urmish Xu, Thakker, Shanya Sharma, Eliza Szczechla, Taewoon, Gunjan Kim, Nihal Chhablani, Nayak, Debajyoti, Jonathan Datta, Mike Tian-Jian Chang, Han Jiang, Matteo Wang, S. Manica, Zheng Xin Shen, Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Tripathi Neeraj, Jos Rozen, Abheesht Sharma, A. Santilli, Thibault F\\u00e9vry, Jason Alan Fries, Maarten Sap, Hannah Rashkin, Derek Chen, Ronan, Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Adam R. Brown, Adam Santoro, Adri\\u00e0 Gupta, Agnieszka Garriga-Alonso, Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par-765, Allen Nie, Aman Hussain, Amanda Askell, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, A. Santilli, Andreas Stuhlm\\u00fcller, Andrew M. Dai, Andrew La, Andrew Lampinen, Angela Zou, Angelica Jiang, Anh Chen, Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-773, Arul Menezes, Arun Kirubarajan, Asher Mul-774, Ashish lokandov, Austin Sabharwal, Herrick, Avia, A. Efrat, Ayla Erdem, B. Karaka\\u00b8s, Ryan Roberts, B. S. Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan \\u00d6zyurt, Behnam Hedayatnia, Behnam, Benjamin Neyshabur, Benno Inden, Berk Stein, Ek-779 mekci, Bill Yuchen, Blake Lin, Cameron Howald, Cameron Diao, Catherine Dour, Cedrick Stinson, Ar-781 C\\u00e9sar, Chandan Ferri Ram\\u00edrez, Charles Singh, Christopher D. Manning, Christopher Potts, Cindy 785 Ramirez, Clara Rivera, Clemencia Siro, Colin Raf-786, Courtney Ashcraft, Cristina Garbacea, Dan Sileo, Daniel H Garrette, Dan Hendrycks, Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\\u00ed Gonz\\u00e1lez, Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, D. Gilboa, David Dohan, D. Drakard, David Ju-792, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Tam, m\\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-807 L\\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijase-808 vic, Hannah Kim, Harsh Mehta, H. Bogar, Henry Shevlin, Hinrich Sch\\u00fctze, H. Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jae-813 hoon Lee, J. Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Ko-815 co\\u00b4n, Jana Thompson, Jared Kaplan, Jarema Radom, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, K. Markert, Kaustubh D. Dhole, Kevin Gim-827 pel, Kevin Omondi, K. Mathewson, Kristen Chi-828 afullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-829 Donell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras Ochando, Louis-Philippe Morency, Luca Moschella, Maarten \\u00b8Senel, Maarten Bosma, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Jose Maru, Marie Ram\\u00edrez Quintana, Tolkiehn Mario, Martha Giulianelli, Martin Lewis, L. PotthastMatthew, Matthew L. Leavitt, M\\u00e1ty\\u00e1s Schu-840 bert Hagen, Medina Orduna, Melody Baitemirova, Arnaud Melvin, Michael A McElrath, Michael Yee, Michael Co-842 hen, Michael Gu, M. Ivanitskiy, Michael Star-843 ritt, M. Strube, Michele Sw\\u02dbedrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mo Suzgun, Monica Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T. Varma, Nanyun Peng, tish Shirish Keskar, Niveditha Iyer, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio, Moreno Casares, Parth Doshi, Jason Wei, Maarten Bosma, Vincent Y. Zhao, Adams Wei Guu, Brian Yu, Nan Lester, An-921 Du, M. Dai, Quoc V. Le, Finetuned, Adina Williams, Nikita Nangia, Samuel R. Bowman, Thomas Wolf, Lysandre Debut, Clement Chaumond, Anthony Delangue, Pier-339 Moi, Tim ric Cistac, R\\u00e9mi Rault, Morgan Louf, Funtow-900 Joe, Sam Davison, Patrick Shleifer, von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Sylvain Gugger, Mariama Drame, Yinfei Yang, Yuan Zhang, Chris Tar, Hailey Schoelkopf, Niklas Muen-954, Alham Fikri, David Ifeoluwa Adelani, M Saiful Bari, Lintang Sutawika, Zhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo, Jonathan May; Abstract: Adapters have been positioned as a parameter-001 efficient fine-tuning (PEFT) approach, whereby 002 a minimal number of parameters are added to 003 the model and fine-tuned. However, adapters 004 have not been sufficiently analyzed to under-005 stand if PEFT translates to benefits in train-006 ing/deployment efficiency and maintainabil-007 ity/extensibility. Through extensive experi-008 ments on many adapters, tasks, and languages 009 in supervised and cross-lingual zero-shot set-010 tings, we clearly show that for Natural Lan-011 guage Understanding tasks, the parameter ef-012 ficiency in adapters does not translate to effi-013 ciency gains compared to full fine-tuning of 014 models. More precisely, adapters are relatively 015 expensive to train and have slightly higher de-016 ployment latency. Furthermore, the maintain-017 ability /extensibility benefits of adapters can be 018 achieved with simpler approaches like multi-019 task training via full fine-tuning, which also 020 provide relatively faster training times. We, 021 therefore, recommend that for moderately sized 022 models practitioners should rely on full fine-023 tuning or multi-task training rather than using 024 adapters. 025; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is recommended that for moderately sized models practitioners should rely on full fine-023 tuning or multi-task training rather than using adapters, as adapters are relatively expensive to train and have slightly higher de-016 ployment latency.'} Author (LTI's Professor): Maarten Sap; Title: Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language; Authors: Jimin Mun, Emily Allaway, Akhila Yerukola, Laura Vianna, Sarah-Jane Leslie, Maarten Sap; Abstract: Counterspeech, i.e., responses to counteract potential harms of hateful speech, has become an increasingly popular solution to address online hate speech without censorship. However, properly countering hateful language requires countering and dispelling the underlying inaccurate stereotypes implied by such language. In this work, we draw from psychology and philosophy literature to craft six psychologically inspired strategies to challenge the underlying stereotypical implications of hateful language. We first examine the convincingness of each of these strategies through a user study, and then compare their usages in both human- and machine-generated counterspeech datasets. Our results show that human-written counterspeech uses countering strategies that are more specific to the implied stereotype (e.g., counter examples to the stereotype, external factors about the stereotype's origins), whereas machine-generated counterspeech uses less specific strategies (e.g., generally denouncing the hatefulness of speech). Furthermore, machine-generated counterspeech often employs strategies that humans deem less convincing compared to human-produced counterspeech. Our findings point to the importance of accounting for the underlying stereotypical implications of speech when generating counterspeech and for better machine reasoning about anti-stereotypical examples.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 2; TLDR: None Author (LTI's Professor): Maarten Sap; Title: BiasX: \\\"Thinking Slow\\\" in Toxic Content Moderation with Explanations of Implied Social Biases; Authors: Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap; Abstract: Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, is introduced and it is shown that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content.\\\"} Author (LTI's Professor): Maarten Sap; Title: Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory; Authors: Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi; Abstract: The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.; Year: 2023; Venue: arXiv.org; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs, and underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.'} Author (LTI's Professor): Maarten Sap; Title: Improving Language Models with Advantage-based Offline Policy Gradients; Authors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark O. Riedl; Abstract: Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data. We also release our experimental code. https://github.com/abaheti95/LoL-RL; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data that assumes the entire LM output sequence as a single action, and allows incorporating sequence-level classifiers or human-designed scoring functions as rewards.'} Author (LTI's Professor): Maarten Sap; Title: From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models; Authors: Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap; Abstract: Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word \\u201ccosmopolitan\\u201d in a sentence such as \\u201cwe need to end the cosmopolitan experiment\\u201d can mean \\u201cworldly\\u201d to many but also secretly mean \\u201cJewish\\u201d to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians\\u2019 speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3\\u2019s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources to facilitate future research in modeling dogwhistles and mitigating their online harms.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 7; TLDR: None Author (LTI's Professor): Maarten Sap; Title: NLPositionality: Characterizing Design Biases of Datasets and Models; Authors: Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap; Abstract: Design biases in NLP systems, such as performance differences for different populations, often stem from their creator\\u2019s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks\\u2014social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries.We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 17; TLDR: {'model': 'tldr@v2.0.0', 'text': 'NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models, is introduced and it is found that dataset and models align predominantly with Western, White, college-educated, and younger populations.'} Author (LTI's Professor): Maarten Sap; Title: Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting; Authors: Akhila Yerukola, Xuhui Zhou, Maarten Sap; Abstract: Most existing stylistic text rewriting methods and evaluation metrics operate on a sentence level, but ignoring the broader context of the text can lead to preferring generic, ambiguous, and incoherent rewrites. In this paper, we investigate integrating the preceding textual context into both the $\\\\textit{rewriting}$ and $\\\\textit{evaluation}$ stages of stylistic text rewriting, and introduce a new composite contextual evaluation metric $\\\\texttt{CtxSimFit}$ that combines similarity to the original sentence with contextual cohesiveness. We comparatively evaluate non-contextual and contextual rewrites in formality, toxicity, and sentiment transfer tasks. Our experiments show that humans significantly prefer contextual rewrites as more fitting and natural over non-contextual ones, yet existing sentence-level automatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences ($\\\\rho$=0--0.3). In contrast, human preferences are much better reflected by both our novel $\\\\texttt{CtxSimFit}$ ($\\\\rho$=0.7--0.9) as well as proposed context-infused versions of common metrics ($\\\\rho$=0.4--0.7). Overall, our findings highlight the importance of integrating context into the generation and especially the evaluation stages of stylistic text rewriting.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A new composite contextual evaluation metric is introduced that combines similarity to the original sentence with contextual cohesiveness and highlights the importance of integrating context into the generation and especially the evaluation stages of stylistic text rewriting.'} Author (LTI's Professor): Maarten Sap; Title: Where Do People Tell Stories Online? Story Detection Across Online Communities; Authors: Maria Antoniak, Joel Mire, Maarten Sap, Elliott Ash, Andrew Piper; Abstract: Story detection in online communities is a challenging task as stories are scattered across communities and interwoven with non-storytelling spans within a single text. We address this challenge by building and releasing the StorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts and comments, a detailed codebook adapted to the social media context, and models to predict storytelling at the document and span level. Our dataset is sampled from hundreds of popular English-language Reddit communities ranging across 33 topic categories, and it contains fine-grained expert annotations, including binary story labels, story spans, and event spans. We evaluate a range of detection methods using our data, and we identify the distinctive textual features of online storytelling, focusing on storytelling span detection, which we introduce as a new task. We illuminate distributional characteristics of storytelling on a large community-centric social media platform, and we also conduct a case study on r/ChangeMyView, where storytelling is used as one of many persuasive strategies, illustrating that our data and models can be used for both inter- and intra-community research. Finally, we discuss implications of our tools and analyses for narratology and the study of online communities.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The StorySeeker toolkit is built, including a richly annotated dataset of 502 Reddit posts and comments, a detailed codebook adapted to the social media context, and models to predict storytelling at the document and span level, to identify the distinctive textual features of online storytelling.'} Author (LTI's Professor): Maarten Sap; Title: Queer In AI: A Case Study in Community-Led Participatory AI; Authors: AI OrganizersOfQueerin, Anaelia Ovalle, Arjun Subramonian, Ashwin Singh, C. Voelcker, Danica J. Sutherland, Davide Locatelli, Eva Breznik, Filip Klubicka, Hang Yuan, J. Hetvi, Huan Zhang, Jaidev Shriram, Kruno Lehman, Luca Soldaini, Maarten Sap, M. Deisenroth, Maria Leonor Pacheco, Maria Ryskina, Martin Mundt, M. Agarwal, Nyx McLean, Pan Xu, Pranav A, Raj Korpan, Ruchira Ray, Sarah Mathew, Sarthak Arora, S. T. John, Tanvi Anand, Vishakha Agrawal, William Agnew, Yanan Long, Zijie J. Wang, Zeerak Talat, Avijit Ghosh, N. Dennler, Michael Noseworthy, Sharvani Jha, Emi Baylor, Aditya Joshi, Natalia Y. Bilenko, Andrew McNamara, Raphael Gontijo-Lopes, Alex Markham, Evyn D\\u01d2ng, J. Kay, Manu Saraswat, Nikhil Vytla, Luke Stark; Abstract: Queerness and queer people face an uncertain future in the face of ever more widely deployed and invasive artificial intelligence (AI). These technologies have caused numerous harms to queer people, including privacy violations, censoring and downranking queer content, exposing queer people and spaces to harassment by making them hypervisible, deadnaming and outing queer people. More broadly, they have violated core tenets of queerness by classifying and controlling queer identities. In response to this, the queer community in AI has organized Queer in AI, a global, decentralized, volunteer-run grassroots organization that employs intersectional and community-led participatory design to build an inclusive and equitable AI future. In this paper, we present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community\\u2019s programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization\\u2019s impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative participatory practices, and bringing participation to institutions outside of individual research projects. Queer in AI\\u2019s work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods.; Year: 2023; Venue: Conference on Fairness, Accountability and Transparency; Citations: 12; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper examines how participatory design and intersectional tenets started and shaped this community\\u2019s programs over the years, and discusses different challenges that emerged in the process, and looks at ways this organization has fallen short of operationalizing participatory and intersectionsal principles.'} Author (LTI's Professor): Maarten Sap; Title: Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties; Authors: Taylor Sorensen, Liwei Jiang, Jena D. Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, Maarten Sap, J. Tasioulas, Yejin Choi; Abstract: Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT-4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.; Year: 2023; Venue: arXiv.org; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Kaleido is built, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence of human values, rights, and duties within a specific context and demonstrates that Kaleido can help explain variability in human decision-making by outputting contrasting values.'} Author (LTI's Professor): Maarten Sap; Title: Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models; Authors: Natalie Shapira, Mosh Levy, S. Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz; Abstract: The escalating debate on AI's capabilities warrants developing reliable metrics to assess machine\\\"intelligence\\\". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.; Year: 2023; Venue: arXiv.org; Citations: 41; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is found that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust, indicating reliance on shallow heuristics rather than robust ToM abilities.'} Author (LTI's Professor): Maarten Sap; Title: Towards Countering Essentialism through Social Bias Reasoning; Authors: Emily Allaway, Nina Taneja, S. Leslie, Maarten Sap; Abstract: Essentialist beliefs (i.e., believing that members of the same group are fundamentally alike) play a central role in social stereotypes and can lead to harm when left unchallenged. In our work, we conduct exploratory studies into the task of countering essentialist beliefs (e.g., ``liberals are stupid''). Drawing on prior work from psychology and NLP, we construct five types of counterstatements and conduct human studies on the effectiveness of these different strategies. Our studies also investigate the role in choosing a counterstatement of the level of explicitness with which an essentialist belief is conveyed. We find that statements that broaden the scope of a stereotype (e.g., to other groups, as in ``conservatives can also be stupid'') are the most popular countering strategy. We conclude with a discussion of challenges and open questions for future work in this area (e.g., improving factuality, studying community-specific variation) and we emphasize the importance of work at the intersection of NLP and psychology.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: None Author (LTI's Professor): Maarten Sap; Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents; Authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap; Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.; Year: 2023; Venue: arXiv.org; Citations: 13; TLDR: {'model': 'tldr@v2.0.0', 'text': 'SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence, is presented and it is found that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.'} Author (LTI's Professor): Rita Singh; Title: BASS: Block-wise Adaptation for Speech Summarization; Authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj; Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.; Year: 2023; Venue: Interspeech; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'} Author (LTI's Professor): Rita Singh; Title: Rethinking Voice-Face Correlation: A Geometry View; Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, B. Raj; Abstract: Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.; Year: 2023; Venue: ACM Multimedia; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction and finds significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium.'} Author (LTI's Professor): Rita Singh; Title: A Gene-Based Algorithm for Identifying Factors That May Affect a Speaker\\u2019s Voice; Authors: Rita Singh; Abstract: Over the past decades, many machine-learning- and artificial-intelligence-based technologies have been created to deduce biometric or bio-relevant parameters of speakers from their voice. These voice profiling technologies have targeted a wide range of parameters, from diseases to environmental factors, based largely on the fact that they are known to influence voice. Recently, some have also explored the prediction of parameters whose influence on voice is not easily observable through data-opportunistic biomarker discovery techniques. However, given the enormous range of factors that can possibly influence voice, more informed methods for selecting those that may be potentially deducible from voice are needed. To this end, this paper proposes a simple path-finding algorithm that attempts to find links between vocal characteristics and perturbing factors using cytogenetic and genomic data. The links represent reasonable selection criteria for use by computational by profiling technologies only, and are not intended to establish any unknown biological facts. The proposed algorithm is validated using a simple example from medical literature\\u2014that of the clinically observed effects of specific chromosomal microdeletion syndromes on the vocal characteristics of affected people. In this example, the algorithm attempts to link the genes involved in these syndromes to a single example gene (FOXP2) that is known to play a broad role in voice production. We show that in cases where strong links are exposed, vocal characteristics of the patients are indeed reported to be correspondingly affected. Validation experiments and subsequent analyses confirm that the methodology could be potentially useful in predicting the existence of vocal signatures in na\\u00efve cases where their existence has not been otherwise observed.; Year: 2023; Venue: Entropy; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A simple path-finding algorithm that attempts to find links between vocal characteristics and perturbing factors using cytogenetic and genomic data and shows that in cases where strong links are exposed, vocal characteristics of the patients are indeed reported to be correspondingly affected.'} Author (LTI's Professor): Rita Singh; Title: Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation; Authors: Wayne Zhao, Rita Singh; Abstract: During phonation, the vocal folds exhibit a self-sustained oscillatory motion, which is influenced by the physical properties of the speaker\\u2019s vocal folds and driven by the balance of bio-mechanical and aerodynamic forces across the glottis. Subtle changes in the speaker\\u2019s physical state can affect voice production and alter these oscillatory patterns. Measuring these can be valuable in developing computational tools that analyze voice to infer the speaker\\u2019s state. Traditionally, vocal fold oscillations (VFOs) are measured directly using physical devices in clinical settings. In this paper, we propose a novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker-by-speaker basis. The approach, called the ADLES-VFT algorithm, is proposed in the context of a joint model that combines a phonation model (with a glottal flow waveform as the output) and a vocal tract acoustic wave propagation model such that the output of the joint model is an estimated waveform. The ADLES-VFT algorithm is a forward-backward algorithm which minimizes the error between the recorded waveform and the output of this joint model to estimate its parameters. Once estimated, these parameter values are used in conjunction with a phonation model to obtain its solutions. Since the parameters correlate with the physical properties of the vocal folds of the speaker, model solutions obtained using them represent the individualized VFOs for each speaker. The approach is flexible and can be applied to various phonation models. In addition to presenting the methodology, we show how the VFOs can be quantified from a dynamical systems perspective for classification purposes. Mathematical derivations are provided in an appendix for better readability.; Year: 2023; Venue: Entropy; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker- by-speaker basis is proposed and it is shown how the V FOs can be quantified from a dynamical systems perspective for classification purposes.'} Author (LTI's Professor): Rita Singh; Title: Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations; Authors: Hao Chen, Ankit Shah, Jindong Wang, R. Tao, Yidong Wang, Xingxu Xie, Masashi Sugiyama, Rita Singh, B. Raj; Abstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \\\\textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Imprecise label learning (ILL) is introduced, a framework for the unification of learning with various imprecise label configurations, marking the first unified framework with robust and effective performance across various challenging settings.'} Author (LTI's Professor): Rita Singh; Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features; Authors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj; Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.; Year: 2023; Venue: Interspeech; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes an analysis pipeline to help explore the voice-face relationship in a fine-grained manner, i.s. phonemes v. facial anthropometric measurements (AM), and indicates that AMs are more predictable from vowels compared to consonants, particularly with plosives.'} Author (LTI's Professor): Rita Singh; Title: Pengi: An Audio Language Model for Audio Tasks; Authors: Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang; Abstract: In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question&Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding; Year: 2023; Venue: Neural Information Processing Systems; Citations: 34; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Pengi is introduced, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks, and shows that connecting language models with audio models is a major step towards general-purpose audio understanding.'} Author (LTI's Professor): Emma Strubell; Title: To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing; Authors: Sireesh Gururaja, Amanda Bertsch, Clara Na, D. Widder, Emma Strubell; Abstract: NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as well as new shifts without historical parallel, including changes in benchmark culture and software infrastructure. We complement this discussion with quantitative analysis of citation, authorship, and language use in the ACL Anthology over time. We conclude by discussing shared visions, concerns, and hopes for the future of NLP. We hope that this study of our field's past and present can prompt informed discussion of our community's implicit norms and more deliberate action to consciously shape the future.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work conducts long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity to study factors that shape NLP as a field, including culture, incentives, and infrastructure.'} Author (LTI's Professor): Emma Strubell; Title: Energy and Carbon Considerations of Fine-Tuning BERT; Authors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni; Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities is performed to place fine- Tuning energy and carbon costs into perspective with respect to pre-training and inference.'} Author (LTI's Professor): Emma Strubell; Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models; Authors: Gustavo Gon\\u00e7alves, Emma Strubell; Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs finds that longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.'} Author (LTI's Professor): Emma Strubell; Title: Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research; Authors: Ji-Ung Lee, Haritz Puerto, Betty van Aken, Yuki Arase, J. Forde, Leon Derczynski, Andreas Ruckl'e, Iryna Gurevych, Roy Schwartz, Emma Strubell, Jesse Dodge; Abstract: Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found disparities, some of which already successfully implemented. Finally, we discuss additional concerns raised by many participants in free-text responses.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work captures existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process; and provides an analysis and devise recommendations to mitigate found disparities.'} Author (LTI's Professor): Emma Strubell; Title: On the Interactions of Structural Constraints and Data Resources for Structured Prediction; Authors: Zhisong Zhang, Emma Strubell, E. Hovy; Abstract: ,; Year: 2023; Venue: SUSTAINLP; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Emma Strubell; Title: Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation; Authors: Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Pete Walsh, Noah A. Smith, Hannaneh Hajishirzi; Abstract: Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"Pentathlon is a benchmark for holistic and realistic evaluation of model efficiency, which focuses on inference, which accounts for a majority of the compute in a model's lifecycle, and is designed to mirror real-world applications scenarios.\\\"} Author (LTI's Professor): Emma Strubell; Title: Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models; Authors: Harnoor Dhingra, Preetiha Jayashanker, Sayali S. Moghe, Emma Strubell; Abstract: Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.'} Author (LTI's Professor): Emma Strubell; Title: Power Hungry Processing: Watts Driving the Cost of AI Deployment?; Authors: A. Luccioni, Yacine Jernite, Emma Strubell; Abstract: Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of\\\"generality\\\"comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.; Year: 2023; Venue: arXiv.org; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is found that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters.'} Author (LTI's Professor): Emma Strubell; Title: Making Scalable Meta Learning Practical; Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing; Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'} Author (LTI's Professor): Emma Strubell; Title: Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints; Authors: Rajshekhar Das, Jonathan M Francis, Sanket Vaibhav Mehta, Jean Oh, Emma Strubell, Jose Moura; Abstract: Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for semantic segmentation problems. A notable drawback, however, is that this family of approaches is susceptible to erroneous pseudo labels that arise from confirmation biases in the source domain and that manifest as nuisance factors in the target domain. A possible source for this mismatch is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub-optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise conventional self-training objectives. Specifically, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal clustering. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for unsupervised domain adaptation. In this work, we show that our regularizer significantly improves top performing self-training methods (by up to $2$ points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The regularizer significantly improves top performing self-training methods in various UDA benchmarks for semantic segmentation and introduces a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart.'} Author (LTI's Professor): Emma Strubell; Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment; Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell; Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\\\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'} Author (LTI's Professor): Emma Strubell; Title: Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training; Authors: Zhisong Zhang, Emma Strubell, E. Hovy; Abstract: In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"This work proposes a pragmatic method that reduces the annotation cost for structured label spaces using active learning by adopting an error estimator to adaptively decide the partial selection ratio according to the current model's capability.\\\"} Author (LTI's Professor): Emma Strubell; Title: How To Train Your (Compressed) Large Language Model; Authors: A. Jha, Dirk Groeneveld, Emma Strubell, Iz Beltagy; Abstract: With the increase in the size of large language models (LLMs), we need compression methods that can reduce the model size while preserving the generality and zero-shot promptability of the model. This goal is more ambitious than the typical compression setup, which reduces the model's size at the expense of specializing it to a specific end-task. To study this, we develop a task-agnostic compression pipeline with a large-scale evaluation comprising language modeling perplexity and 12 zero-shot end-tasks. Our results show that a simple layer-wise pruning followed by continued language model pretraining matches or outperforms three existing state-of-the-art baselines while being 1.5x more computationally efficient. However, unlike typical task-specialized compression, our best-compressed model significantly underperforms a similar-sized model trained from scratch. We posit the half-sized pretrained model as an upper bound for task-agnostic compression and call for future work to bridge this gap under a reasonable token budget. Our findings highlight the inadequacy of existing compression methods for LLMs and establish a requirement for new methods that preserve a model's generality and zero-shot promptability under compression. We release our code and evaluation setup to facilitate reproducibility and help iterate on method design.; Year: 2023; Venue: ; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A task-agnostic compression pipeline with a large-scale evaluation comprising language modeling perplexity and 12 zero-shot end-tasks is developed and shows that a simple layer-wise pruning followed by continued language model pretraining matches or outperforms three existing state-of-the-art baselines while being 1.5x more computationally efficient.'} Author (LTI's Professor): Alexander Waibel; Title: SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization; Authors: Tuan-Nam Nguyen, Ngoc-Quan Pham, A. Waibel; Abstract: Conventional multi-speaker text-to-speech synthesis (TTS) is known to be capable of synthesizing speech for multiple voices, yet it cannot generate speech in different accents. This limitation has motivated us to develop SYNTACC (Synthesizing speech with accents) which adapts conventional multi-speaker TTS to produce multi-accent speech. Our method uses the YourTTS model and involves a novel multi-accent training mechanism. The method works by decomposing each weight matrix into a shared component and an accent-dependent component, with the former being initialized by the pretrained multi-speaker TTS model and the latter being factorized into vectors using rank-1 matrices to reduce the number of training parameters per accent. This weight factorization method proves to be effective in fine-tuning the SYNTACC on multi-accent data sets in a low-resource condition. Our SYNTACC model eventually allows speech synthesis in not only different voices but also in different accents.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The SYNTACC model eventually allows speech synthesis in not only different voices but also in different accents, and the weight factorization method proves to be effective in fine-tuning the SYNT ACC on multi-accent data sets in a low-resource condition.'} Author (LTI's Professor): Alexander Waibel; Title: KIT\\u2019s Multilingual Speech Translation System for IWSLT 2023; Authors: Danni Liu, T. Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, A. Waibel, J. Niehues; Abstract: Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk translation, although their performance remains similar on TED talks.; Year: 2023; Venue: International Workshop on Spoken Language Translation; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper describes the speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks, and observes that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules.'} Author (LTI's Professor): Alexander Waibel; Title: Convoifilter: A case study of doing cocktail party speech recognition; Authors: T. Nguyen, A. Waibel; Abstract: This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise (ConVoiFilter) and an ASR module. The model can decrease ASR's word error rate (WER) from 80% to 26.4% through this approach. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning. We openly share our pre-trained model to foster further research hf.co/nguyenvulebinh/voice-filter.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"An end-to-end model designed to improve automatic speech recognition for a particular speaker in a crowded, noisy environment that utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise and an ASR module.\\\"} Author (LTI's Professor): Alexander Waibel; Title: Continually learning new languages; Authors: Ngoc-Quan Pham, J. Niehues, A. Waibel; Abstract: Multilingual speech recognition with neural networks is often implemented with batch-learning, when all of the languages are available before training. An ability to add new languages after the prior training sessions can be economically bene-\\ufb01cial, but the main challenge is catastrophic forgetting. In this work, we combine the qualities of weight factorization, transfer learning and Elastic Weight Consolidation in order to counter catastrophic forgetting and facilitate learning new languages quickly. Such combination allowed us to eliminate catastrophic forgetting while still achieving performance for the new languages comparable with having all languages at once, in experiments of learning from an initial 10 languages to achieve 27 languages.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work combines the qualities of weight factorization, transfer learning and Elastic Weight Consolidation in order to counter catastrophic forgetting and facilitate learning new languages quickly.'} Author (LTI's Professor): Alexander Waibel; Title: Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff; Authors: Peter Pol\\u00e1k, Brian Yan, Shinji Watanabe, A. Waibel, Ondrej Bojar; Abstract: Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \\\\textit{incremental} translation to users. Further, this method lacks mechanisms for \\\\textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.; Year: 2023; Venue: Interspeech; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control is proposed and applied to models trained for online or offline translation and it is demonstrated that both types can be effectively used in online mode.'} Author (LTI's Professor): Alexander Waibel; Title: End-to-End Evaluation for Low-Latency Simultaneous Speech Translation; Authors: Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc-Quan Pham, T. Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, J. Niehues, A. Waibel; Abstract: The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components. Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions and directly compares state-of-the-art cascaded as well as end-to-end systems.'} Author (LTI's Professor): Alexander Waibel; Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN; Authors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, R. Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Est\\u00e8ve, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, D\\u00e1vid Javorsk\\u00fd, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Pol\\u00e1k, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian St\\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, Marco Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos; Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.; Year: 2023; Venue: International Workshop on Spoken Language Translation; Citations: 26; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Shinji Watanabe; Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval; Authors: Sho Miyamoto, Y. Kuroda, T. Kanno, A. Ueno, N. Shiwa-Sudo, N. Iwata-Yoshikawa, Yusuke Sakai, N. Nagata, T. Arashiro, A. Ainai, Saya Moriyama, N. Kishida, Shinji Watanabe, K. Nojima, Y. Seki, T. Mizukami, H. Hasegawa, H. Ebihara, S. Fukushi, Yoshimasa Takahashi, Maeda Ken, Tadaki Suzuki; Abstract: None; Year: 2023; Venue: iScience; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The results highlight the importance of vaccine dosage intervals of 4 months or longer, regardless of the antigenicity of the exposed antigen, to maximize the breadth of serum cross-neutralization covering SARS-CoV-2 Omicron lineages.'} Author (LTI's Professor): Shinji Watanabe; Title: Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning; Authors: William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe; Abstract: Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM\\u2019s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than $10 \\\\%$ of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain $94 \\\\%$ of XLS-R\\u2019s performance with only $3 \\\\%$ of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet.; Year: 2023; Venue: Automatic Speech Recognition & Understanding; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes WavLabLM, which extends WavLM\\u2019s joint prediction and denoising to 40k hours of data across 136 languages, and devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data.'} Author (LTI's Professor): Shinji Watanabe; Title: Tensor decomposition for minimization of E2E SLU model toward on-device processing; Authors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe; Abstract: Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and aims to minimize the computational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in our E2E SLU models. We propose to apply singular value decomposition to linear layers and the Tucker decomposition to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposition to the Tucker decomposition. Since the E2E model is represented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 million parameters.; Year: 2023; Venue: Interspeech; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper aims to minimize the computational cost of the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and reduces the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in the E2E SLU models.'} Author (LTI's Professor): Shinji Watanabe; Title: Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation; Authors: E. Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe; Abstract: Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes using a decoder-only architecture for ASR with simple text augmentation training that had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.'} Author (LTI's Professor): Shinji Watanabe; Title: ML-SUPERB: Multilingual Speech Universal PERformance Benchmark; Authors: Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Huang, Xuankai Chang, Shang-Wen Li, Abdel-rahman Mohamed, Hung-yi Lee, Shinji Watanabe; Abstract: Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.; Year: 2023; Venue: Interspeech; Citations: 17; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Shinji Watanabe; Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding; Authors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe; Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 11; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes three task-specific structured pruning methods to deal with heterogeneous speech models that not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning.'} Author (LTI's Professor): Shinji Watanabe; Title: Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization; Authors: Puyuan Peng, Brian Yan, Shinji Watanabe, David F. Harwath; Abstract: We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper; Year: 2023; Venue: Interspeech; Citations: 17; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work investigates the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering, and designs task-specific prompts that improve performance on the three zero-shot tasks and even outperform SotA supervised models on some datasets.'} Author (LTI's Professor): Shinji Watanabe; Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks; Authors: Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe; Abstract: Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.; Year: 2023; Venue: Interspeech; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training.'} Author (LTI's Professor): Shinji Watanabe; Title: Efficient Sequence Transduction by Jointly Predicting Tokens and Durations; Authors: Hainan Xu, Fei Jia, Somshubra Majumdar, Hengguan Huang, Shinji Watanabe, Boris Ginsburg; Abstract: This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.; Year: 2023; Venue: International Conference on Machine Learning; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel Token-and-Duration Transducer architecture for sequence-to-sequence tasks by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token.'} Author (LTI's Professor): Shinji Watanabe; Title: Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders; Authors: Takatomo Kano, A. Ogawa, Marc Delcroix, Roshan Sharma, Kohei Matsuura, Shinji Watanabe; Abstract: Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube), and exploits auxiliary text information from ASR transcriptions to improve the modeling capabilities.'} Author (LTI's Professor): Shinji Watanabe; Title: UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures; Authors: Zhong-Qiu Wang, Shinji Watanabe; Abstract: In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR, an algorithm for over-determined training mixtures that can promote unsupervised separation of speakers.'} Author (LTI's Professor): Shinji Watanabe; Title: Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference; Authors: Masao Someki, N. Eng, Yosuke Higuchi, Shinji Watanabe; Abstract: Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothesis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Experimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy.; Year: 2023; Venue: Automatic Speech Recognition & Understanding; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture, which is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy.'} Author (LTI's Professor): Shinji Watanabe; Title: Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study; Authors: Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, W. El-Hajj, Ahmed Ali; Abstract: .; Year: 2023; Venue: arXiv.org; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Shinji Watanabe; Title: BASS: Block-wise Adaptation for Speech Summarization; Authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj; Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.; Year: 2023; Venue: Interspeech; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'} Author (LTI's Professor): Shinji Watanabe; Title: I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition; Authors: Yifan Peng, Jaesong Lee, Shinji Watanabe; Abstract: Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 8; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs and interesting analysis on the gate probabilities and the input-dependency, which helps to better understand deep encoders.'} Author (LTI's Professor): Shinji Watanabe; Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech; Authors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky; Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.; Year: 2023; Venue: AAAI Conference on Artificial Intelligence; Citations: 17; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces the MQTTS system, a Text-to-Speech system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality, and shows that MqTTS outperforms existing TTS systems in several objective and subjective measures.'} Author (LTI's Professor): Shinji Watanabe; Title: A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning; Authors: Jiyang Tang, William Chen, Xuankai Chang, Shinji Watanabe, B. MacWhinney; Abstract: Aphasia is a language disorder that affects the speaking ability of millions of patients. This paper presents a new benchmark for Aphasia speech recognition and detection tasks using state-of-the-art speech recognition techniques with the AphsiaBank dataset. Specifically, we introduce two multi-task learning methods based on the CTC/Attention architecture to perform both tasks simultaneously. Our system achieves state-of-the-art speaker-level detection accuracy (97.3%), and a relative WER reduction of 11% for moderate Aphasia patients. In addition, we demonstrate the generalizability of our approach by applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.; Year: 2023; Venue: Interspeech; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A new benchmark for Aphasia speech recognition and detection tasks using state-of-the-art speech recognition techniques with the AphsiaBank dataset is presented and two multi-task learning methods based on the CTC/Attention architecture are introduced to perform both tasks simultaneously.'} Author (LTI's Professor): Shinji Watanabe; Title: Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses; Authors: E. Takashita, S. Murakami, Y. Matsuzaki, Seiichiro Fujisaki, H. Morita, Shiho Nagata, Misa Katayama, K. Mizuta, H. Nishimura, Shinji Watanabe, T. Horimoto, H. Hasegawa; Abstract: The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.; Year: 2023; Venue: Viruses; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses.'} Author (LTI's Professor): Shinji Watanabe; Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement; Authors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj; Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \\u2013 such as spectral tilt, spectral flux, shimmer, etc. \\u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance.'} Author (LTI's Professor): Shinji Watanabe; Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing; Authors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin; Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.; Year: 2023; Venue: Special Interest Group on Computational Morphology and Phonology Workshop; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'In this submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), approaches to data augmentation and modeling across seven low-resource languages are explored and token classification models are found to be the best performing.'} Author (LTI's Professor): Shinji Watanabe; Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation; Authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhongqiu Wang, Nobutaka Ono, Y. Qian, Shinji Watanabe; Abstract: Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).; Year: 2023; Venue: IEEE Workshop on Applications of Signal Processing to Audio and Acoustics; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end, and employs the recent self-supervised learning representation (SSLR) as a feature and improves the recognition performance from the case with filterbank features.'} Author (LTI's Professor): Shinji Watanabe; Title: Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech; Authors: Chien-yu Huang, Ke-Han Lu, Shi Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee; Abstract: Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion, and invites the community to collaborate and contribute, facilitating the dynamic growth of the benchmark.'} Author (LTI's Professor): Shinji Watanabe; Title: Speaker-Independent Acoustic-to-Articulatory Speech Inversion; Authors: Peter Wu, Li-Wei Chen, Cheol Jun Cho, Shinji Watanabe, L. Goldstein, A. Black, G. Anumanchipalli; Abstract: To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work builds an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers, and proposes a resynthesis-based AAI evaluation metric that does not rely on articulatory labels.'} Author (LTI's Professor): Shinji Watanabe; Title: Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens; Authors: Minsu Kim, J. Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, Y. Ro; Abstract: In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper starts with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp, and sets the output of the proposed Im2 Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model.'} Author (LTI's Professor): Shinji Watanabe; Title: Enhancing Speech-To-Speech Translation with Multiple TTS Targets; Authors: Jiatong Shi, Yun Tang, Ann Lee, H. Inaguma, Changhan Wang, J. Pino, Shinji Watanabe; Abstract: It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually utilize text-to-speech (TTS) systems to generate samples in the target language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the synthesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for direct S2ST models. We find that simply combining the target speech from different TTS systems can potentially improve the S2ST performances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is found that simply combining the target speech from different TTS systems can potentially improve the S2ST performances, and a multi-task framework is proposed that jointly optimizes the S1ST system with multiple targets from differentTTS systems.'} Author (LTI's Professor): Shinji Watanabe; Title: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head; Authors: Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jia-Bin Huang, Jinglin Liu, Yixiang Ren, Zhou Zhao, Shinji Watanabe; Abstract: Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \\\\url{https://github.com/AIGC-Audio/AudioGPT}.; Year: 2023; Venue: arXiv.org; Citations: 83; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A multi-modal AI system named AudioGPT is proposed, which complements LLMs with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.'} Author (LTI's Professor): Shinji Watanabe; Title: Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation; Authors: Shih-Lun Wu, Xuankai Chang, G. Wichern, Jee-weon Jung, Franccois G. Germain, Jonathan Le Roux, Shinji Watanabe; Abstract: Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATs to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work utilizes BEATs to extract fine-grained audio features and proposes a novel data augmentation method that uses ChatGPT to produce caption mix-ups which increase not only the amount but also the complexity and diversity of training data.'} Author (LTI's Professor): Shinji Watanabe; Title: Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks; Authors: Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, Shinji Watanabe; Abstract: We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.; Year: 2023; Venue: arXiv.org; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A decoder-only language model that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation, VoxtLM is proposed, which exhibits a significant improvement in speech synthesis and improves speech intelligibility and objective quality.'} Author (LTI's Professor): Shinji Watanabe; Title: Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining; Authors: Takaaki Saeki, Soumi Maiti, Xinjian Li, Shinji Watanabe, Shinnosuke Takamichi, H. Saruwatari; Abstract: While neural text-to-speech (TTS) has achieved human-like natural synthetic speech, multilingual TTS systems are limited to resource-rich languages due to the need for paired text and studio-quality audio data. This paper proposes a method for zero-shot multilingual TTS using text-only data for the target language. The use of text-only data allows the development of TTS systems for low-resource languages for which only textual resources are available, making TTS accessible to thousands of languages. Inspired by the strong cross-lingual transferability of multilingual language models, our framework first performs masked language model pretraining with multilingual text-only data. Then we train this model with a paired data in a supervised manner, while freezing a language-aware embedding layer. This allows inference even for languages not included in the paired data but present in the text-only data. Evaluation results demonstrate highly intelligible zero-shot TTS with a character error rate of less than 12% for an unseen language.; Year: 2023; Venue: International Joint Conference on Artificial Intelligence; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Inspired by the strong cross-lingual transferability of multilingual language models, this framework first performs masked language model pretraining with multilingual text-only data, and trains this model with a paired data in a supervised manner, while freezing a language-aware embedding layer.'} Author (LTI's Professor): Shinji Watanabe; Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling; Authors: Zhongqiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeonghak Kim, Shinji Watanabe; Abstract: We propose FSB-LSTM, a novel long short-term memory (LSTM) based architecture that integrates full- and sub-band (FSB) modeling, for single- and multi-channel speech enhancement in the short-time Fourier transform (STFT) domain. The model maintains an information highway to flow an over-complete input representation through multiple FSB-LSTM modules. Each FSB-LSTM module consists of a full-band block to model spectro-temporal patterns at all frequencies and a sub-band block to model patterns within each sub-band, where each of the two blocks takes a down-sampled representation as input and returns an up-sampled discriminative representation to be added to the block input via a residual connection. The model is designed to have a low algorithmic complexity, a small run-time buffer and a very low algorithmic latency, at the same time producing a strong enhancement performance on a noisy-reverberant speech enhancement task even if the hop size is as low as 2 ms.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The proposed FSB-LSTM model is designed to have a low algorithmic complexity, a small run-time buffer and a very lowgorithmic latency, at the same time producing a strong enhancement performance on a noisy-reverberant speech enhancement task even if the hop size is as low as 2 ms.'} Author (LTI's Professor): Shinji Watanabe; Title: Improving Massively Multilingual ASR with Auxiliary CTC Objectives; Authors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe; Abstract: Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 18; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID), and investigates techniques inspired from recent Connectionist Temporal Classification studies to help the model handle the large number of languages.'} Author (LTI's Professor): Shinji Watanabe; Title: Toward Universal Speech Enhancement For Diverse Input Conditions; Authors: Wangyou Zhang, Kohei Saijo, Zhong-Qiu Wang, Shinji Watanabe, Yanmin Qian; Abstract: The past decade has witnessed substantial growth of data-driven speech enhancement (SE) techniques thanks to deep learning. While existing approaches have shown impressive performance in some common datasets, most of them are designed only for a single condition (e.g., single-channel, multi-channel, or a fixed sampling frequency) or only consider a single task (e.g., denoising or dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.; Year: 2023; Venue: Automatic Speech Recognition & Understanding; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies, and designs a universal SE benchmark by combining existing public corpora with multiple conditions.'} Author (LTI's Professor): Shinji Watanabe; Title: The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge; Authors: Hayato Futami, Jessica Huynh, Siddhant Arora, Shih-Lun Wu, Yosuke Kashiwagi, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe; Abstract: This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023, adopts a pipeline approach of ASR and NLU and applies masked LM (MLM) -based data augmentation.'} Author (LTI's Professor): Shinji Watanabe; Title: A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge; Authors: Siddhant Arora, Hayato Futami, Shih-Lun Wu, Jessica Huynh, Yifan Peng, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe; Abstract: Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper describes the proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023.'} Author (LTI's Professor): Shinji Watanabe; Title: A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023; Authors: E. Takashita, Seiichiro Fujisaki, H. Morita, Shiho Nagata, H. Miura, Yuki Matsuura, Saya Yamamoto, Shoko Chiba, Yumiko Inoue, Iori Minami, Sayaka Yoshikawa, Seiko Yamazaki, N. Kishida, Kazuya Nakamura, Masayuki Shirakura, Shinji Watanabe, Hideki Hasegawa; Abstract: A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.; Year: 2023; Venue: Euro surveillance : bulletin Europeen sur les maladies transmissibles = European communicable disease bulletin; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023 and showed reduced susceptibility to baloxavir in vitro; however, the clinical significance remains unclear.'} Author (LTI's Professor): Shinji Watanabe; Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study; Authors: Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang; Abstract: Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.; Year: 2023; Venue: arXiv.org; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This study undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models, demonstrating that discrete units achieve reasonably good results in almost all the settings.'} Author (LTI's Professor): Shinji Watanabe; Title: Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History; Authors: Siddhant Arora, Hayato Futami, E. Tsunoo, Brian Yan, Shinji Watanabe; Abstract: Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel model architecture is proposed that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance and achieves similar results to task-specific classifiers and can effectively integrateDialog context to further improve the SLU performance.'} Author (LTI's Professor): Shinji Watanabe; Title: Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization; Authors: A. Hussein, Brian Yan, Antonios Anastasopoulos, Shinji Watanabe, S. Khudanpur; Abstract: Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments, and proposes context dropout to ensure robustness to the absence of context, and improves performance by adding speaker information.'} Author (LTI's Professor): Shinji Watanabe; Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter; Authors: Shinji Watanabe; Abstract: None; Year: 2023; Venue: Oleoscience; Citations: 0; TLDR: None Author (LTI's Professor): Shinji Watanabe; Title: The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction; Authors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao; Abstract: Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.; Year: 2023; Venue: arXiv.org; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge is delivered, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments.'} Author (LTI's Professor): Shinji Watanabe; Title: Intrusion of Coastal Oyashio water to Funka Bay and Tsugaru Strait occasionally disturbed by Kuroshio-originating warm core ring; Authors: H. Abe, Y. Yahiro, T. Hasegawa, T. Hirawake, H. Onishi, A. Ooki, T. Takatsu, K. Sasaki, M. Wakita, H. Kaneko, Shinji Watanabe, T. Tanaka, T. Okunishi, S. Ohno, S. Hashizume; Abstract: None; Year: 2023; Venue: Journal of Oceanography; Citations: 0; TLDR: None Author (LTI's Professor): Shinji Watanabe; Title: Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data; Authors: Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, Shinji Watanabe; Abstract: Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet; Year: 2023; Venue: Automatic Speech Recognition & Understanding; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data and even supports more translation directions and can be more efficient to train.'} Author (LTI's Professor): Shinji Watanabe; Title: Visual Speech Recognition for Languages with Limited Labeled Data using Automatic Labels from Whisper; Authors: Jeong Hun Yeo, Minsu Kim, Shinji Watanabe, Y. Ro; Abstract: This paper proposes a powerful Visual Speech Recognition (VSR) method for multiple languages, especially for low-resource languages that have a limited number of labeled data. Different from previous methods that tried to improve the VSR performance for the target language by using knowledge learned from other languages, we explore whether we can increase the amount of training data itself for the different languages without human intervention. To this end, we employ a Whisper model which can conduct both language identification and audio-based speech recognition. It serves to filter data of the desired languages and transcribe labels from the unannotated, multilingual audio-visual data pool. By comparing the performances of VSR models trained on automatic labels and the human-annotated labels, we show that we can achieve similar VSR performance to that of human-annotated labels even without utilizing human annotations. Through the automated labeling process, we label large-scale unlabeled multilingual databases, VoxCeleb2 and AVSpeech, producing 1,002 hours of data for four low VSR resource languages, French, Italian, Spanish, and Portuguese. With the automatic labels, we achieve new state-of-the-art performance on mTEDx in four languages, significantly surpassing the previous methods. The automatic labels are available online: https://github.com/JeongHun0716/Visual-Speech-Recognition-for-Low-Resource-Languages; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'It is shown that VSR performance can be similar to that of human-annotated labels even without utilizing human annotations, and new state-of-the-art performance on mTEDx in four languages is achieved, significantly surpassing the previous methods.'} Author (LTI's Professor): Shinji Watanabe; Title: The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios; Authors: Samuele Cornell, Matthew Wiesner, Shinji Watanabe, Desh Raj, Xuankai Chang, Paola Garc\\u00eda, Yoshiki Masuyama, Zhong-Qiu Wang, S. Squartini, S. Khudanpur; Abstract: The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).; Year: 2023; Venue: 7th International Workshop on Speech Processing in Everyday Environments (CHiME 2023); Citations: 19; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The ChiME-7 distant ASR (DASR) task, within the 7th CHiME challenge, is introduced and the baseline system is presented, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).'} Author (LTI's Professor): Shinji Watanabe; Title: Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing; Authors: Brian Yan, Xuankai Chang, Antonios Anastasopoulos, Yuya Fujita, Shinji Watanabe; Abstract: Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally and reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length.'} Author (LTI's Professor): Shinji Watanabe; Title: ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit; Authors: Brian Yan, Jiatong Shi, Yun Tang, H. Inaguma, Yifan Peng, Siddharth Dalmia, Peter Pol'ak, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, J. Pino, Shinji Watanabe; Abstract: ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \\u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.; Year: 2023; Venue: Annual Meeting of the Association for Computational Linguistics; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2 are described, which is publicly available at https://github.com/espnet/esp net.'} Author (LTI's Professor): Shinji Watanabe; Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge; Authors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono; Abstract: This paper describes our submission to the Second Clarity Enhancement Challenge (CEC2), which consists of target speech enhancement for hearing-aid (HA) devices in noisy-reverberant environments with multiple interferers such as music and competing speakers. Our approach builds upon the powerful iterative neural/beamforming enhancement (iNeuBe) framework introduced in our recent work, and this paper extends it for target speaker extraction. We therefore name the proposed approach as iNeuBe-X, where the X stands for extraction. To address the challenges encountered in the CEC2 setting, we introduce four major novelties: (1) we extend the state-of-the-art TF-GridNet model, originally designed for monaural speaker separation, for multi-channel, causal speech enhancement, and large improvements are observed by replacing the TCNDenseNet used in iNeuBe with this new architecture; (2) we leverage a recent dual window size approach with future-frame prediction to ensure that iNueBe-X satisfies the 5 ms constraint on algorithmic latency required by CEC2; (3) we introduce a novel speaker-conditioning branch for TF-GridNet to achieve target speaker extraction; (4) we propose a fine-tuning step, where we compute an additional loss with respect to the target speaker signal compensated with the listener audiogram. Without using external data, on the official development set our best model reaches a hearing-aid speech perception index (HASPI) score of 0.942 and a scale-invariant signal-to-distortion ratio improvement (SI-SDRi) of 18.8 dB. These results are promising given the fact that the CEC2 data is extremely challenging (e.g., on the development set the mixture SI-SDR is -12.3 dB). A demo of our submitted system is available at WAVLab CEC2 demo.; Year: 2023; Venue: arXiv.org; Citations: 5; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The approach builds upon the powerful iterative neural/beamforming enhancement (iNeuBe) framework introduced in recent work, and this paper extends it for target speaker extraction, and is named as iNeu be-X, where the X stands for extraction.'} Author (LTI's Professor): Shinji Watanabe; Title: Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model; Authors: Takashi Maekaku, Yuya Fujita, Xuankai Chang, Shinji Watanabe; Abstract: Unsupervised topic clustering of spoken audio is an important research topic for zero-resourced unwritten languages. A classical approach is to find a set of spoken terms from only the audio based on dynamic time warping or generative modeling (e.g., hidden Markov model), and apply a topic model to classify topics. The spoken term discovery is the most important and difficult part. In this paper, we propose to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models. Most SSRL methods pre-train a model which predicts high-quality pseudo labels generated from an audio-only corpus. These pseudo labels can be used to produce a sequence of pseudo subwords by applying deduplication and a subword model. Then, we apply a topic model based on latent Dirichlet allocation for these pseudo-subword sequences in an unsupervised manner. The clustering performance is evaluated on the Fisher corpus using normalized mutual information. We confirm the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models although the experimental setups are not directly comparable.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models and confirms the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models.'} Author (LTI's Professor): Shinji Watanabe; Title: TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement; Authors: YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj; Abstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work shows that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility and uses data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from the method.'} Author (LTI's Professor): Shinji Watanabe; Title: End-to-End Speech Recognition: A Survey; Authors: Rohit Prabhavalkar, Takaaki Hori, Tara N. Sainath, R. Schluter, Shinji Watanabe; Abstract: In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.; Year: 2023; Venue: IEEE/ACM Transactions on Audio Speech and Language Processing; Citations: 38; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A taxonomy of E2E ASR models and corresponding improvements is provided, and their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures are discussed.'} Author (LTI's Professor): Shinji Watanabe; Title: The Multimodal Information Based Speech Processing (Misp) 2022 Challenge: Audio-Visual Diarization And Recognition; Authors: Zhe Wang, Shilong Wu, Hang Chen, Maokui He, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu; Abstract: The Multi-modal Information based Speech Processing (MISP) challenge aims to extend the application of signal processing technology in specific scenarios by promoting the research into wake-up words, speaker diarization, speech recognition, and other technologies. The MISP2022 challenge has two tracks: 1) audio-visual speaker diarization (AVSD), aiming to solve \\\"who spoken when\\\" using both audio and visual data; 2) a novel audio-visual diarization and recognition (AVDR) task that focuses on addressing \\\"who spoken what when\\\" with audio-visual speaker diarization results. Both tracks focus on the Chinese language, and use far-field audio and video in real home-tv scenarios: 2-6 people communicating each other with TV noise in the background. This paper introduces the dataset, track settings, and baselines of the MISP2022 challenge. Our analyses of experiments and examples indicate the good performance of AVDR baseline system, and the potential difficulties in this challenge due to, e.g., the far-field video quality, the presence of TV noise in the background, and the indistinguishable speakers.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 10; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The dataset, track settings, and baselines of the MISP2022 challenge are introduced, and analyses of experiments and examples indicate the good performance of AVDR baseline system, and the potential difficulties in this challenge due to, e.g., the far-field video quality, the presence of TV noise in the background, andThe indistinguishable speakers.'} Author (LTI's Professor): Shinji Watanabe; Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models; Authors: Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe; Abstract: Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.; Year: 2023; Venue: Interspeech; Citations: 12; TLDR: {'model': 'tldr@v2.0.0', 'text': 'DPHuBERT is proposed, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning that requires little training time and performs well with limited training data, making it suitable for resource-constrained applications.'} Author (LTI's Professor): Shinji Watanabe; Title: An external quality assessment feasibility study; cross laboratory comparison of haemagglutination inhibition assay and microneutralization assay performance for seasonal influenza serology testing: A FLUCOP study; Authors: J. Waldock, C. Weiss, Wei Wang, M. Levine, Stacie N. Jefferson, S. Ho, K. Hoschler, B. Londt, E. Masat, Louise A. Carolan, Stephany S\\u00e1nchez-Ovando, A. Fox, Shinji Watanabe, Miki Akimoto, Aya Sato, N. Kishida, A. Buys, Lorens Maake, Cardia Fourie, Catherine Caillet, Sandrine Raynaud, R. Webby, J. Debeauchamp, R. Cox, Sarah Lartey, C. Trombetta, S. Marchi, E. Montomoli, I. Sanz-Mu\\u00f1oz, J. Eiros, Javier S\\u00e1nchez-Mart\\u00ednez, D. Duijsings, O. Engelhardt; Abstract: Introduction External Quality Assessment (EQA) schemes are designed to provide a snapshot of laboratory proficiency, identifying issues and providing feedback to improve laboratory performance and inter-laboratory agreement in testing. Currently there are no international EQA schemes for seasonal influenza serology testing. Here we present a feasibility study for conducting an EQA scheme for influenza serology methods. Methods We invited participant laboratories from industry, contract research organizations (CROs), academia and public health institutions who regularly conduct hemagglutination inhibition (HAI) and microneutralization (MN) assays and have an interest in serology standardization. In total 16 laboratories returned data including 19 data sets for HAI assays and 9 data sets for MN assays. Results Within run analysis demonstrated good laboratory performance for HAI, with intrinsically higher levels of intra-assay variation for MN assays. Between run analysis showed laboratory and strain specific issues, particularly with B strains for HAI, whilst MN testing was consistently good across labs and strains. Inter-laboratory variability was higher for MN assays than HAI, however both assays showed a significant reduction in inter-laboratory variation when a human sera pool is used as a standard for normalization. Discussion This study has received positive feedback from participants, highlighting the benefit such an EQA scheme would have on improving laboratory performance, reducing inter laboratory variation and raising awareness of both harmonized protocol use and the benefit of biological standards for seasonal influenza serology testing.; Year: 2023; Venue: Frontiers in Immunology; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A feasibility study for conducting an EQA scheme for influenza serology methods showing good laboratory performance for HAI, with intrinsically higher levels of intra-assay variation for MN assays, and a significant reduction in inter-laboratory variation when a human sera pool is used as a standard for normalization.'} Author (LTI's Professor): Shinji Watanabe; Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge; Authors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini; Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.; Year: 2023; Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work details the submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments, and builds on the previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X.'} Author (LTI's Professor): Shinji Watanabe; Title: AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models; Authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David F. Harwath, Yu Tsao, Shinji Watanabe, Abdel-rahman Mohamed, Chi-Luen Feng, Hung-yi Lee; Abstract: Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned representations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task. We release our benchmark with evaluation code and a model submission platform to encourage further research in audio-visual learning.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The AV-SUPERB benchmark is proposed that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing and shows that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task.'} Author (LTI's Professor): Shinji Watanabe; Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation; Authors: Yu-Kuan Fu, Liang-Hsuan Tseng, Jiatong Shi, Chen-An Li, Tsung-Yuan Hsu, Shinji Watanabe, Hung-yi Lee; Abstract: Most of the speech translation models heavily rely on parallel data, which is hard to collect especially for low-resource languages. To tackle this issue, we propose to build a cascaded speech translation system without leveraging any kind of paired data. We use fully unpaired data to train our unsupervised systems and evaluate our results on CoVoST 2 and CVSS. The results show that our work is comparable with some other early supervised methods in some language pairs. While cascaded systems always suffer from severe error propagation problems, we proposed denoising back-translation (DBT), a novel approach to building robust unsupervised neural machine translation (UNMT). DBT successfully increases the BLEU score by 0.7--0.9 in all three translation directions. Moreover, we simplified the pipeline of our cascaded system to reduce inference latency and conducted a comprehensive analysis of every part of our work. We also demonstrate our unsupervised speech translation results on the established website.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work proposed denoising back-translation (DBT), a novel approach to building robust unsupervised neural machine translation (UNMT), which successfully increases the BLEU score by 0.7--0.9 in all three translation directions.'} Author (LTI's Professor): Shinji Watanabe; Title: Deep Speech Synthesis from MRI-Based Articulatory Representations; Authors: Peter Wu, Tingle Li, Yijingxiu Lu, Yubin Zhang, Jiachen Lian, A. Black, L. Goldstein, Shinji Watanabe, G. Anumanchipalli; Abstract: In this paper, we study articulatory synthesis, a speech synthesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable synthesizers. While recent advances have enabled intelligible articulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excitation and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to enhance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Finally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and identify the most suitable MRI feature subset for articulatory synthesis.; Year: 2023; Venue: Interspeech; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'An MRI-to-speech model that improves both computational efficiency and speech fidelity is proposed and the proposed MRI representation is more comprehensive than EMA and the most suitable MRI feature subset for articulatory synthesis is identified.'} Author (LTI's Professor): Shinji Watanabe; Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN; Authors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, R. Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Est\\u00e8ve, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, D\\u00e1vid Javorsk\\u00fd, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Pol\\u00e1k, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian St\\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, Marco Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos; Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.; Year: 2023; Venue: International Workshop on Spoken Language Translation; Citations: 26; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Shinji Watanabe; Title: Speech collage: code-switched audio generation by collaging monolingual corpora; Authors: A. Hussein, Dorsa Zeinali, Ondrej Klejch, Matthew Wiesner, Brian Yan, Shammur A. Chowdhury, Ahmed Ali, Shinji Watanabe, S. Khudanpur; Abstract: Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': \\\"Speech Collage is introduced, a method that synthesizes CS data from monolingual corpora by splicing audio segments that improves the smoothness quality of audio generation using an overlap-add approach and demonstrates that CS augmentation bolsters the model's code-switching inclination and reduces itsmonolingual bias.\\\"} Author (LTI's Professor): Shinji Watanabe; Title: Exploration on HuBERT with Multiple Resolutions; Authors: Jiatong Shi, Yun Tang, H. Inaguma, Hongyu Gong, J. Pino, Shinji Watanabe; Abstract: Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.; Year: 2023; Venue: Interspeech; Citations: 4; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Through experiments, it is demonstrated that HuBERT with multiple resolutions outperforms the original model, highlighting the potential of utilizing multiple resolutions in SSL models like HuberT to capture diverse information from speech signals.'} Author (LTI's Professor): Sean Welleck; Title: Self-Refine: Iterative Refinement with Self-Feedback; Authors: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, Peter Clark; Abstract: Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 415; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Self-Refine is introduced, an approach for improving initial outputs from LLMs through iterative feedback and refinement that demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using this simple, standalone approach.'} Author (LTI's Professor): Sean Welleck; Title: D RAFT , S KETCH , AND P ROVE : G UIDING F ORMAL T HEOREM P ROVERS WITH I NFORMAL P ROOFS; Authors: Albert Qiaochu Jiang, S. Welleck, J. Zhou, Wen-Ding Li, Jiacheng Liu, M. Jamnik, Timoth\\u00e9e Lacroix, Guillaume Lample, Yuhuai Wu; Abstract: The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce wellstructured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems. Figure 1: Draft, Sketch, and Prove. Starting with an informal statement, our framework yields a formal proof through a three-stage process: drafting informal proofs, mapping them into formal sketches, and proving the remaining conjectures. Concretely, an informal statement is a mathematical problem described in a mixture of natural and mathematical languages (e.g., formulae in LTEX). Then, we use a large language model to autoformalize each informal proof into a formal sketch, which is a skeleton of the formal proof with open conjectures left unproven (indicated by the <proof> blocks). The formal sketch mirrors the structure of the informal proof. Finally, the open conjectures/gaps inside each formal sketch are proved by an off-the-shelf prover. \\u2020Equal contributions as leading authors. Correspondence to: qj213@cam.ac.uk. \\u2021Equal contributions as senior authors.; Year: 2023; Venue: ; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work introduces Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems.'} Author (LTI's Professor): Sean Welleck; Title: Faith and Fate: Limits of Transformers on Compositionality; Authors: Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, S. Welleck, Xiang Ren, Allyson Ettinger, Za\\u00efd Harchaoui, Yejin Choi; Abstract: Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\\\,increased\\\\,task\\\\,complexity.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 96; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills.'} Author (LTI's Professor): Sean Welleck; Title: STEER: Unified Style Transfer with Expert Reinforcement; Authors: Skyler Hallinan, Faeze Brahman, Ximing Lu, Jaehun Jung, S. Welleck, Yejin Choi; Abstract: While text style transfer has many applications across natural language processing, the core premise of transferring from a single source style is unrealistic in a real-world setting. In this work, we focus on arbitrary style transfer: rewriting a text from an arbitrary, unknown style to a target style. We propose STEER: Unified Style Transfer with Expert Reinforcement, a unified frame-work developed to overcome the challenge of limited parallel data for style transfer. STEER involves automatically generating a corpus of style-transfer pairs using a product of experts during decoding. The generated offline data is then used to pre-train an initial policy before switching to online, off-policy reinforcement learning for further improvements via fine-grained reward signals. STEER is unified and can transfer to multiple target styles from an arbitrary, unknown source style, making it particularly flexible and efficient. Experimental results on a challenging dataset with text from a diverse set of styles demonstrate state-of-the-art results compared to competitive baselines. Remarkably, STEER outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size. We also show STEER is robust, maintaining its style transfer capabilities on out-of-domain data, and surpassing nearly all baselines across various styles. The success of our method highlights the potential of RL algorithms when augmented with controllable decoding to overcome the challenge of limited data supervision.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The proposed STEER: Unified Style Transfer with Expert Reinforcement, a unified frame-work developed to overcome the challenge of limited parallel data for style transfer, outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size.'} Author (LTI's Professor): Sean Welleck; Title: LLMSTEP: LLM proofstep suggestions in Lean; Authors: S. Welleck, Rahul Saha; Abstract: We present LLMSTEP, a tool for integrating a language model into the Lean proof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to a server hosting a language model. The language model generates suggestions, which are checked in Lean and displayed to a user in their development environment. We provide a baseline language model, along with code for fine-tuning and evaluation to support further development. We provide server implementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a step towards fast, effective language model suggestions for any user.; Year: 2023; Venue: arXiv.org; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A baseline language model is provided, along with code for fine-tuning and evaluation to support further development, and server implementations that run on CPU, a CUDA GPU, or a Google Colab notebook are provided as a step towards fast, effective language model suggestions for any user.'} Author (LTI's Professor): Sean Welleck; Title: Llemma: An Open Language Model For Mathematics; Authors: Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, S. Welleck; Abstract: We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.; Year: 2023; Venue: arXiv.org; Citations: 47; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Llemma is a large language model for mathematics that outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis, and is capable of tool use and formal theorem proving without any further finetuning.'} Author (LTI's Professor): Sean Welleck; Title: Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning; Authors: Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Raghavi Chandu, Abhilasha Ravichander, Lianhui Qin, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian R. Fisher, Bill Yuchen Lin, Skyler Hallinan, Xiang Ren, S. Welleck, Yejin Choi; Abstract: While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.; Year: 2023; Venue: Conference on Empirical Methods in Natural Language Processing; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Inference-time Policy Adapters (IPA) is proposed, which efficiently tailors a language model such as GPT-3 without fine-tuning it, and consistently brings significant improvements over off-the-shelf language models.'} Author (LTI's Professor): Eric P. Xing; Title: Identification of Nonlinear Latent Hierarchical Models; Authors: Lingjing Kong, Biwei Huang, Feng Xie, E. Xing, Yuejie Chi, Kun Zhang; Abstract: Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of causal structures and latent variables (up to invertible transformations) can be achieved under mild assumptions: on causal structures, we allow for multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we permit general nonlinearity and multi-dimensional continuous variables, alleviating existing work's parametric assumptions. Specifically, we first develop an identification criterion in the form of novel identifiability guarantees for an elementary latent variable model. Leveraging this criterion, we show that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure. To the best of our knowledge, our work is the first to establish identifiability guarantees for both causal structures and latent variables in nonlinear latent hierarchical models.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 6; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work develops an identification criterion in the form of novel identifiability guarantees for an elementary latent variable model and shows that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure.'} Author (LTI's Professor): Eric P. Xing; Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields; Authors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, E. Xing; Abstract: 3D style transfer aims to render stylized novel views of a 3D scene with multiview consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 12; TLDR: {'model': 'tldr@v2.0.0', 'text': 'StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field, achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner.'} Author (LTI's Professor): Eric P. Xing; Title: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena; Authors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, E. Xing, Haotong Zhang, Joseph Gonzalez, Ion Stoica; Abstract: Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 780; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans, and LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.'} Author (LTI's Professor): Eric P. Xing; Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models; Authors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang; Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.; Year: 2023; Venue: Computer Vision and Pattern Recognition; Citations: 9; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE, and formulate the underlying data-generating process as a hierarchical latent variable model, and shows that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model.'} Author (LTI's Professor): Chenyan Xiong; Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases; Authors: Zhenghao Liu, Senkun Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu; Abstract: This paper proposes Text mAtching based SequenTial rEcommenda-tion model (TASTE), which maps items and users in an embedding space and recommends items by matching their text representations. TASTE verbalizes items and user-item interactions using identifiers and attributes of items. To better characterize user behaviors, TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding. Our experiments show that TASTE outperforms the state-of-the-art methods on widely used sequential recommendation datasets. TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems. Our further analyses illustrate that TASTE significantly improves the recommendation accuracy by reducing the popularity bias of previous item id based recommendation models and returning more appropriate and text-relevant items to satisfy users. All codes are available at https://github.com/OpenMatch/TASTE.; Year: 2023; Venue: International Conference on Information and Knowledge Management; Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': None} Author (LTI's Professor): Chenyan Xiong; Title: Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval; Authors: S. Yu, Cheng-Chung Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, Zhenghao Liu Tsinghua University, Huazhong University of Science, Technology, Microsoft Research, M. I. O. Technology, N. University; Abstract: Common IR pipelines are typically cascade systems that may involve multiple rankers and/or fusion models to integrate different information step-by-step. In this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5 significantly improves ranking performance over prior pipelines. Analyses find that through global attention, FiT5 is able to jointly utilize the ranking features via gradually attending to related documents, and thus improve the detection of subtle nuances between them. Our code will be open-sourced.; Year: 2023; Venue: arXiv.org; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention, is proposed.'} Author (LTI's Professor): Chenyan Xiong; Title: OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit; Authors: Shi Yu, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu; Abstract: Pre-trained language models (PLMs) have emerged as the foundation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel models, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch.; Year: 2023; Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval; Citations: 7; TLDR: {'model': 'tldr@v2.0.0', 'text': 'As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure.'} Author (LTI's Professor): Yiming Yang; Title: Accurate detection of reactive oxygen species by tuning an elastic motif (GPGGA)4 in nanopores.; Authors: Cunli Wang, Yiming Yang, Shuai Shao, Hangyu Zhang, Na Li, Zheng-Zhu Zhang, Bo Liu; Abstract: We have developed a reactive oxygen species (ROS) sensor based on nanopores modified with GGGCEG(GPGGA)4CEG. The formation of an intramolecular disulfide bond oxidized by ROS leads to conformation changes in GGGCEG(GPGGA)4CEG, which then induces an obvious change in the size of the nanopores and a corresponding ionic current change. This work allows the accurate and dynamic monitoring of ROS through the combination of (GPGGA)4 and nanopores.; Year: 2023; Venue: Chemical Communications; Citations: 0; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation; Authors: Junwei Huang, Zhiqing Sun, Yiming Yang; Abstract: Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.; Year: 2023; Venue: arXiv.org; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'Progressive distillation is proposed to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process.'} Author (LTI's Professor): Yiming Yang; Title: Automatic synchrotron tomographic alignment schemes based on genetic algorithms and human-in-the-loop software; Authors: Zhen Zhang, Xiaoxue Bi, Pengcheng Li, Chenglong Zhang, Yiming Yang, Yu Liu, Gang Chen, Yuhui Dong, Gongfa Liu, Yi Zhang; Abstract: A highly automatic alignment scheme is proposed to address the pressing challenge in tomographic alignment of future scanning tomography experiments. The results show that the proposed method exhibits excellent sub-pixel alignment accuracy and high time efficiency.; Year: 2023; Venue: Journal of Synchrotron Radiation; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The results show that the proposed method exhibits excellent sub-pixel alignment accuracy and high time efficiency.'} Author (LTI's Professor): Yiming Yang; Title: High CD8+tumor-infiltrating lymphocytes indicate severe exhaustion and poor prognosis in angioimmunoblastic T-cell lymphoma; Authors: Qiqi Zhu, Yiming Yang, Xueqin Deng, Ningning Chao, Zihang Chen, Y. Ye, Wenyan Zhang, Weiping Liu, Sha Zhao; Abstract: Background Exhaustion of CD8+ tumor-infiltrating lymphocytes (TILs), characterized by the overexpression of immune checkpoints (IC), is a major impediment to anti-tumor immunity. However, the exhaustion status of CD8+TILs in angioimmunoblastic T cell lymphoma (AITL) remains unclear. Therefore, we aimed to elucidate the exhaustion status of CD8+TILs in AITL and its influence on prognosis. Methods The correlation between CD8+TILs and IC expression in AITL was analyzed using single-cell RNA sequencing (n = 2), flow cytometry (n = 20), and RNA sequencing (n = 20). Biological changes related to CD8+TILs exhaustion at different cytotoxic T lymphocyte (CTL) levels (mean expression levels of CD8A, CD8B, GZMA, GZMB, and PRF1) in AITL were evaluated using RNA sequencing (n = 20) and further validated using the GEO dataset (n = 51). The impact of CD8 protein expression and CTL levels on patient prognosis was analyzed using flow cytometry and RNA sequencing, respectively. Results Our findings demonstrated that the higher the infiltration of CD8+TILs, the higher was the proportion of exhausted CD8+TILs characterized by the overexpression of multiple IC. This was accompanied by extensive exhaustion-related biological changes, which suggested severe exhaustion in CD8+TILs and may be one of the main reasons for the poor prognosis of patients with high CD8+TILs and CTL. Conclusion Our study comprehensively reveals the exhaustion status of CD8+TILs and their potential negative impact on AITL prognosis, which facilitates further mechanistic studies and is valuable for guiding immunotherapy strategies.; Year: 2023; Venue: Frontiers in Immunology; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The study comprehensively reveals the exhaustion status of CD8+TILs and their potential negative impact on AITL prognosis, which facilitates further mechanistic studies and is valuable for guiding immunotherapy strategies.'} Author (LTI's Professor): Yiming Yang; Title: Robust Cross-Domain Pseudo-Labeling and Contrastive Learning for Unsupervised Domain Adaptation NIR-VIS Face Recognition; Authors: Yiming Yang, Weipeng Hu, Haiqi Lin, Haifeng Hu; Abstract: Near-infrared and visible face recognition (NIR-VIS) is attracting increasing attention because of the need to achieve face recognition in low-light conditions to enable 24-hour secure retrieval. However, annotating identity labels for a large number of heterogeneous face images is time-consuming and expensive, which limits the application of the NIR-VIS face recognition system to larger scale real-world scenarios. In this paper, we attempt to achieve NIR-VIS face recognition in an unsupervised domain adaptation manner. To get rid of the reliance on manual annotations, we propose a novel Robust cross-domain Pseudo-labeling and Contrastive learning (RPC) network which consists of three key components, i.e., NIR cluster-based Pseudo labels Sharing (NPS), Domain-specific cluster Contrastive Learning (DCL) and Inter-domain cluster Contrastive Learning (ICL). Firstly, NPS is presented to generate pseudo labels by exploring robust NIR clusters and sharing reliable label knowledge with VIS domain. Secondly, DCL is designed to learn intra-domain compact yet discriminative representations. Finally, ICL dynamically combines and refines intrinsic identity relationships to guide the instance-level features to learn robust and domain-independent representations. Extensive experiments are conducted to verify an accuracy of over 99% in pseudo label assignment and the advanced performance of RPC network on four mainstream NIR-VIS datasets.; Year: 2023; Venue: IEEE Transactions on Image Processing; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a novel Robust cross-domain Pseudo-labeling and Contrastive learning (RPC) network which consists of three key components, i.e., NIR cluster-based Pseudo labels Sharing (NPS), Domain-specific cluster Contrastive Learning (DCL) and Inter-domain cluster Contrastives Learning (ICL).'} Author (LTI's Professor): Yiming Yang; Title: Numerical Investigation of Fatigue Crack Propagation Behaviour of 550E High-Performance Steel; Authors: Linfa Xiao, Heng Lin, Yongxiang Wang, Yiming Yang, Huapeng Chen; Abstract: The fatigue crack propagation behaviour of Q550E high-performance steel (HPS) is studied in this paper. Static tensile testing and fatigue crack propagation testing were carried out, and the results were compared with those of Q235. Finite element models were developed and verified against the experimental results. The impacts of the initial crack angle, crack depth ratio, stress ratio, thickness, and corrosion pitting on the fatigue crack propagation behaviour of the HPS were analysed. The results show that the fatigue life of Q550 was reduced by 18% due to the corrosion pitting, but it did not change the crack propagation path. When the stress intensity factor is higher than a certain value, the fatigue performance of Q235 is better than that of Q550E. The initial crack angle of 52.5\\u00b0 is the critical angle of the crack stress intensity factor. The steel tends to fracture as the crack depth ratio increases, and more attention should be paid to the effective crack length in engineering practice. An increasing stress ratio leads to a smaller stress intensity factor, and the thickness affects the stress intensity factor in the later stage. The crack stress intensity factor around the corrosion pits gradually decreases along the thickness direction, and the crack tips around the corrosion pits tend to reach the yield state initially, accelerating the fatigue fracture of the specimen and ultimately leading to a decrease in fatigue life.; Year: 2023; Venue: Metals; Citations: 0; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Phase Behaviors of Charged Macromolecules in Aqueous Solutions; Authors: Yiming Yang, Di Jia; Abstract: Compared to the charge\\u2013charge interaction, the role of the dipole\\u2010dipole interaction has long been ignored in the phase behaviors of charged macromolecules in solutions. Charged macromolecules in solutions exhibit rich phase behaviors due to their complexity and they have been studied extensively. Phase separation can happen for charged macromolecules in the presence of monovalent salt, multivalent salt, and oppositely charged polymers, surfactants, etc., and for more advanced charged macromolecules such as polyzwitterions and polyampholytes, the phase diagram is even richer. In this perspective, the unacknowledged role of dipole\\u2010dipole interaction in the phase behaviors of charged macromolecular solutions will be introduced. Dipolar polymers can form complex, self\\u2010regulating structures which can be employed in various fields from drug\\u2010delivery systems to next\\u2010generation polymers. More importantly, it will shed light on how some of the life's basic and coherent structures such as biomolecular condensates and membrane\\u2010less organelles are assembled and built by charged biomacromolecules such as DNA, RNA, and proteins.; Year: 2023; Venue: Macromolecular Chemistry and Physics; Citations: 1; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Association between albumin-to-globulin ratio and the risk of overall survival in advanced non-small cell lung cancer patients with anlotinib treatment: a retrospective cohort study; Authors: Jinzhan Chen, Cong-jun Xie, Yiming Yang, Shuwen Yang, Jin-xiang Huang, Feiyang Ye, Zhenyang Lin, L. Tong, Jiaxin Liu; Abstract: None; Year: 2023; Venue: BMC Pulmonary Medicine; Citations: 1; TLDR: {'model': 'tldr@v2.0.0', 'text': 'AGR level is an independent protective factor for OS in advanced NSCLC patients who received anlotinib therapy, and was positively associated with OS when AGR was larger than 1.24, for every 1 unit increase in AGR, the risk of death lowered approximately by 80%.'} Author (LTI's Professor): Yiming Yang; Title: Modification Effect of Pt on the Active Sites of Sulfated CeO2 Nanorods for the Selective Catalytic Reduction of NO; Authors: Hao Fan, Yiming Yang, Xu Yang, Xuefeng He, Jian Sun, Liu Yang, Jiao Li, Zhenxing Shen; Abstract: None; Year: 2023; Venue: ACS Applied Nano Materials; Citations: 1; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology; Authors: Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu; Abstract: To investigate the volatile components of Schisandra chinensis (Turcz.) Bail (commonly known as northern Schisandra) of different colors and to explore their similarities and differences, to identify the main flavor substances in the volatile components of the branch exudates of northern schisandra, and finally to establish a fingerprint map of the volatile components of the dried fruits and branch exudates of northern Schisandra of different colors, we used GC-IMS technology to analyze the volatile components of the dried fruits and branch exudates of three different colors of northern Schisandra and established a fingerprint spectra. The results showed that a total of 60 different volatile chemical components were identified in the branch exudates and dried fruits of Schisandra. The components of germplasm resources with different fruit colors were significantly different. The ion mobility spectrum and OPLS-DA results showed that white and yellow fruits were more similar compared to red fruits. The volatile components in dried fruits were significantly higher than those in branch exudates. After VIP (variable importance in projection) screening, 41 key volatile substances in dried fruits and 30 key volatile substances in branch exudates were obtained. After screening by odor activity value (OAV), there were 24 volatile components greater than 1 in both dried fruits and branch exudates. The most important contributing volatile substance was 3-methyl-butanal, and the most important contributing volatile substance in white fruit was (E)-2-hexenal.; Year: 2023; Venue: Molecules; Citations: 2; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Secreted endogenous macrosomes reduce A\\u03b2 burden and ameliorate Alzheimer\\u2019s disease; Authors: Cunli Wang, Yiming Yang, Xiaoyu Zhang, Zhenqiang Shi, Huiling Gao, Manli Zhong, Yong-gang Fan, Hongyan Zhang, Bo Liu, Guangyan Qing; Abstract: Innovative therapeutic strategies are urgently needed for Alzheimer\\u2019s disease (AD) due to the increasing size of the aging population and the lack of effective drug treatment. Here, we report the therapeutic effects of extracellular vesicles (EVs) secreted by microglia, including macrosomes and small EVs, on AD-associated pathology. Macrosomes strongly inhibited \\u03b2-amyloid (A\\u03b2) aggregation and rescued cells from A\\u03b2 misfolding\\u2013induced cytotoxicity. Furthermore, macrosome administration reduced A\\u03b2 plaques and ameliorated cognitive impairment in mice with AD. In contrast, small EVs slightly promoted A\\u03b2 aggregation and did not improve AD pathology. Proteomic analysis of small EVs and macrosomes revealed that macrosomes harbor several important neuroprotective proteins that inhibit A\\u03b2 misfolding. In particular, the small integral membrane protein 10\\u2013like protein 2B in macrosomes has been shown to inhibit A\\u03b2 aggregation. Our observations provide an alternative therapeutic strategy for the treatment of AD over conventional ineffective drug treatments.; Year: 2023; Venue: Science Advances; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'The therapeutic effects of extracellular vesicles secreted by microglia, including macrosomes and small EVs, on AD-associated pathology are reported and macrosomes strongly inhibited \\u03b2-amyloid aggregation and rescued cells from A\\u03b2 misfolding\\u2013induced cytotoxicity.'} Author (LTI's Professor): Yiming Yang; Title: Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition; Authors: Yiming Yang, Weipeng Hu, Haifeng Hu; Abstract: To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.; Year: 2023; Venue: IEEE transactions on circuits and systems for video technology (Print); Citations: 3; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network is proposed to disentangle the latent attributes of heterogeneous faces and learn neutral face representations to solve the challenge of heterogeneity and non-neutrality in face recognition.'} Author (LTI's Professor): Yiming Yang; Title: DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization; Authors: Zhiqing Sun, Yiming Yang; Abstract: Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark.; Year: 2023; Venue: Neural Information Processing Systems; Citations: 20; TLDR: {'model': 'tldr@v2.0.0', 'text': 'DIFUSCO is introduced, a new graph-based diffusion framework for NPC combinatorial optimization that outperforms the previous state-of-the-art neural solvers on the challenging SATLIB benchmark and investigates two types of diffusion models with Gaussian and Bernoulli noise, respectively.'} Author (LTI's Professor): Yiming Yang; Title: Effects of S Content on Inclusion Formation in the Al and Ti\\u2013Mg Complex Deoxidized Steel; Authors: Pengliang Jin, Yiming Yang, Lei Cao, Xinghu Yuan, Guocheng Wang; Abstract: Two groups of Al and Ti\\u2013Mg complex deoxidized steels with different S contents are designed, and inclusion characteristics of two groups of steel samples are observed by field\\u2010emission scanning electron microscopy\\u2013energy\\u2010dispersive spectroscopy. The results show that there are single TiS inclusions, MgAl2O4 (Al2O3)\\u2013TiS, and MgAl2O4\\u2013TiN\\u2013TiS complex inclusions in No. 1 steel (low\\u2010sulfur content) and No. 2 steel (high\\u2010sulfur content). However, there are also complex inclusions containing MnS in the No. 2 steel but not in No. 1 steel. In order to reveal the precipitation mechanism of MnS, equilibrium phase of inclusion from 1873 K to liquidus temperature is further analyzed, and the mass fractions of different inclusions from liquidus to solidus temperature are quantitatively calculated using the element segregation model combined with FactSage 7.2 thermodynamic software. Furthermore, the mismatch values between different crystal planes of MnS, TiS, and TiN are calculated. The results show that MnS (110) is most likely to precipitate on TiS (001), which is consistent with the observation that there is TiS\\u2013MnS interface in the complex inclusions containing MnS in No. 2 steel. This study could be helpful to the controlling sulfide precipitation in Al and Ti\\u2013Mg complex deoxidized steel.; Year: 2023; Venue: Steel Research International; Citations: 2; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs; Authors: Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu; Abstract: Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into sub goal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on sub goal representation functions and sub goal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent sub goal representations and lack an efficient sub goal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.; Year: 2023; Venue: IEEE International Joint Conference on Neural Network; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome limitations in GCHRL and develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures.'} Author (LTI's Professor): Yiming Yang; Title: Comprehensive evaluation of nine grape varieties based on fundamental physical and chemical indicators, color and volatile compounds; Authors: W. Cao, Nan Shu, Yiming Yang, Jinli Wen, Wenpeng Lu; Abstract: BACKGROUND: In todays\\u2019 society, the rapid development of the wine industry and the improvement of peoples\\u2019 living standards make people pay more and more attention to wine grape quality. OBJECTIVE: To evaluate the wine grape varieties in Northeast Chinas\\u2019 grape growing regions for better wine grape quality, we evaluated the quality of different varieties of wine grapes. METHODS: The grape varieties \\u2018Hassan\\u2019 \\u2018Zuoshaner\\u2019 \\u2018Beibinghong\\u2019 \\u2018Zuoyouhong\\u2019 \\u2018Beta\\u2019 \\u2018Shuanghong\\u2019 \\u2018Zijingganlu\\u2019 \\u2018Cabernet Sauvignon\\u2019 and \\u2018Syrah\\u2019 were planted in the grape growing area of Jilin, Northeast China, were used as the subjects of this study. The grape berries were analyzed and tested for morphological indicators, basic physicochemical indicators, color, and phenolic and aromatic composition. RESULTS: According to lab results, \\u2018Hassan\\u2019 contained the highest amount of total phenolics; \\u2018Zuoyouhong\\u2019 had the highest solids and total sugar content; \\u2018Shuanghong\\u2019 had the most elevated total acid and anthocyanin content; \\u2018Zijngganlu\\u2019 had the highest tannin content and acid fixation ratio; Seventy-one volatile compounds were detected in nine grape varieties. CONCLUSIONS: Each of the nine grape varieties has a distinctive flavor, and because of this, grape processing products with regional flavors can be created. The same offer valuable data for future scientific grape resource collection, conservation, and exploitation.; Year: 2023; Venue: Journal of Berry Research; Citations: 1; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Aligning Large Multimodal Models with Factually Augmented RLHF; Authors: Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, K. Keutzer, Trevor Darrell; Abstract: Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in\\\"hallucination\\\", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io.; Year: 2023; Venue: arXiv.org; Citations: 51; TLDR: {'model': 'tldr@v2.0.0', 'text': 'A new alignment algorithm called Factually Augmented RLHF is proposed that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance.'} Author (LTI's Professor): Yiming Yang; Title: Experimental and numerical research on the static behavior of locally corroded OSBD; Authors: J. Peng, Yi Liu, Yiming Yang, Yadong Zhou, Longzhen Xie; Abstract: None; Year: 2023; Venue: Journal of constructional steel research; Citations: 1; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Syncretic Space Learning Network for NIR-VIS Face Recognition; Authors: Yiming Yang, Weipeng Hu, Haifeng Hu; Abstract: To overcome the technical bottleneck of face recognition in low-light scenarios, Near-InfraRed and VISible (NIR-VIS) heterogeneous face recognition is proposed for matching well-lit VIS faces with poorly lit NIR faces. Current cross-modal synthesis methods visually convert the NIR modality to the VIS modality and then perform face matching in the VIS modality. However, using a heavyweight GAN network on unpaired NIR-VIS faces may lead to high synthesis difficulty, low inference efficiency, and other problems. To alleviate the above problems, we simultaneously synthesize NIR and VIS images into modality-independent syncretic images and propose a novel syncretic space learning (SSL) model to eliminate the modal gap. First, Syncretic Modality Generator (SMG) synthesizes NIR and VIS images into syncretic images using channel-level convolution with a shallow CNN. In particular, the discriminative structural information is well preserved and the face quality can be further improved with small modal variations in a self-supervised learning manner. Second, Modality-adversarial Syncretic space Learning (MSL) projects NIR and VIS images into the syncretic space by a syncretic-modality adversarial learning strategy with syncretic pattern guided objective, so the modal gap of NIR-VIS faces can be effectively reduced. Finally, the Syncretic Distribution Consistency (SDC) constructed by NIR-syncretic, syncretic-syncretic, and VIS-syncretic consistency can enhance the intra-class compactness and learn discriminative representations. Extensive experiments on three challenging datasets demonstrate the effectiveness of the SSL method.; Year: 2023; Venue: ACM Trans. Multim. Comput. Commun. Appl.; Citations: 2; TLDR: {'model': 'tldr@v2.0.0', 'text': 'This work simultaneously synthesizes NIR and VIS images into modality-independent syncretic images and proposes a novelsyncretic space learning (SSL) model to eliminate the modal gap, and develops the Syncretic Distribution Consistency (SDC), which can enhance the intra-class compactness and learn discriminative representations.'} Author (LTI's Professor): Yiming Yang; Title: An Experimental Study on Secondary Transfer Performances of Prestress after Anchoring Failure of Steel Wire Strands; Authors: Rihua Yang, Yiming Yang, Xuhui Zhang, Xinzhong Wang; Abstract: To understand the secondary transfer performances of residual prestress after the anchoring failure of end-anchored steel wire strands due to corrosion fracture, six steel wire strand components of post-tensioning prestress were designed and fabricated. One-side fast corrosion was applied to the steel wire strand components using the electrochemical method until anchoring failure was reached. The sphere of influence, stress changes, and the retraction and swelling effect of broken beams after failure were investigated. The influences of factors such as concrete strength, stirrup area, and the length of the component on the secondary transfer length of residual prestress were discussed. Based on the deformation relationship between prestressed steel wire strands and concrete in the stress transfer zone, a stress equation was established and solved through a bond constitutive model. A prediction model of the effective stress transfer length of prestressed steel wire strand after failure was proposed. The results demonstrated that residual prestress can have a secondary transfer after the corrosion fracture of end-anchored steel wire strands, but some effective prestress may be lost. Moreover, the loss of prestress is inversely proportional to concrete compressive strength. When the specimens are relatively short, the prestress loss increases significantly. Concrete strength has significant influences on the length of secondary transfer. The proposed simplified calculation method of the secondary transfer length of residual prestress has a relatively high accuracy, with an average error of 2.9% and a maximum error of 5.2%.; Year: 2023; Venue: Metals; Citations: 1; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models; Authors: Lei Zhao, Chengcheng Liu, Youhua H. Wang, Yiming Yang; Abstract: Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.; Year: 2023; Venue: International Journal of Applied Electromagnetics and Mechanics; Citations: 0; TLDR: None Author (LTI's Professor): Yiming Yang; Title: Distributed cooperative dual closed loop velocity-attitude consensus controller for rendezvous of the underactuated AUV swarm in 3-dimensional space; Authors: Yu Zhang, W. Zhang, G. Xia, Yiming Yang, Yan-luan Zheng, Peiyu Han; Abstract: None; Year: 2023; Venue: Ocean Engineering; Citations: 2; TLDR: None Author (LTI's Professor): Yiming Yang; Title: MRI Features for Predicting Microvascular Invasion and Postoperative Recurrence in Hepatocellular Carcinoma Without Peritumoral Hypointensity; Authors: Zhiyuan Chen, Xiaohuan Li, Yu Zhang, Yiming Yang, Yan Zhang, Dongjing Zhou, Yu Yang, Shuping Zhang, Yupin Liu; Abstract: Purpose To identify MRI features of hepatocellular carcinoma (HCC) that predict microvascular invasion (MVI) and postoperative intrahepatic recurrence in patients without peritumoral hepatobiliary phase (HBP) hypointensity. Patients and Methods One hundred and thirty patients with HCC who underwent preoperative gadoxetate-enhanced MRI and curative hepatic resection were retrospectively reviewed. Two radiologists reviewed all preoperative MR images and assessed the radiological features of HCCs. The ability of peritumoral HBP hypointensity to identify MVI and intrahepatic recurrence was analyzed. We then assessed the MRI features of HCC that predicted the MVI and intrahepatic recurrence-free survival (RFS) in the subgroup without peritumoral HBP hypointensity. Finally, a two-step flowchart was constructed to assist in clinical decision-making. Results Peritumoral HBP hypointensity (odds ratio, 3.019; 95% confidence interval: 1.071\\u20138.512; P=0.037) was an independent predictor of MVI. The sensitivity, specificity, positive predictive value, negative predictive value, and AUROC of peritumoral HBP hypointensity in predicting MVI were 23.80%, 91.04%, 71.23%, 55.96%, and 0.574, respectively. Intrahepatic RFS was significantly shorter in patients with peritumoral HBP hypointensity (P<0.001). In patients without peritumoral HBP hypointensity, the only significant difference between MVI-positive and MVI-negative HCCs was the presence of a radiological capsule (P=0.038). Satellite nodule was an independent risk factor for intrahepatic RFS (hazard ratio,3.324; 95% CI: 1.733\\u20136.378; P<0.001). The high-risk HCC detection rate was significantly higher when using the two-step flowchart that incorporated peritumoral HBP hypointensity and satellite nodule than when using peritumoral HBP hypointensity alone (P<0.001). Conclusion In patients without peritumoral HBP hypointensity, a radiological capsule is useful for identifying MVI and satellite nodule is an independent risk factor for intrahepatic RFS.; Year: 2023; Venue: Journal of Hepatocellular Carcinoma; Citations: 0; TLDR: {'model': 'tldr@v2.0.0', 'text': 'In patients without peritumoral HBP hypointensity, a radiological capsule is useful for identifying MVI and satellite nodule is an independent risk factor for intrahepatic RFS, and a two-step flowchart was constructed to assist in clinical decision-making.'} Author (LTI's Professor): Yiming Yang; Title: Impact of local governments\\u2019 construction land allocation strategies on innovation-driven development of China; Authors: Jian Wang, Shangui Peng, Yuhao Feng, Yiming Yang, Qun Wu; Abstract: None; Year: 2023; Venue: \\u8d44\\u6e90\\u79d1\\u5b66; Citations: 0; TLDR: None \",\n          \"Professor Eric P. Xing Title: Professor (On Leave), Language Technologies Institute Professor Eric P. Xing Office: 8101 Gates & Hillman Centers Professor Eric P. Xing Email: epxing@andrew.cmu.edu Professor Eric P. Xing Phone: 412-268-2559 Professor Eric P. Xing Research Area: None Professor Eric P. Xing Research: The major theme of Professor Xing's research lies in the development of machine learning and statistical methodology; especially for building quantitative models and predictive understandings of the evolutionary mechanism, regulatory circuitry, and developmental processes of biological systems; and for building intelligent systems for a wide range of applications in vision, IR and NLP that involves computational learning and reasoning under uncertainty.Foundations of Statistical Learning , including theory and algorithms for: 1) Time/space varying-coefficient models with evolving structures; 2) Sparse structured input/output models in high-dimensional problems; 3) Nonparametric Bayesian techniques for infinite-dimensional models; 4) RKHS embedding, nonparametric inference, and spectral methods for graphical models; 5) Distributed and online algorithms for optimization, approximate inference, and sampling on massive data.Large-scale Information & Intelligent System: 1) Development of scalable parallel architecture, protocol, programming interface, generic algorithms and models, for Big Learning; 2) Multi-view latent space models, topics models, and sparse coding for image/text/relational data mining; 3) Evolving structure, stable metrics, and prediction for dynamic social networks, goal-driven network design and optimization; 4) Web-scale image understanding, search, prediction, and storyline synthesis; 5) Information visualization, indexing and storage, web/mobile app development.Computational Biology: 1) Understanding genome-microenvironment interactions in cancer and embryogenesis via joint analysis of genomic, proteomic, and pathway signaling data; 2) Genetic analysis of population variation, demography and evolution; 3) Statistical inference of genome-transcriptome-phenome association in complex diseases; 4) Personalized diagnosis and treatment of spectrum diseases via next generation sequencing and computational \\\"omic\\\" analysis; 5) Biological image and text mining. Professor Eric P. Xing Projects: None Professor Eric P. Xing Bio: None Professor Eric P. Xing Education: None \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 168
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkXb_JbscK0z"
   },
   "source": [
    "# Creating a Chroma Index\n",
    "\n",
    "refer to: https://docs.llamaindex.ai/en/stable/optimizing/building_rag_from_scratch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1710348098711,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     },
     "user_tz": 240
    },
    "id": "J7BnuuZSD2rI",
    "outputId": "d9b092a9-e506-4e0d-e72b-9cdb5cb61f61"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "# !mkdir chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhJWUwe7a9Yk"
   },
   "outputs": [],
   "source": [
    "client_embed = {\"cmu-rag-baai-large\": [\"BAAI/bge-large-en-v1.5\", 1024],\n",
    "                \"cmu-rag-baai-large-v2\": [\"BAAI/bge-large-en-v1.5\", 512],\n",
    "                \"cmu-rag-baai-large-v3\": [\"BAAI/bge-large-en-v1.5\", 1024],\n",
    "                \"cmu-rag-baai-large-meta\": [\"BAAI/bge-large-en-v1.5\", 1024]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EgflgX1iktU"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions # for chroma db embed\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding # for llama index embed\n",
    "\n",
    "\n",
    "embed_name = \"BAAI/bge-large-en-v1.5\" # https://huggingface.co/models\n",
    "path = os.path.join('/content/drive/MyDrive/ANLP/ANLP-HW2', 'chroma_db')\n",
    "\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=\"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\", # huggingface api\n",
    "    model_name=embed_name\n",
    ")\n",
    "\n",
    "# same embedding function as chroma index\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_name)\n",
    "\n",
    "# get the Chroma Client\n",
    "client = chromadb.PersistentClient(path=path)\n",
    "\n",
    "# create/get a collection, create_collection if first time\n",
    "collection = client.create_collection(name=\"cmu-rag-baai-large-meta\", embedding_function=huggingface_ef)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# client.delete_collection(name=\"cmu-rag-baai-large-v3\")\n",
    "client.list_collections()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DsfCQLs74pD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710100502456,
     "user_tz": 240,
     "elapsed": 1,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "54bf9e3c-6dac-4710-cbdb-a3518ecbb33b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Collection(name=cmu-rag-baai-large-v2),\n",
       " Collection(name=cmu-rag-baai-large),\n",
       " Collection(name=cmu-rag-baai-large-v3),\n",
       " Collection(name=cmu-rag-baai-large-meta)]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6b8mvPkcJiXQ"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter, TokenTextSplitter\n",
    "\n",
    "# text_parser = SentenceSplitter(chunk_size=1024, paragraph_separator='\\n', chunk_overlap=400)\n",
    "text_parser = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "\n",
    "text_chunks = []\n",
    "# maintain relationship with source doc index, to help inject doc metadata\n",
    "doc_idxs = []\n",
    "for doc_idx, doc in enumerate(df_categorized['text']):\n",
    "    cur_text_chunks = text_parser.split_text(doc)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVWNIsdUJwVu"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    title, text, category, description = df_categorized.iloc[doc_idxs[idx]]\n",
    "    metadata = {\n",
    "        \"source\": title,\n",
    "        \"category\": category,\n",
    "        \"description\": description\n",
    "    }\n",
    "    node.metadata = metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JU9jHWsIJwa2"
   },
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztPMrm4vmIPB"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# vector_store.add(nodes)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vector_retriever = index.as_retriever()\n",
    "response = vector_retriever.retrieve(\"Who is teaching 11711 in Spring 2024?\")"
   ],
   "metadata": {
    "id": "bXL6-HLK-Ytt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response"
   ],
   "metadata": {
    "id": "qbIuTNKpA5E4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNdtln8qNNL3"
   },
   "outputs": [],
   "source": [
    "# construct vector store query\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "query_mode = \"default\"\n",
    "# query_mode = \"sparse\"\n",
    "# query_mode = \"hybrid\"\n",
    "query = \"Who is teaching 11711 in Spring 2024?\"\n",
    "# query = \"Who teaches 11711 Advanced Natural Language Processing?\"\n",
    "\n",
    "query = query.lower()\n",
    "query_embedding = embed_model.get_query_embedding(query)\n",
    "\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=3, mode=query_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgvKp3nINglZ"
   },
   "outputs": [],
   "source": [
    "query_result = vector_store.query(vector_store_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sf8WxYxOBIn"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "nodes_with_scores"
   ],
   "metadata": {
    "id": "nWEeeUF5EDDL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2c8vullm3j0"
   },
   "outputs": [],
   "source": [
    "query = \"Who teaches 11711 Advanced Natural Language Processing?\"\n",
    "query = query.lower()\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load a Chorma Index"
   ],
   "metadata": {
    "id": "HMkztPsyezTl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Any, List, Optional\n",
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    ")\n",
    "import re\n",
    "# from llama_index.core import QueryBundle\n",
    "\n",
    "\n",
    "class ChromadbRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Retriever over a Chroma database vector store.\n",
    "    Refer from https://docs.llamaindex.ai/en/stable\\\n",
    "    /examples/low_level/oss_ingestion_retrieval.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection: chromadb.PersistentClient,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "        self._index = VectorStoreIndex.from_vector_store\\\n",
    "                    (vector_store=self._vector_store, embed_model=embed_model)\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_str: str) -> List[NodeWithScore]:\n",
    "        query_embedding = self._embed_model.get_query_embedding(\n",
    "            query_str\n",
    "        )\n",
    "        query_str = query_str.lower()\n",
    "        filters = None\n",
    "        if 'course' in query_str:\n",
    "            pattern = r'(course|unit)\\s*(\\d{5})'\n",
    "            _match = re.findall(pattern, query_str)\n",
    "            if _match:\n",
    "                filters = MetadataFilters(\n",
    "                      filters=[\n",
    "                          MetadataFilter(\n",
    "                              key=\"Course Number\", \\\n",
    "                              operator=FilterOperator.EQ,\\\n",
    "                              value=str(_match[0][1])\n",
    "                          ),\n",
    "                      ]\n",
    "                )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "            filters=filters\n",
    "        )\n",
    "        query_result = self._vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ],
   "metadata": {
    "id": "czPmMPxkBvEN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create multiple Retriever"
   ],
   "metadata": {
    "id": "3TCZCod4XtGI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Papers"
   ],
   "metadata": {
    "id": "axRbloieK4Ut"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd"
   ],
   "metadata": {
    "id": "BafiBQLeXxBv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df=pd.read_csv(\"knowledge_source_pd/papers/papers.csv\")\n",
    "print(df.shape)\n",
    "df.head(1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "JfZJrtTBzW3W",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710349000677,
     "user_tz": 240,
     "elapsed": 1329,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "93424241-c464-457c-c6c1-ec587e260b8d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(325, 8)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          Author                                              Title  \\\n",
       "0  Yonantan Bisk  SPAE: Semantic Pyramid AutoEncoder for Multimo...   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar...   \n",
       "\n",
       "                                            Abstract  Year  \\\n",
       "0  In this work, we introduce Semantic Pyramid Au...  2023   \n",
       "\n",
       "                                   Venue  Citations  \\\n",
       "0  Neural Information Processing Systems          9   \n",
       "\n",
       "                                                TLDR  \n",
       "0  {'model': 'tldr@v2.0.0', 'text': 'This method ...  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-cb6958be-baba-412e-bb78-dd7106d84b3c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "      <th>Venue</th>\n",
       "      <th>Citations</th>\n",
       "      <th>TLDR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yonantan Bisk</td>\n",
       "      <td>SPAE: Semantic Pyramid AutoEncoder for Multimo...</td>\n",
       "      <td>Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar...</td>\n",
       "      <td>In this work, we introduce Semantic Pyramid Au...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Neural Information Processing Systems</td>\n",
       "      <td>9</td>\n",
       "      <td>{'model': 'tldr@v2.0.0', 'text': 'This method ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb6958be-baba-412e-bb78-dd7106d84b3c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-cb6958be-baba-412e-bb78-dd7106d84b3c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-cb6958be-baba-412e-bb78-dd7106d84b3c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 325,\n  \"fields\": [\n    {\n      \"column\": \"Author\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"Lori Levin\",\n          \"Eric P. Xing\",\n          \"Daphne Ippolito\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 297,\n        \"samples\": [\n          \"FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions\",\n          \"Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding\",\n          \"Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 292,\n        \"samples\": [\n          \"Leena Mathur, Maja J Matari'c, Louis-Philippe Morency\",\n          \"S. Welleck, Rahul Saha\",\n          \"A. F. Cooper, Katherine Lee, James Grimmelmann, Daphne Ippolito, Christopher Callison-Burch, Christopher A. Choquette-Choo, Niloofar Mireshghallah, Miles Brundage, David Mimno, M. Choksi, J. Balkin, Nicholas Carlini, Christopher De Sa, Jonathan Frankle, Deep Ganguli, Bryant Gipson, A. Guadamuz, Swee Leng Harris, Abigail Z. Jacobs, Elizabeth Joh, Gautam Kamath, M. Lemley, Cass Matthews, C. McLeavey, Corynne Mcsherry, Milad Nasr, Paul Ohm, Adam Roberts, Tom Rubin, Pamela Samuelson, Ludwig Schubert, Kristen Vaccaro, Luis Villa, Felix Wu, Elana Zeide\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 278,\n        \"samples\": [\n          \"We present Score, a rule engine designed and implemented for the Scone knowledge base system. Scone is a knowledge base system designed for storing and manipulating rich representations of general knowledge in symbolic form. It represents knowledge in the form of nodes and links in a network structure, and it can perform basic inference about the relationships between different elements efficiently. On its own, Scone acts as a sort of\\\"smart memory\\\"that can interface with other software systems. One area of improvement for Scone is how useful it can be in supplying knowledge to an intelligent agent that can use the knowledge to perform actions and update the knowledge base with its observations. We augment the Scone system with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone's knowledge base, potentially improving the capabilities of any planning systems built on top of Scone. Production rule systems consist of\\\"if-then\\\"production rules that try to match their predicates to existing knowledge and fire their actions when their predicates are satisfied. We propose two kinds of production rules, if-added and if-needed rules, that differ in how they are checked and fired to cover multiple use cases. We then implement methods to efficiently check and fire these rules in a large knowledge base. The new rule engine is not meant to be a complex stand-alone planner, so we discuss how it fits into the context of Scone and future work on planning systems.\",\n          \"The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transfer- ability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.\",\n          \"Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2023,\n        \"max\": 2023,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2023\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Venue\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          \"Conference of the European Chapter of the Association for Computational Linguistics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Citations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 54,\n        \"min\": 0,\n        \"max\": 780,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TLDR\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 262,\n        \"samples\": [\n          \"{'model': 'tldr@v2.0.0', 'text': 'A novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention, is proposed.'}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 39
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# !mkdir chroma_meta\n",
    "client_embed = {\"paper_baai\": [\"BAAI/bge-large-en-v1.5\", None]}"
   ],
   "metadata": {
    "id": "M6Bdxrxu3sjz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions # for chroma db embed\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding # for llama index embed\n",
    "import os\n",
    "\n",
    "embed_name = \"BAAI/bge-large-en-v1.5\" # https://huggingface.co/models\n",
    "path = os.path.join('/content/drive/MyDrive/ANLP/ANLP-HW2', 'chroma_meta')\n",
    "\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=\"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\", # huggingface api\n",
    "    model_name=embed_name\n",
    ")\n",
    "\n",
    "# same embedding function as chroma index\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_name)\n",
    "\n",
    "# get the Chroma Client\n",
    "client = chromadb.PersistentClient(path=path)\n",
    "\n",
    "# create/get a collection, create_collection if first time\n",
    "collection = client.get_collection(name=\"paper_baai\", embedding_function=huggingface_ef)\n",
    "# client.delete_collection(name=\"paper_baai\")"
   ],
   "metadata": {
    "id": "tN6j7PjizW5G",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710348574542,
     "user_tz": 240,
     "elapsed": 12066,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333,
     "referenced_widgets": [
      "9ae345b97fac4a59a3cb86871d2e9118",
      "d1c15eb837c6458790e85f1f86abbfe4",
      "9ef70a69e8004acc95cb81082f51b05d",
      "c1838c1031994a36aee4544b0fd4774e",
      "ff04e10d3a38437e85bd6a64ad6590e6",
      "bf9e5b4c26bc4875b6529adeccf31673",
      "6376d6f79c85426689afe8b34e4a26c0",
      "1d5118b800524938a742d1177729aa53",
      "7601e941a691454baa2bdee7e8b1c49a",
      "161f1802750d49158360944652d1abbd",
      "3de23fb3b40c4abcbf83ab6cb1e86635",
      "33e6f16da28740c198559f547af5a67b",
      "8b542f17579f43c5a8d9ce2c6d7af9b9",
      "11d0ccc33c254581b553ad1dfdaed29a",
      "60c58d8bb620423eb3b34e14d21ab211",
      "7f5d7b30d285448ba8012b8b1ae7e659",
      "a5f9f8765d454ca79fe19ba55db386f3",
      "c7e9e84b374945d095cb0144463c8404",
      "c1cc292914fc469da88e5b0b4de362f8",
      "f4bd5425554045f79173851d4cd72018",
      "9e3ba5f5932b46a1bffa7dd93f0c863f",
      "56030eb9d7854104a44078718bc3413d",
      "6093f1b0f0994f46b5fe49703c9979e9",
      "16482e5e03cf4ed4b1d26c117e96f074",
      "5f2da951b177425aaa76a4c9ec53ed2b",
      "600102a0043e44ef8ae47ba89d0f8152",
      "1bc975eea6654ab2949dc2a45d2247d2",
      "0e721b3083bb4f02a35222d02c25a721",
      "7880b641ed1b4e5fb3486f9dcdb55b66",
      "363b205d380d4b20aa98f8f5f89f518d",
      "4b32bac43d8c4f1eb5929a42a234a6c2",
      "139936304b014a26a84a64bc54ff3985",
      "fb096773ac704323a990979418d197b0",
      "52c67c6c7a484ad889316af03a81d626",
      "03c76c1417bd4be1a3977535a5cc1fc8",
      "b94e9ef0c18b4cf386739b83a1a63bc1",
      "1b82622cac5c4d519c41bdc78bf9fdde",
      "6c048c06ee6e4dbeb624625a1c46aac4",
      "62a50d3eb8a3473db443102ae6265447",
      "c29b7fb1652948059fa7c264ade3eb00",
      "d4d16028ea124968a49b6271619e9489",
      "a34643700ca545a5b36c114e0488cd1a",
      "e62443fd7a7744fbb2a048b7e15d73c1",
      "10b6d187125d4b9988ba659e762f259a",
      "fc935fa78b3b4aaca03335c501d2f26d",
      "228a41f8c0a2484dbac57abd7056fc27",
      "2ec0ee9afe014c41b9890b436943f47f",
      "7a33e0c28c6f4087836ed44c7162381b",
      "f37e6a41268d4af08c25b4482c6ebdd0",
      "42e0f16e2d744ba6ae2dc550364d9f3f",
      "3056660ead814f7a916809b4f6f3a3a7",
      "587fb732821c41439b0c5005de06944e",
      "9793af0d854e400896239e2cfd26ceb4",
      "973158f2600348d7b1d86b4afa4bf9ac",
      "117e3e3599114e9c91a52dc64dc3fb0e",
      "d5332f0a75fa4622b281030e4fc77649",
      "27e5cf64e4794199b1eef32c9a7b183d",
      "271de0eac4b3474798073e42586e0802",
      "528dc6bd0fa7462c829f0d6f68499bfb",
      "2212315d4e0e41259c5c2617ed5fa1b7",
      "ee21d5970b5144daad472f6551f5917a",
      "409b15fb0afe45088375096c7ffa6047",
      "d8b25eb44e6e4a6a8557097a6917ff7b",
      "627f1e57724a423788a249ceb39532e2",
      "9dcf5feb21a847de814eee7a5e6df277",
      "4c6d6cd43e0b46baaa559f23df6f776e"
     ]
    },
    "outputId": "abdffd38-8725-477a-ddb4-8202e441036d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ae345b97fac4a59a3cb86871d2e9118"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33e6f16da28740c198559f547af5a67b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6093f1b0f0994f46b5fe49703c9979e9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52c67c6c7a484ad889316af03a81d626"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc935fa78b3b4aaca03335c501d2f26d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5332f0a75fa4622b281030e4fc77649"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# client.delete_collection(name=\"paper_baai\")"
   ],
   "metadata": {
    "id": "jFhVoGBWcxrE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# df.columns.values.tolist()"
   ],
   "metadata": {
    "id": "N7rXBKIV6J8c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# df['Author'] = df['Author'].astype(str).fillna(\"\")\n",
    "# df['Title'] = df['Title'].astype(str).fillna(\"\")\n",
    "# df['Authors'] = df['Authors'].astype(str).fillna(\"\")\n",
    "# df['Authors'] = df['Authors'].astype(str).fillna(\"\")\n",
    "# df['Year'] = df['Year'].astype(str)\n",
    "# df['Venue'] = df['Venue'].astype(str).fillna(\"\")\n",
    "# df['Citations'] = df['Citations'].astype(str).fillna(\"\")\n",
    "# df['TLDR'] = df['TLDR'].astype(str).fillna(\"\")"
   ],
   "metadata": {
    "id": "zKkDM1L_CJU4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from llama_index.core.schema import TextNode\n",
    "\n",
    "# nodes = []\n",
    "# for idx, text_chunk in df.iterrows():\n",
    "#     Author, Title, Authors, Abstract, Year, Venue, Citations, TLDR\\\n",
    "#         = df.iloc[idx]\n",
    "\n",
    "#     node = TextNode(\n",
    "#         text=f'The Paper: {Title} written by LTI Prof.: {Author} has an abstract: {Abstract}',\n",
    "#     )\n",
    "#     metadata = {\n",
    "#         \"LTI Author\": Author,\n",
    "#         \"Title\": Title,\n",
    "#         \"CO Authors\": Authors,\n",
    "#         \"Year\": Year,\n",
    "#         \"Venue\": Venue,\n",
    "#         \"Citations\": Citations,\n",
    "#         # \"Abstract\": Abstract,\n",
    "#         \"TLDR\": TLDR\n",
    "#     }\n",
    "#     node.metadata = metadata\n",
    "#     nodes.append(node)"
   ],
   "metadata": {
    "id": "9U6BC37DzW6n"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# for node in nodes:\n",
    "#     node_embedding = embed_model.get_text_embedding(\n",
    "#         node.get_content(metadata_mode=\"all\")\n",
    "#     )\n",
    "#     node.embedding = node_embedding"
   ],
   "metadata": {
    "id": "CmzEkEdyzW8O"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from llama_index.core import StorageContext\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# vector_store.add(nodes)\n",
    "# index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embed_model)"
   ],
   "metadata": {
    "id": "_59KQpUg-0Az"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df[df['Title'].str.contains('SenteCon')]['Abstract'].values[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "jJZMw2Xevr_s",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710349079789,
     "user_tz": 240,
     "elapsed": 4,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "82196b6a-9863-4583-b32e-48d7d6dd981b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 45
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "from llama_index.llms.huggingface import (\n",
    "    HuggingFaceInferenceAPI,\n",
    "    HuggingFaceLLM,\n",
    ")\n",
    "\n",
    "\n",
    "retriever = ChromadbRetriever(\n",
    "    collection, embed_model, query_mode=\"default\", similarity_top_k=5\n",
    ")\n",
    "\n",
    "HF_TOKEN = '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'\n",
    "\n",
    "query_str = 'What does paper SenteCon do to a given passage of text?'\n",
    "response = retriever._retrieve(query_str)"
   ],
   "metadata": {
    "id": "e5oWD5EX-0FN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response[0].get_content()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "dwXs2qLMvOkX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710349200327,
     "user_tz": 240,
     "elapsed": 5,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "c5572c33-e192-4805-b94c-c88849d52b41"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"The Paper: SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations written by LTI Prof.: Louis-Philippe Morency has an abstract: Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 48
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## course"
   ],
   "metadata": {
    "id": "6LokBc_8K76k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "path = \"knowledge_source_pd/courses/*.csv\"\n",
    "csv_files = glob.glob(path)\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df = df.drop(0, axis=0).reset_index(drop=True)\n",
    "df['begin'] = df['begin'].fillna('TBD')\n",
    "df['end'] = df['end'].fillna('TBD')\n",
    "# content\n",
    "df['course_info'] = df.apply(lambda row: f\"The course {row['course_number']}: {row['title']} is taught by: {row['instructor']}\", axis=1)\n",
    "df.head(1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "SDY1iQOHqkXo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710539618777,
     "user_tz": 240,
     "elapsed": 4145,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "8fb48bfd-c6ab-4000-acd0-1a29212c8d72"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    schedule_title course_number                              title units  \\\n",
       "0  Summer Two 2024         48731  Sustainable Design Synthesis Prep  1-18   \n",
       "\n",
       "  section  day begin  end room                  location      instructor  \\\n",
       "0       U  TBA   TBD  TBD  TBA  Pittsburgh, Pennsylvania  Instructor TBA   \n",
       "\n",
       "                                         course_info  \n",
       "0  The course 48731: Sustainable Design Synthesis...  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-14e9fd3e-20e7-4fc2-87ba-39c80e7ad69a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schedule_title</th>\n",
       "      <th>course_number</th>\n",
       "      <th>title</th>\n",
       "      <th>units</th>\n",
       "      <th>section</th>\n",
       "      <th>day</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>room</th>\n",
       "      <th>location</th>\n",
       "      <th>instructor</th>\n",
       "      <th>course_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer Two 2024</td>\n",
       "      <td>48731</td>\n",
       "      <td>Sustainable Design Synthesis Prep</td>\n",
       "      <td>1-18</td>\n",
       "      <td>U</td>\n",
       "      <td>TBA</td>\n",
       "      <td>TBD</td>\n",
       "      <td>TBD</td>\n",
       "      <td>TBA</td>\n",
       "      <td>Pittsburgh, Pennsylvania</td>\n",
       "      <td>Instructor TBA</td>\n",
       "      <td>The course 48731: Sustainable Design Synthesis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14e9fd3e-20e7-4fc2-87ba-39c80e7ad69a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-14e9fd3e-20e7-4fc2-87ba-39c80e7ad69a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-14e9fd3e-20e7-4fc2-87ba-39c80e7ad69a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5873,\n  \"fields\": [\n    {\n      \"column\": \"schedule_title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Summer One 2024\",\n          \"Spring 2024\",\n          \"Summer Two 2024\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"course_number\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4435,\n        \"samples\": [\n          \"57794\",\n          \"15110\",\n          \"80244\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3512,\n        \"samples\": [\n          \"Microeconomics I\",\n          \"Professional Development for Mechanical Engineers\",\n          \"Intermediate Game Studio: Interactivity\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"units\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 108,\n        \"samples\": [\n          \"1,2\",\n          \"0-99\",\n          \"12,36\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 83,\n        \"samples\": [\n          \"12\",\n          \"U\",\n          \"M2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"TRF\",\n          \"WF\",\n          \"MTS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"begin\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"04:00PM\",\n          \"05:30PM\",\n          \"01:00PM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 71,\n        \"samples\": [\n          \"09:20PM\",\n          \"TBD\",\n          \"08:50AM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"room\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 351,\n        \"samples\": [\n          \"PH 107E\",\n          \"INI 205\",\n          \"NSH 3002\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"San Jose, California\",\n          \"Pittsburgh, Pennsylvania\",\n          \"Washington, District of Columbia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"instructor\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1928,\n        \"samples\": [\n          \"Moskal\",\n          \"Reilly, Helin\",\n          \"Durschmid, Kang\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"course_info\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5149,\n        \"samples\": [\n          \"The course 54519: Acting for the Camera is taught by: Kovitz\",\n          \"The course 09222: Laboratory II: Organic Synthesis and Analysis is taught by: Botcha\",\n          \"The course 54207: Movement II is taught by: Warman\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions # for chroma db embed\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding # for llama index embed\n",
    "import os\n",
    "\n",
    "embed_name = \"BAAI/bge-large-en-v1.5\" # https://huggingface.co/models\n",
    "path = os.path.join('/content/drive/MyDrive/ANLP/ANLP-HW2', 'chroma_meta')\n",
    "\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=\"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\", # huggingface api\n",
    "    model_name=embed_name\n",
    ")\n",
    "\n",
    "# same embedding function as chroma index\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_name)\n",
    "\n",
    "# get the Chroma Client\n",
    "client = chromadb.PersistentClient(path=path)\n",
    "\n",
    "# create/get a collection, create_collection if first time\n",
    "collection = client.get_collection(name=\"course_baai\", embedding_function=huggingface_ef)"
   ],
   "metadata": {
    "id": "Kvq7yDY8LOxO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# store embedding\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "\n",
    "for idx, _ in df.iterrows():\n",
    "    schedule_title, course_number, title, units, section, day, begin, end, \\\n",
    "      room, location, instructor, course_info = df.iloc[idx]\n",
    "\n",
    "    node = TextNode(\n",
    "        text=(course_info)\n",
    "    )\n",
    "    metadata = {\n",
    "        \"Schedule Title\": schedule_title,\n",
    "        \"Course Number\": course_number,\n",
    "        \"Course Title\": title,\n",
    "        \"Units\": units,\n",
    "        \"Section\": section,\n",
    "        \"Day\": day,\n",
    "        \"Begin Time\": begin,\n",
    "        \"End Time\": end,\n",
    "        \"Room\": room,\n",
    "        \"Location\": location,\n",
    "        \"Instructor\": instructor\n",
    "    }\n",
    "    node.metadata = metadata\n",
    "    nodes.append(node)\n",
    "\n",
    "\n",
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ],
   "metadata": {
    "id": "_Sx9Qf8kqE6b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "vector_store.add(nodes)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embed_model)"
   ],
   "metadata": {
    "id": "j382HVsoKx7C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# from IPython.display import Markdown, display\n",
    "from llama_index.llms.huggingface import (\n",
    "    HuggingFaceInferenceAPI,\n",
    "    HuggingFaceLLM,\n",
    ")\n",
    "\n",
    "retriever = ChromadbRetriever(\n",
    "    collection, embed_model, query_mode=\"default\", similarity_top_k=3\n",
    ")\n",
    "\n",
    "HF_TOKEN = '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'\n",
    "\n",
    "gemma_7b = HuggingFaceInferenceAPI(\n",
    "    model_name=\"google/gemma-7b\",\n",
    "    token=HF_TOKEN,\n",
    "#     query_wrapper_prompt=query_wrapper_prompt,\n",
    "#     response_wrapper_prompt=response_wrapper_prompt\n",
    ")\n",
    "\n",
    "query_engine = retriever._index.as_query_engine(\n",
    "    llm=gemma_7b,\n",
    "    similarity_top_k=3,\n",
    "    )\n",
    "\n",
    "query_str = 'How many Chemical Engineering courses are going to be held in Summer 2024?'\n",
    "# response = retriever._retrieve(query_str)\n",
    "\n",
    "# for i in range(len(response)):\n",
    "#    print(response[i].get_content(metadata_mode=\"all\"))"
   ],
   "metadata": {
    "id": "1TnuKcUEL5aq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import requests\n",
    "# url = 'https://raw.githubusercontent.com/neubig/nlp-from-scratch-assignment-spring2024/main/data/questions.txt'\n",
    "# response = requests.get(url)\n",
    "\n",
    "# for i in [i for i in response.text.split('\\n') if 'course' in i.lower()]:\n",
    "#     response = retriever._retrieve(i)\n",
    "#     print(f'\\nquestions: {i}')\n",
    "#     print(f'response:')\n",
    "#     for i in range(len(response)):\n",
    "#         print(response[i].get_content())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkleRB9lsJiF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710391612483,
     "user_tz": 240,
     "elapsed": 5098,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "d6b7ba6c-c64c-436c-a553-8444fef31c95"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "questions: What's the course number for large language models methods and application?\n",
      "response:\n",
      "The course 11667: Large Language Models Methods and Application is taught by: Ippolito, Xiong\n",
      "The course 11868: Large Language Model Systems is taught by: Li\n",
      "The course 46924: Natural Language Processing is taught by: Yurko\n",
      "\n",
      "questions: In spring 2024, How many units is course 10315?\n",
      "response:\n",
      "The course 10315: Introduction to Machine Learning (SCS Majors) is taught by: Virtue\n",
      "The course 10315: Introduction to Machine Learning (SCS Majors) is taught by: Balcan\n",
      "\n",
      "questions: In fall 2023, What is the title of course 05291?\n",
      "response:\n",
      "The course 05291: Learning Media Design is taught by: Louw\n",
      "\n",
      "questions: When does the Spring 2025 course registeration start for masters students?\n",
      "response:\n",
      "The course 27756: Masters Project is taught by: Rohrer\n",
      "The course 11928: Masters Thesis I is taught by: Frederking\n",
      "The course 03655: Graduate Topics in Research is taught by: Brasier\n",
      "\n",
      "questions: In fall 2023, What is the location of course 05317?\n",
      "response:\n",
      "The course 05317: Design of Artificial Intelligence Products is taught by: Martelaro, Saffer\n",
      "\n",
      "questions: In fall 2023, What is the title of course 05391?\n",
      "response:\n",
      "The course 05391: Designing Human Centered Software is taught by: Lindlbauer\n",
      "\n",
      "questions: In fall 2023, Who is the instructor for course 05315?\n",
      "response:\n",
      "The course 05315: Persuasive Design is taught by: Kaufman\n",
      "\n",
      "questions: In spring 2024, What is the title of course 15151?\n",
      "response:\n",
      "The course 15151: Mathematical Foundations for Computer Science is taught by: Peng\n",
      "The course 15151: Mathematical Foundations for Computer Science is taught by: Mackey\n",
      "\n",
      "questions: What are the course number(s) for the courses on LLMs?\n",
      "response:\n",
      "The course 36462: Special Topics: Statistical Machine Learning is taught by: Lei\n",
      "The course 36662: Statistical Machine Learning is taught by: Lei\n",
      "The course 21301: Combinatorics is taught by: Lew\n",
      "\n",
      "questions: What are the course number(s) for the Search Engines course?\n",
      "response:\n",
      "The course 11442: Search Engines is taught by: Callan\n",
      "The course 11442: Search Engines is taught by: Callan\n",
      "The course 11642: Search Engines is taught by: Callan\n",
      "\n",
      "questions: To complete the course requirements for the PhD in Language and Information Technologies degree, how many course units of graduate courses does the student have to pass?\n",
      "response:\n",
      "The course 82989: Second Language Acquisition Dissertation Research is taught by: Burns\n",
      "The course 82989: Second Language Acquisition Dissertation Research is taught by: Van Compernolle\n",
      "The course 76922: PHD/R Dissertation Writing is taught by: Loughran\n",
      "\n",
      "questions: In spring 2024, What is the title of course 10701?\n",
      "response:\n",
      "The course 10701: Introduction to Machine Learning (PhD) is taught by: Chai\n",
      "The course 10701: Introduction to Machine Learning (PhD) is taught by: Chai, Lipton\n",
      "\n",
      "questions: In spring 2024, What is the title of course 17200?\n",
      "response:\n",
      "The course 17200: Ethics and Policy Issues in Computing is taught by: Skirpan, Dabbish\n",
      "\n",
      "questions: In spring 2024, What is the deadline for adding or dropping a Mini-4 course with tuition adjustment?\n",
      "response:\n",
      "The course 73230: Intermediate Microeconomics is taught by: Nguyen\n",
      "The course 73240: Intermediate Macroeconomics is taught by: Shi\n",
      "The course 47724: Seminar in Finance IV (Corporate Finance) is taught by: Mayer\n",
      "\n",
      "questions: In fall 2023, When is the deadline to drop a Mini-2 course with a withdrawal grade assigned?\n",
      "response:\n",
      "The course 88397: SDS Undergraduate Research - mini is taught by: Oppenheimer\n",
      "The course 11390: LTI Minor Project - Juniors is taught by: Instructor TBA\n",
      "The course 11590: LTI Minor Project - Advanced is taught by: Instructor TBA\n",
      "\n",
      "questions: Does LTI offer a course on text mining?\n",
      "response:\n",
      "The course 11441: Machine Learning for Text and Graph-based Mining is taught by: Yang\n",
      "The course 11741: Machine Learning for Text and Graph-based Mining is taught by: Yang\n",
      "The course 11641: Machine Learning for Text and Graph-based Mining is taught by: Yang\n",
      "\n",
      "questions: Does SCS Interdisciplinary offer more than 1 course in Summer 2024?\n",
      "response:\n",
      "The course 15698: MSCS Research Thesis is taught by: Eckhardt\n",
      "The course 15591: Independent Study in Computer Science is taught by: Stehlik\n",
      "The course 03210: Independent Study is taught by: Willard\n",
      "\n",
      "questions: How many courses does Abdelghany teach in Summer 2024?\n",
      "response:\n",
      "The course 82114: Arabic for Global Exchange Online is taught by: Al Masaeed\n",
      "The course 09050: Study Abroad is taught by: Stump\n",
      "The course 54495: Internship is taught by: Instructor TBA\n",
      "\n",
      "questions: In spring 2024, What is the course number for Game Theoretic Probability, Statistics and Learning?\n",
      "response:\n",
      "The course 10880: Game Theoretic Probability, Statistics and Learning is taught by: Ramdas\n",
      "The course 10422: Foundations of Learning, Game Theory, and Their Connections is taught by: Balcan\n",
      "The course 88312: Decision Models and Games is taught by: Gonzalez\n",
      "\n",
      "questions: In spring 2024, When do Mini-3 faculty course evaluations open?\n",
      "response:\n",
      "The course 47802: Microeconomics III is taught by: Shourideh\n",
      "The course 17634: Applied Machine Learning is taught by: Worrell, Scanlon\n",
      "The course 73230: Intermediate Microeconomics is taught by: Nguyen\n",
      "\n",
      "questions: Which two faculty are co-teaching the neural code generation course?\n",
      "response:\n",
      "The course 10733: Representation and Generation in Neuroscience and AI is taught by: Wehbe\n",
      "The course 10423: Generative AI is taught by: Gormley, Li, Lipton\n",
      "The course 15386: Neural Computation is taught by: Lee\n",
      "\n",
      "questions: What are the course numbers for question answering courses at LTI?\n",
      "response:\n",
      "The course 11797: Question Answering is taught by: Mitamura, Nyberg\n",
      "The course 46924: Natural Language Processing is taught by: Yurko\n",
      "The course 11411: Natural Language Processing is taught by: Mortensen, Strubell\n",
      "\n",
      "questions: In fall 2023, What is the course number for Undergraduate Research in Computational Biology?\n",
      "response:\n",
      "The course 02500: Undergraduate Research in Computational Biology is taught by: Compeau\n",
      "The course 02500: Undergraduate Research in Computational Biology is taught by: Compeau\n",
      "The course 03601: Computational Biology Internship is taught by: Brasier\n",
      "\n",
      "questions: In spring 2024, What is the title of course 15110?\n",
      "response:\n",
      "The course 15110: Principles of Computing is taught by: Rivers\n",
      "The course 15110: Principles of Computing is taught by: Rivers\n",
      "\n",
      "questions: In spring 2024, How many units is course 15090 worth?\n",
      "response:\n",
      "The course 15090: Computer Science Practicum is taught by: Stehlik\n",
      "The course 15090: Computer Science Practicum is taught by: Stehlik\n",
      "\n",
      "questions: Where does the sharp right-hand turn of the buggy course occur?\n",
      "response:\n",
      "The course 16711: Kinematics, Dynamic Systems and Control is taught by: Geyer\n",
      "The course 16833: Robot Localization and Mapping is taught by: Kaess\n",
      "The course 16778: Mechatronic Design is taught by: Riviere\n",
      "\n",
      "questions: In spring 2024, Who is the instructor for course 15151?\n",
      "response:\n",
      "The course 15151: Mathematical Foundations for Computer Science is taught by: Peng\n",
      "The course 15151: Mathematical Foundations for Computer Science is taught by: Mackey\n",
      "\n",
      "questions: In spring 2024, How many units is course 17214?\n",
      "response:\n",
      "The course 17214: Principles of Software Construction: Objects, Design, and Concurrency is taught by: Aldrich, Garrod, Lacomis\n",
      "The course 17214: Principles of Software Construction: Objects, Design, and Concurrency is taught by: Kaestner, Lacomis\n",
      "\n",
      "questions: In spring 2024, What is the location of course 10500?\n",
      "response:\n",
      "The course 10500: Senior Research Project is taught by: Gormley\n",
      "The course 10500: Senior Research Project is taught by: Gormley\n",
      "The course 10500: Senior Research Project is taught by: Gormley\n",
      "\n",
      "questions: Who taught the first freshman-level computer programming course at CMU?\n",
      "response:\n",
      "The course 11637: Foundations of Computational Data Science is taught by: Rose\n",
      "The course 10681: Computational Foundations for Machine Learning is taught by: Wilder\n",
      "The course 17691: Machine Learning in Practice is taught by: Faber\n",
      "\n",
      "questions: When was the first freshman-level computer programming course offered at CMU?\n",
      "response:\n",
      "The course 11637: Foundations of Computational Data Science is taught by: Rose\n",
      "The course 10681: Computational Foundations for Machine Learning is taught by: Wilder\n",
      "The course 17691: Machine Learning in Practice is taught by: Faber\n",
      "\n",
      "questions: In spring 2024, What is the title of course 15122?\n",
      "response:\n",
      "The course 15122: Principles of Imperative Computation is taught by: Cervesato, Kohlbrenner\n",
      "The course 15122: Principles of Imperative Computation is taught by: Kaynar, Cervesato\n",
      "The course 15122: Principles of Imperative Computation is taught by: Kohlbrenner\n",
      "\n",
      "questions: In summer 2024, When do Semester & Mini-6 Faculty Course Evaluations open?\n",
      "response:\n",
      "The course 03445: Undergraduate Research is taught by: Willard\n",
      "The course 03700: MS Thesis Research is taught by: Brasier\n",
      "The course 03210: Independent Study is taught by: Willard\n",
      "\n",
      "questions: In fall 2023, When are the Semester & Mini-2 Faculty Course Evaluations open?\n",
      "response:\n",
      "The course 85510: Research in Psychology - Mini is taught by: Thiessen\n",
      "The course 06700: M.S. Research is taught by: Robinson\n",
      "The course 85483: Internship in Psychology - Mini is taught by: Thiessen\n",
      "\n",
      "questions: How many StuCo or Student Led Courses are going to be held in Spring 2024?\n",
      "response:\n",
      "The course 98172: Student Taught Courses(StuCo): Introduction to Command-line Tools is taught by: Ledon\n",
      "The course 98317: Student Taught Courses (StuCo): Hype for Types is taught by: Alsuhaibani, Battleman, Simkin, O'Flynn\n",
      "The course 98374: Student Taught Courses (StuCo): Steep by Steep: Investeagation into Tea Culture is taught by: Han, Huang, Yang, Chen\n",
      "\n",
      "questions: How many Electrical & Computer Engineering courses are going to be held in Summer 2024?\n",
      "response:\n",
      "The course 18994: Internship for Electrical and Computer Engineering MS Students is taught by: Bain\n",
      "The course 18998: Internship Three Electrical and Computer Engineering PhD Students is taught by: Bain\n",
      "The course 18997: Internship Two for Electrical and Computer Engineering PhD Students is taught by: Bain\n",
      "\n",
      "questions: In spring 2024, Who is the instructor for course 15195?\n",
      "response:\n",
      "The course 15195: Competition Programming I is taught by: Sleator\n",
      "The course 15195: Competition Programming I is taught by: Sleator\n",
      "\n",
      "questions: In Spring 2024, what time does \"Issues of Practice\" course start at in the morning?\n",
      "response:\n",
      "The course 48381: Issues of Practice is taught by: Coppedge\n",
      "The course 76836: Issues in the Discipline is taught by: Williams\n",
      "The course 80747: Global Justice is taught by: Wenner\n",
      "\n",
      "questions: Does LTI offer a course on ethics?\n",
      "response:\n",
      "The course 80130: Introduction to Ethics is taught by: Gray\n",
      "The course 46898: Ethics and Artificial Intelligence is taught by: Leben\n",
      "The course 47920: Seminar in Ethical Theory is taught by: Kim\n",
      "\n",
      "questions: In spring 2024, What is the day and time of course 17422?\n",
      "response:\n",
      "The course 17422: Building User-Focused Sensing Systems is taught by: Agarwal, Goel\n",
      "\n",
      "questions: What are the course number(s) for the NLP course?\n",
      "response:\n",
      "The course 46924: Natural Language Processing is taught by: Yurko\n",
      "The course 11611: Natural Language Processing is taught by: Mortensen, Strubell\n",
      "The course 11411: Natural Language Processing is taught by: Mortensen, Strubell\n",
      "\n",
      "questions: How many courses does Mechanical Engineering offer in Summer 2024?\n",
      "response:\n",
      "The course 24391: Mechanical Engineering Project is taught by: Hertz\n",
      "The course 24392: Mechanical Engineering Project is taught by: Hertz\n",
      "The course 18994: Internship for Electrical and Computer Engineering MS Students is taught by: Bain\n",
      "\n",
      "questions: How many units is the MIIS Capstone Project with course number 11927 for?\n",
      "response:\n",
      "The course 11927: MIIS Capstone Project is taught by: Brown\n",
      "The course 11927: MIIS Capstone Project is taught by: Instructor TBA\n",
      "The course 11690: MIIS Directed Study is taught by: Mitamura\n",
      "\n",
      "questions: In spring 2024, What is the title of course 15150?\n",
      "response:\n",
      "The course 15150: Principles of Functional Programming is taught by: Erdmann, Kaynar\n",
      "The course 15150: Principles of Functional Programming is taught by: Brookes\n",
      "The course 15150: Principles of Functional Programming is taught by: Grodin\n",
      "\n",
      "questions: In spring 2025, When do Mini-3 course drop and withdrawal grade assignment occur?\n",
      "response:\n",
      "The course 76338: Internship Mini is taught by: Instructor TBA\n",
      "The course 80496: Undergraduate Independent Study Mini is taught by: Avigad\n",
      "The course 85483: Internship in Psychology - Mini is taught by: Thiessen\n",
      "\n",
      "questions: In spring 2024, Who are the instructors for course 15122?\n",
      "response:\n",
      "The course 15122: Principles of Imperative Computation is taught by: Cervesato, Kohlbrenner\n",
      "The course 15122: Principles of Imperative Computation is taught by: Kohlbrenner\n",
      "The course 15122: Principles of Imperative Computation is taught by: Kaynar, Cervesato\n",
      "\n",
      "questions: In summer 2024, When do Mini-5 Faculty Course Evaluations open?\n",
      "response:\n",
      "The course 03445: Undergraduate Research is taught by: Willard\n",
      "The course 17685: Dynamic Network Analysis is taught by: Carley\n",
      "The course 54495: Internship is taught by: Instructor TBA\n",
      "\n",
      "questions: In fall 2023, When are the Semester & Mini-2 Faculty Course Evaluations closed?\n",
      "response:\n",
      "The course 85510: Research in Psychology - Mini is taught by: Thiessen\n",
      "The course 06200: Sophomore Research Project is taught by: Robinson\n",
      "The course 06700: M.S. Research is taught by: Robinson\n",
      "\n",
      "questions: What are the codes/numbers of the distinct courses, all titled \"Introduction to Computer Systems\", that will be offered in the Summer of 2024?\n",
      "response:\n",
      "The course 15513: Introduction to Computer Systems is taught by: Railing\n",
      "The course 15213: Introduction to Computer Systems is taught by: Andersen, Railing, Beckmann\n",
      "The course 18213: Introduction to Computer Systems is taught by: Kesden, Kumar\n",
      "\n",
      "questions: In spring 2024, Who are the instructors for course 10615?\n",
      "response:\n",
      "The course 10615: Art and Machine Learning is taught by: Kang\n",
      "\n",
      "questions: In spring 2024, What is the title of course 10735?\n",
      "response:\n",
      "The course 10735: Responsible AI is taught by: Heidari, London\n",
      "\n",
      "questions: In spring 2024, What is the course number for Independent Study: Research?\n",
      "response:\n",
      "The course 48599: Undergraduate Independent Study is taught by: Yang\n",
      "The course 95921: Independent Study is taught by: Zhu\n",
      "The course 95903: Independent Study is taught by: Zhang\n",
      "\n",
      "questions: What 11-6XX courses were not taught by LTI faculty in Spring 2024?\n",
      "response:\n",
      "The course 24789: Intermediate Deep Learning for Engineers is taught by: Barati Farimani\n",
      "The course 15111: College Programming and Computer Science is taught by: Stehlik\n",
      "The course 24782: Machine Learning and Artificial Intelligence for Engineers - Project is taught by: Kang\n",
      "\n",
      "questions: In spring 2024, What is the deadline for withdrawing from a semester course with a withdrawal grade assigned?\n",
      "response:\n",
      "The course 09302: Undergraduate Seminar IV is taught by: Stump\n",
      "The course 21201: Undergraduate Colloquium is taught by: Offner\n",
      "The course 80606: Decision Theory is taught by: Zollman\n",
      "\n",
      "questions: In spring 2024, Who are the instructors for course 17313?\n",
      "response:\n",
      "The course 17313: Foundations of Software Engineering is taught by: Hilton, Feo Flushing\n",
      "The course 17313: Foundations of Software Engineering is taught by: Padhye, Begel\n",
      "\n",
      "questions: In spring 2024, What is the location of course 10716?\n",
      "response:\n",
      "The course 10716: Advanced Machine Learning: Theory and Methods is taught by: Ravikumar\n",
      "\n",
      "questions: In spring 2024, Who are the instructors for course 15210?\n",
      "response:\n",
      "The course 15210: Parallel and Sequential Data Structures and Algorithms is taught by: Acar, Sleator\n",
      "The course 15210: Parallel and Sequential Data Structures and Algorithms is taught by: Garrod, Blelloch\n",
      "\n",
      "questions: In fall 2023, What is the title of course 05410?\n",
      "response:\n",
      "The course 05410: ser-Centered Research and Evaluation is taught by: Musuraca, Shen\n",
      "\n",
      "questions: In spring 2024, How many units is course 15112 worth?\n",
      "response:\n",
      "The course 15112: Fundamentals of Programming and Computer Science is taught by: Taylor, Kosbie\n",
      "The course 15112: Fundamentals of Programming and Computer Science is taught by: Taylor, Virtue\n",
      "The course 15112: Fundamentals of Programming and Computer Science is taught by: Sands\n",
      "\n",
      "questions: In spring 2024, What is the title of course 15195?\n",
      "response:\n",
      "The course 15195: Competition Programming I is taught by: Sleator\n",
      "The course 15195: Competition Programming I is taught by: Sleator\n",
      "\n",
      "questions: What course did Lanni teach in Spring 2023?\n",
      "response:\n",
      "The course 80249: AI, Society, and Humanity is taught by: Leben\n",
      "The course 49705: It Depends: An Inquiry into Innovative Thinking is taught by: Marchetti\n",
      "The course 79234: Technology and Society is taught by: Laemmli\n",
      "\n",
      "questions: In spring 2024, Who is the instructor for course 10403?\n",
      "response:\n",
      "The course 10403: Deep Reinforcement Learning & Control is taught by: Fragkiadaki\n",
      "\n",
      "questions: What are the two courses that are prerequisities for the undergraduate concentration termed the LT concentration? Include the title and course number in parentheses.\n",
      "response:\n",
      "The course 09207: Techniques in Quantitative Analysis is taught by: Botcha\n",
      "The course 09221: Laboratory I:  Introduction to Chemical Analysis is taught by: Sherwood\n",
      "The course 24635: Structural Analysis is taught by: Thompson\n",
      "\n",
      "questions: In spring 2024, Who are the instructors for course 15112?\n",
      "response:\n",
      "The course 15112: Fundamentals of Programming and Computer Science is taught by: Taylor, Kosbie\n",
      "The course 15112: Fundamentals of Programming and Computer Science is taught by: Taylor, Virtue\n",
      "The course 15112: Fundamentals of Programming and Computer Science is taught by: Sands\n",
      "\n",
      "questions: In spring 2024, What is the title of course 17416?\n",
      "response:\n",
      "The course 17416: AI Governance: Identifying & Mitigating Risks in Design & Dev of AI Solutions is taught by: Sadeh\n",
      "\n",
      "questions: In summer 2024, When is the deadline for withdrawing from a Mini-5 course and receiving a withdrawal grade?\n",
      "response:\n",
      "The course 84505: Undergraduate Internship is taught by: Half\n",
      "The course 33495: Internship is taught by: Dodelson\n",
      "The course 09445: Undergraduate Research is taught by: Stump\n",
      "\n",
      "questions: How many Chemical Engineering courses are going to be held in Summer 2024?\n",
      "response:\n",
      "The course 06600: Masters Chemical Engineering Project is taught by: Laird\n",
      "The course 06364: Chemical Reaction Engineering is taught by: Jen\n",
      "The course 06100: Introduction to Chemical Engineering is taught by: Beckwith\n",
      "\n",
      "questions: What is the room number for the Advanced Natural Language Processing course?\n",
      "response:\n",
      "The course 11711: Advanced Natural Language Processing is taught by: Frederking, Fried\n",
      "The course 11611: Natural Language Processing is taught by: Mortensen, Strubell\n",
      "The course 11411: Natural Language Processing is taught by: Mortensen, Strubell\n",
      "\n",
      "questions: What is the location of the courses that will be taught by Affara in Summer 2024?\n",
      "response:\n",
      "The course 70050: Study Abroad is taught by: Farooqi\n",
      "The course 76459: Transnational Feminisms is taught by: Aguiar\n",
      "The course 70417: Topics in Entrepreneurship is taught by: Hakim\n",
      "\n",
      "questions: In spring 2024, What is the title of course 17437?\n",
      "response:\n",
      "The course 17437: Web Application Development is taught by: Lee, Eppinger\n",
      "The course 17437: Web Application Development is taught by: Eppinger\n",
      "\n",
      "questions: In spring 2024, What is the day and time of course 17445-A?\n",
      "response:\n",
      "The course 17445: Machine Learning in Production is taught by: Kaestner, Kang\n",
      "\n",
      "questions: In spring 2024, What is the title of course 15210?\n",
      "response:\n",
      "The course 15210: Parallel and Sequential Data Structures and Algorithms is taught by: Acar, Sleator\n",
      "The course 15210: Parallel and Sequential Data Structures and Algorithms is taught by: Garrod, Blelloch\n",
      "\n",
      "questions: What is the course name/title for CMU 03128? \n",
      "response:\n",
      "The course 24888: Introduction to Deep Learning is taught by: Barati Farimani\n",
      "The course 14832: Cyber Forensics Capstone is taught by: Leontiadis\n",
      "The course 84390: Social Media, Technology, and Conflict is taught by: Marcellino\n",
      "\n",
      "questions: In spring 2024, What is the day and time of course 17413?\n",
      "response:\n",
      "The course 17413: Software Engineering Practicum is taught by: Timperley\n",
      "The course 17413: Software Engineering Practicum is taught by: Schmerl, Bloch\n",
      "\n",
      "questions: What Psychology course in Summer 2024 will be offered at Doha, Qatar? \n",
      "response:\n",
      "The course 85364: Environmental Psychology is taught by: Bruder\n",
      "The course 85350: Psychology of Prejudice is taught by: Crittenden\n",
      "The course 98351: Student Taught Courses (StuCo): Beyond Physical: The Psychology of Athletes is taught by: Mohamed\n",
      "\n",
      "questions: When does the Fall 2024 course registeration start for masters students?\n",
      "response:\n",
      "The course 04900: MSIT Practicum is taught by: Mugume\n",
      "The course 24794: Master of Science Research is taught by: Brown, Hertz\n",
      "The course 24794: Master of Science Research is taught by: Hertz, Brown\n",
      "\n",
      "questions: In spring 2024, What is the day and time of course 17604-C?\n",
      "response:\n",
      "The course 17604: Communications for Software Leaders II is taught by: Frollini\n",
      "\n",
      "questions: In fall 2023, What is the title of course 05431?\n",
      "response:\n",
      "The course 05431: Software Structures for User Interfaces is taught by: Hudson\n",
      "\n",
      "questions: In spring 2024, What is the course number for Generative AI?\n",
      "response:\n",
      "The course 10423: Generative AI is taught by: Gormley, Li, Lipton\n",
      "The course 10623: Generative AI is taught by: Gormley, Li, Lipton\n",
      "The course 94844: Generative AI Lab is taught by: Li\n",
      "\n",
      "questions: In spring 2024, When is the final deadline for withdrawing from a Mini-4 course?\n",
      "response:\n",
      "The course 73230: Intermediate Microeconomics is taught by: Nguyen\n",
      "The course 57184: Solfege IV is taught by: Young\n",
      "The course 73240: Intermediate Macroeconomics is taught by: Shi\n",
      "\n",
      "questions: In spring 2024, Who are the instructors for course 17514?\n",
      "response:\n",
      "The course 17514: Principles of Software Construction: Objects, Design, and Concurrency is taught by: Aldrich, Garrod, Lacomis\n",
      "The course 17514: Principles of Software Construction: Objects, Design, and Concurrency is taught by: Kaestner, Lacomis\n",
      "\n",
      "questions: In spring 2024, What is the title of course 10301?\n",
      "response:\n",
      "The course 10301: Introduction to Machine Learning is taught by: Hoda Heidari and Henry Chai and Matt Gormley\n",
      "\n",
      "questions: When does the Fall 2024 course registeration start for doctoral students?\n",
      "response:\n",
      "The course 24992: Professional Development for PhD students II is taught by: Wolfe\n",
      "The course 03900: Doctoral Thesis Research is taught by: Hinman, Hiller\n",
      "The course 03900: Doctoral Thesis Research is taught by: Hinman\n",
      "\n",
      "questions: In spring 2024, What is the title of course 10601?\n",
      "response:\n",
      "The course 10601: Introduction to Machine Learning (Master's) is taught by: Chai, Gormley, Heidari\n",
      "The course 10601: Introduction to Machine Learning (Master's) is taught by: Chai, Gormley\n",
      "The course 10601: Introduction to Machine Learning (Master's) is taught by: Chai\n",
      "\n",
      "questions: Who is teaching the Multimodal Machine Learning course this semester?\n",
      "response:\n",
      "The course 11777: Multimodal Machine Learning is taught by: Morency\n",
      "The course 11877: Advanced Topics in Multimodal Machine Learning is taught by: Liang, Fried\n",
      "The course 11777: Multimodal Machine Learning is taught by: Bisk\n",
      "\n",
      "questions: In summer 2024, What is the deadline for withdrawing from a Semester course and receiving a withdrawal grade?\n",
      "response:\n",
      "The course 18580: Undergraduate Projects is taught by: Bain\n",
      "The course 03445: Undergraduate Research is taught by: Willard\n",
      "The course 03445: Undergraduate Research is taught by: Willard\n",
      "\n",
      "questions: In fall 2023, When is the deadline to drop a Mini-1 course with a withdrawal grade assigned?\n",
      "response:\n",
      "The course 88397: SDS Undergraduate Research - mini is taught by: Oppenheimer\n",
      "The course 88951: Dissertation is taught by: Miller\n",
      "The course 93889: Capstone Management Project I is taught by: Green\n",
      "\n",
      "questions: In spring 2024, What is the title of course 17634?\n",
      "response:\n",
      "The course 17634: Applied Machine Learning is taught by: Worrell, Scanlon\n",
      "\n",
      "questions: In spring 2024, Who are the instructors for course 15150?\n",
      "response:\n",
      "The course 15150: Principles of Functional Programming is taught by: Erdmann, Kaynar\n",
      "The course 15150: Principles of Functional Programming is taught by: Brookes\n",
      "The course 15150: Principles of Functional Programming is taught by: Grodin\n",
      "\n",
      "questions: When was the buggy course laid out in lanes for the first time?\n",
      "response:\n",
      "The course 79269: Russian History: From Socialism to Capitalism is taught by: Goldman\n",
      "The course 48241: History of Modern Architecture is taught by: Gutschow\n",
      "The course 57665: History of the Piano is taught by: Stumpf\n",
      "\n",
      "questions: What is the title of LTI's text mining course?\n",
      "response:\n",
      "The course 11441: Machine Learning for Text and Graph-based Mining is taught by: Yang\n",
      "The course 11741: Machine Learning for Text and Graph-based Mining is taught by: Yang\n",
      "The course 11641: Machine Learning for Text and Graph-based Mining is taught by: Yang\n",
      "\n",
      "questions: In spring 2024, When do Mini-4 faculty course evaluations close?\n",
      "response:\n",
      "The course 17634: Applied Machine Learning is taught by: Worrell, Scanlon\n",
      "The course 15494: Cognitive Robotics: The Future of Robot Toys is taught by: Touretzky\n",
      "The course 09302: Undergraduate Seminar IV is taught by: Stump\n",
      "\n",
      "questions: What is the title of the ethics course offered at LTI?\n",
      "response:\n",
      "The course 80130: Introduction to Ethics is taught by: Gray\n",
      "The course 46898: Ethics and Artificial Intelligence is taught by: Leben\n",
      "The course 47920: Seminar in Ethical Theory is taught by: Kim\n",
      "\n",
      "questions: How many courses are offered by BXA Intercollege Degree Programs in Spring 2024 (exclude the BXA Studio courses)?\n",
      "response:\n",
      "The course 57499: BXA Studio (4th semester) is taught by: Aylmer\n",
      "The course 57497: BXA Studio (2nd semester) is taught by: Aylmer\n",
      "The course 57498: BXA Studio (3rd semester) is taught by: Aylmer\n",
      "\n",
      "questions: In spring 2024, What is the day and time of course 17645-F?\n",
      "response:\n",
      "The course 17645: Machine Learning in Production is taught by: Kaestner, Kang\n",
      "\n",
      "questions: In fall 2023, What is the title of course 05360?\n",
      "response:\n",
      "The course 05360: Interaction Design Fundamentals is taught by: Matthews, Saffer\n",
      "\n",
      "questions: If you wanted to take Arts & Community Development in Fall 2023, what were the course numbers of the courses offered?\n",
      "response:\n",
      "The course 62830: Disruptive Technologies in Arts Enterprises is taught by: Crawford\n",
      "The course 93830: Disruptive Technologies in Arts Enterprises is taught by: Crawford\n",
      "The course 60387: Critical Studies: Curating as Artistic Practice is taught by: Ragona\n",
      "\n",
      "questions: What are the 4 common MCDS core courses? List them in the following format: course number - title; course number - title; ...\n",
      "response:\n",
      "The course 38402: MCS Leadership Development Seminar is taught by: Hannon, Murphy\n",
      "The course 95736: Advanced Relational Database Management is taught by: Trzeciak\n",
      "The course 38402: MCS Leadership Development Seminar is taught by: Hannon\n",
      "\n",
      "questions: In spring 2024, What is the title of course 17537?\n",
      "response:\n",
      "The course 17537: Artificial Intelligence Methods for Social Good is taught by: Fang\n",
      "\n",
      "questions: In spring 2024, What is the title of course 15050?\n",
      "response:\n",
      "The course 15050: Functional Programming is taught by: Michael Erdmann and Dilsun Kaynar\n",
      "\n",
      "questions: In spring 2024, What is the deadline for adding or dropping a Mini-3 course with tuition adjustment?\n",
      "response:\n",
      "The course 73230: Intermediate Microeconomics is taught by: Nguyen\n",
      "The course 47802: Microeconomics III is taught by: Shourideh\n",
      "The course 47805: Macroeconomics III is taught by: Shi\n",
      "\n",
      "questions: When does the Spring 2025 course registeration start for sophomores?\n",
      "response:\n",
      "The course 03202: Undergraduate Colloquium for Sophomores is taught by: Younis\n",
      "The course 57596: Sophomore Recital is taught by: Cardenes\n",
      "The course 06200: Sophomore Research Project is taught by: Robinson\n",
      "\n",
      "questions: When did buggy rules change to include a permanent driver and four pushers along the course?\n",
      "response:\n",
      "The course 18744: Autonomous Driving is taught by: Rajkumar\n",
      "The course 16663: F1Tenth Autonomous Racing is taught by: Dolan\n",
      "The course 15414: Bug Catching: Automated Program Verification is taught by: Martins\n",
      "\n",
      "questions: In spring 2024, How many units is course 10605?\n",
      "response:\n",
      "The course 10605: Machine Learning with Large Datasets is taught by: Gordon, Talwalkar\n",
      "The course 10605: Machine Learning with Large Datasets is taught by: Talwalkar, Gordon\n",
      "\n",
      "questions: Who propels a buggy via a pushbar along one of the five hills of the buggy course?\n",
      "response:\n",
      "The course 24659: Mechanics Modeling Guided by Fundamentals is taught by: Beuth\n",
      "The course 24673: Soft Robots: Mechanics, Design and Modeling is taught by: Majidi\n",
      "The course 16665: Robot Mobility on Air, Land, & Sea is taught by: Apostolopoulos, Dolan, Geyer, Kaess, Shi\n",
      "\n",
      "questions: In fall 2023, What is the title of course 05318?\n",
      "response:\n",
      "The course 05318: Human AI Interaction is taught by: Zhu\n",
      "\n",
      "questions: In fall 2023, Who are the instructors for course 05380?\n",
      "response:\n",
      "The course 05380: Prototyping Algorithmic Experiences is taught by: Holstein\n",
      "\n",
      "questions: In fall, who were the instructors for the Introduction to Deep Learning course at LTI?\n",
      "response:\n",
      "The course 11785: Introduction to Deep Learning is taught by: Ramakrishnan, Singh\n",
      "The course 11485: Introduction to Deep Learning is taught by: Ramakrishnan, Singh\n",
      "The course 11685: Introduction to Deep Learning is taught by: Singh, Ramakrishnan\n",
      "\n",
      "questions: In spring 2024, What is the title of course 17356?\n",
      "response:\n",
      "The course 17356: Software Engineering for Startups is taught by: Wright, Brown\n",
      "\n",
      "questions: How many courses is Sindi teaching in Spring 2024?\n",
      "response:\n",
      "The course 60433: Advanced SIS: Ceramics is taught by: Sekino-Bove\n",
      "The course 24788: Introduction to Deep Learning is taught by: Barati Farimani\n",
      "The course 15445: Database Systems is taught by: Patel\n",
      "\n",
      "questions: Who is teaching the question answering course at LTI?\n",
      "response:\n",
      "The course 11797: Question Answering is taught by: Mitamura, Nyberg\n",
      "The course 11411: Natural Language Processing is taught by: Mortensen, Strubell\n",
      "The course 46924: Natural Language Processing is taught by: Yurko\n",
      "\n",
      "questions: Who is the main instructor for the search engines course?\n",
      "response:\n",
      "The course 11642: Search Engines is taught by: Callan\n",
      "The course 11442: Search Engines is taught by: Callan\n",
      "The course 11642: Search Engines is taught by: Callan\n",
      "\n",
      "questions: In spring 2024, When is the final deadline for withdrawing from a Mini-3 course?\n",
      "response:\n",
      "The course 47802: Microeconomics III is taught by: Shourideh\n",
      "The course 73230: Intermediate Microeconomics is taught by: Nguyen\n",
      "The course 60474: Advanced DP3: Photobook is taught by: Beck\n",
      "\n",
      "questions: Which independent organization set a course record of 2:06.20 in 1988 buggy?\n",
      "response:\n",
      "The course 49796: Software Management Independent Study is taught by: Fang, Mercier, Shaikh\n",
      "The course 49790: Software Management Independent Study is taught by: Mercier\n",
      "The course 04980: Engineering Independent Study is taught by: Gueye\n",
      "\n",
      "questions: Does LTI offer a course on large language models?\n",
      "response:\n",
      "The course 11667: Large Language Models Methods and Application is taught by: Ippolito, Xiong\n",
      "The course 11868: Large Language Model Systems is taught by: Li\n",
      "The course 46924: Natural Language Processing is taught by: Yurko\n",
      "\n",
      "questions: In fall 2023, What is the course title for unit 02090?\n",
      "response:\n",
      "The course 02090: Computational Biology UndergraduatInternship is taught by: Compeau\n",
      "\n",
      "questions: In fall 2023, Who are the instructors for course 05430?\n",
      "response:\n",
      "The course 05430: Programming Usable Interfaces is taught by: Das\n",
      "\n",
      "questions: Which section of the freeroll portion of the buggy course do buggies make a sharp right-hand turn?\n",
      "response:\n",
      "The course 16711: Kinematics, Dynamic Systems and Control is taught by: Geyer\n",
      "The course 18744: Autonomous Driving is taught by: Rajkumar\n",
      "The course 16663: F1Tenth Autonomous Racing is taught by: Dolan\n",
      "\n",
      "questions: In fall 2023, Who is the instructor for course 05432?\n",
      "response:\n",
      "The course 05432: Personalized Online Learning is taught by: Aleven\n",
      "\n",
      "questions: In fall 2023, What is the course title for unit 02801?\n",
      "response:\n",
      "The course 02801: Computational Biology Internship is taught by: Kingsford\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## People"
   ],
   "metadata": {
    "id": "8-7JjNMOsg6n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"knowledge_source_pd/faculty/faculty_info.csv\")\n",
    "print(df.shape)\n",
    "print(df.isna().sum())\n",
    "# df = df.fillna('')\n",
    "df.head(1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "3J2EHC5kslNc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710382275315,
     "user_tz": 240,
     "elapsed": 4,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "48372767-cd39-43b9-ab21-f542d7e858c9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(32, 10)\n",
      "Name              0\n",
      "Title             0\n",
      "Office            3\n",
      "Email             0\n",
      "Phone            10\n",
      "Research Area    11\n",
      "Research         19\n",
      "Projects         31\n",
      "Bio              31\n",
      "Education        28\n",
      "dtype: int64\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           Name                                              Title  \\\n",
       "0  Yonatan Bisk  Assistant Professor, Language Technologies Ins...   \n",
       "\n",
       "                         Office             Email Phone  \\\n",
       "0  6703 Gates & Hillman Centers  ybisk@cs.cmu.edu   NaN   \n",
       "\n",
       "                                       Research Area  \\\n",
       "0  Embodiment, Grounding, RoboNLP, Unsupervised L...   \n",
       "\n",
       "                                            Research Projects  Bio Education  \n",
       "0  My work broadly falls into: 1. Uncovering the ...      NaN  NaN       NaN  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-2aae404c-460c-4583-9050-f1e4fd0c3788\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Office</th>\n",
       "      <th>Email</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Research Area</th>\n",
       "      <th>Research</th>\n",
       "      <th>Projects</th>\n",
       "      <th>Bio</th>\n",
       "      <th>Education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yonatan Bisk</td>\n",
       "      <td>Assistant Professor, Language Technologies Ins...</td>\n",
       "      <td>6703 Gates &amp; Hillman Centers</td>\n",
       "      <td>ybisk@cs.cmu.edu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Embodiment, Grounding, RoboNLP, Unsupervised L...</td>\n",
       "      <td>My work broadly falls into: 1. Uncovering the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2aae404c-460c-4583-9050-f1e4fd0c3788')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-2aae404c-460c-4583-9050-f1e4fd0c3788 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-2aae404c-460c-4583-9050-f1e4fd0c3788');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 32,\n  \"fields\": [\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"Eric P. Xing\",\n          \"David Mortensen\",\n          \"Rita Singh\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"Assistant Professor, Language Technologies Institute\",\n          \"Senior Systems Scientist/Chair of Admissions, Language Technologies Institute\",\n          \"Associate Professor, Language Technologies Institute\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Office\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"3525 Newell-Simon Hall\",\n          \"6405 Gates & Hillman Centers\",\n          \"5519 Gates & Hillman Centers\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"epxing@andrew.cmu.edu\",\n          \"dmortens@cs.cmu.edu\",\n          \"rsingh@cs.cmu.edu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Phone\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"412-268-8298\",\n          \"412-268-9826\",\n          \"412-268-6355\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Research Area\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          \"Embodiment, Grounding, RoboNLP, Unsupervised Learning, Vision and Language\",\n          \"Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing\",\n          \"Machine Learning, Multimodal Computing and Interaction, Privacy and Security, Speech Processing, Spoken Interfaces and Dialogue Processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Research\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"The major theme of Professor Xing's research lies in the development of machine learning and statistical methodology; especially for building quantitative models and predictive understandings of the evolutionary mechanism, regulatory circuitry, and developmental processes of biological systems; and for building intelligent systems for a wide range of applications in vision, IR and NLP that involves computational learning and reasoning under uncertainty.Foundations of Statistical Learning\\n, including theory and algorithms for: 1) Time/space varying-coefficient models with evolving structures; 2) Sparse structured input/output models in high-dimensional problems; 3) Nonparametric Bayesian techniques for infinite-dimensional models; 4) RKHS embedding, nonparametric inference, and spectral methods for graphical models; 5) Distributed and online algorithms for optimization, approximate inference, and sampling on massive data.Large-scale Information & Intelligent System:\\n1) Development of scalable parallel architecture, protocol, programming interface, generic algorithms and models, for Big Learning; 2) Multi-view latent space models, topics models, and sparse coding for image/text/relational data mining; 3) Evolving structure, stable metrics, and prediction for dynamic social networks, goal-driven network design and optimization; 4) Web-scale image understanding, search, prediction, and storyline synthesis; 5) Information visualization, indexing and storage, web/mobile app development.Computational Biology:\\n1) Understanding genome-microenvironment interactions in cancer and embryogenesis via joint analysis of genomic, proteomic, and pathway signaling data; 2) Genetic analysis of population variation, demography and evolution; 3) Statistical inference of genome-transcriptome-phenome association in complex diseases; 4) Personalized diagnosis and treatment of spectrum diseases via next generation sequencing and computational \\\"omic\\\" analysis; 5) Biological image and text mining.\",\n          \"My research focuses on better understanding the social and pragmatic nature of conversation, and using this understanding to build computational systems that improve the efficacy of conversation both between people, and between people and computers. In order to pursue these goals, I invoke approaches from computational discourse analysis and text mining, conversational agents, and computer-supported collaborative learning. I ground my research in the fields of language technologies and human-computer interaction, and am fortunate to work closely with students and post-docs from the LTI\\u00a0and the\\nHuman-Computer Interaction Institute\\n, as well as to direct my own lab,\\nTELEDIA\\n. My group\\u2019s highly interdisciplinary work, published in 160 peer-reviewed publications, is represented in the top venues in five fields: language technologies, learning sciences, cognitive science, educational technology and human-computer interaction.An exciting direction of my group's work is spearheading a satellite working group to support social interaction for learning in MOOCs with EdX called\\nDANCE\\n. My research toward this end has birthed and substantially contributed to the growth of two thriving interrelated research areas: automated analysis of collaborative learning processes and dynamic support for collaborative learning. Both areas use intelligent conversational agents to support collaborative learning in a context-sensitive way.All of my work draws insight from rich theoretical models from sociolinguistics and discourse analysis, and pares them down to precise operationalizations that capture the most important essence for achieving impact. I always start by investigating how conversation works and formalizing this understanding in models that are precise enough to be reproducible and that demonstrate explanatory power in connection with outcomes with real-world value. The next step is to adapt, extend and apply machine learning and text mining technologies that leverage this deep understanding to build computational models capable of automatically applying these constructs to naturally occurring language interactions. Finally, with the technology to automatically monitor naturalistic language communication in place, we can build interventions with real-world benefits.This approach leads to three aspects included in each project:Basic research on discourse analysis\\nto identify conversational constructs that predict important group outcomes such as learning, knowledge transfer or motivation.\\nBasic research on text classification technology\\nfor automated analysis of conversational constructs identified under research on discourse analysis, as well as tools to enable other researchers to do the same.\\nBasic research on conversational agent technology and summarization\\nthat eases development of interventions triggered by automatic analyses from basic research on text classification that either enables human facilitators to offer support, directly provide feedback to groups or influence group participation in positive ways.\",\n          \"My work broadly falls into: 1. Uncovering the latent structures of natural language, 2. Modeling the semantics of the physical world, and 3. Connecting language to perception and control.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Projects\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Lemur: The Lemur Project develops open-source search engines, toolbars, text analysis tools, search services and datasets that support international research and development. The project is best known for its Indri and Galago search engines, and large-scale ClueWeb datasets. Our software and datasets are widely used in scientific and research applications, and some commercial applications. Lemur's software development philosophy emphasizes state-of-the-art accuracy, flexibility and efficiency.Search Engines With Knowledge Resources: This project develops new methods for using knowledge graphs and ontologies to improve search engine accuracy, especially for vague, ambiguous or poorly specified queries. Knowledge graphs and ontologies are less structured than typical relational databases and semantic web resources, but more structured than text stored in full-text search engines. The weak semantics in these semi-structured information resources can support interesting applications, but can also accommodate contradictions, inconsistencies and mistakes \\u2014 making them easier to scale for large amounts of information. A search engine can use these resources to identify the probable meanings of query terms, and use this knowledge to identify documents that match those meanings.Retrieval of Scientific Data: Numerical data continues to expand as the results of scholarly research in data-rich sciences (e.g., non-textual data) continue to grow. This project extends search engine architectures to support large, centralized, universal repositories of affordable and easily used scientific data. Our goal is to access tabular, numeric and other non-textual information as easily and readily as documents without laborious additional work.Selective and Federated Search: I have a long-term interest in environments that contain numerous search engines. Much of my prior research focused on integrating many independent search engines \\u2014 perhaps operated by different organizations with different interests\\u2014 into a single integrated federated search system. My recent work investigates a related problem: decomposing a massive text collection into hundreds or thousands of small search engines designed to have skewed utility distributions that enable most index partitions to be ignored for most queries. This\\nselective search\\narchitecture is as effective as conventional search engine architectures, but has far lower computational costs and reveals new challenges and opportunities in large-scale search. The decomposition process creates text collections, thus inviting research on the characteristics desired or to be avoided in a text collection to enable accurate search. We've developed new resource selection algorithms to address efficiency problems in existing algorithms and dynamically adjust search costs based on query difficulty. Our goal is an easily customizable and extensible off-the-shelf method that provides an order of magnitude reduction in search costs over the current state-of-the-art, especially on corpora of more than a billion documents.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bio\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Dr. Alexander Waibel is a Professor of Computer Science at Carnegie Mellon University, Pittsburgh and at the Karlsruhe Institute of Technology, Germany. He is the director of the International Center for Advanced Communication Technologies (interACT). The Center works in a network with eight of the world\\u2019s top research institutions. The Center\\u2019s mission is to develop advanced machine learning algorithms to improve human-human and human-machine communication technologies. \\u00a0Prof. Waibel and his team developed many statistical and neural network learning \\u00a0algorithms that made such communication breakthroughs possible. Most notably, the \\u201cTime-Delay Neural Network\\u201d (1987) (the first \\u201cconvolutional\\u201d neural network) now is at the heart of many of today\\u2019s AI technologies. System breakthroughs at Waibel\\u2019s lab included early multimodal interfaces, speech and language interfaces, the first speech translation system in Europe & USA (1990/1991), the first simultaneous lecture translation system (2005), and Jibbigo, the first commercial speech translator on a phone (2009).Dr. Waibel founded and served as chairmen of C-STAR, the Consortium for Speech Translation Advanced Research in 1991. He directed many research programs in speech, translation, multimodal interfaces and machine learning in the US, Europe and Asia. He served as director of EU-Bridge (2012-2015) and CHIL (2004-2007), two large European multi-site Integrated Project initiatives on intelligent assistants and speech translation services. He also was co-director of IMMI, a joint venture between KIT, CNRS & RWTH.Dr. Waibel is an IEEE Fellow and received many awards for his work on multilingual and multimodal communication \\u00a0and translation. He published extensively (>750 publications, >25,000 citations, h-index 80) in the field and received/filed numerous patents. Waibel was elected to the National Academy of Sciences of Germany in 2017, and was named Honorary Senator of the Hochschulrektorenkonferenz (Representation of German Universities).During his career, Dr. Waibel founded and built ten successful companies. Following the acquisition of Jibbigo by Facebook, Waibel served as founding director of the Language Technology Group at FB. He also deployed speech translation technologies in humanitarian and disaster relief missions. His team recently deployed the first simultaneous interpretation service for lectures at Universities and interpretation tools at the European Parliament.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Education\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Dr. Waibel received his BS, MS and PhD degrees at Massachusetts Institute of Technology and CMU, respectively.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 111
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df['faculty_info'] = df.apply(lambda row: f\"The faculty name: {row['Name']} with title: {row['Title']} has research area\t: {row['Research Area']}\", axis=1)"
   ],
   "metadata": {
    "id": "X76dqQeauX3z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.head(1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "iB5ykSHQuu5Q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710382316948,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "bf94914a-e293-40fc-ecd2-fc598200f8fc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           Name                                              Title  \\\n",
       "0  Yonatan Bisk  Assistant Professor, Language Technologies Ins...   \n",
       "\n",
       "                         Office             Email Phone  \\\n",
       "0  6703 Gates & Hillman Centers  ybisk@cs.cmu.edu   NaN   \n",
       "\n",
       "                                       Research Area  \\\n",
       "0  Embodiment, Grounding, RoboNLP, Unsupervised L...   \n",
       "\n",
       "                                            Research Projects  Bio Education  \\\n",
       "0  My work broadly falls into: 1. Uncovering the ...      NaN  NaN       NaN   \n",
       "\n",
       "                                        faculty_info  \n",
       "0  The faculty name: Yonatan Bisk with title: Ass...  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-80a62fde-36fc-4a41-8c08-fbd4a94bccee\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Office</th>\n",
       "      <th>Email</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Research Area</th>\n",
       "      <th>Research</th>\n",
       "      <th>Projects</th>\n",
       "      <th>Bio</th>\n",
       "      <th>Education</th>\n",
       "      <th>faculty_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yonatan Bisk</td>\n",
       "      <td>Assistant Professor, Language Technologies Ins...</td>\n",
       "      <td>6703 Gates &amp; Hillman Centers</td>\n",
       "      <td>ybisk@cs.cmu.edu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Embodiment, Grounding, RoboNLP, Unsupervised L...</td>\n",
       "      <td>My work broadly falls into: 1. Uncovering the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The faculty name: Yonatan Bisk with title: Ass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80a62fde-36fc-4a41-8c08-fbd4a94bccee')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-80a62fde-36fc-4a41-8c08-fbd4a94bccee button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-80a62fde-36fc-4a41-8c08-fbd4a94bccee');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 32,\n  \"fields\": [\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"Eric P. Xing\",\n          \"David Mortensen\",\n          \"Rita Singh\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"Assistant Professor, Language Technologies Institute\",\n          \"Senior Systems Scientist/Chair of Admissions, Language Technologies Institute\",\n          \"Associate Professor, Language Technologies Institute\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Office\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"3525 Newell-Simon Hall\",\n          \"6405 Gates & Hillman Centers\",\n          \"5519 Gates & Hillman Centers\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"epxing@andrew.cmu.edu\",\n          \"dmortens@cs.cmu.edu\",\n          \"rsingh@cs.cmu.edu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Phone\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"412-268-8298\",\n          \"412-268-9826\",\n          \"412-268-6355\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Research Area\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          \"Embodiment, Grounding, RoboNLP, Unsupervised Learning, Vision and Language\",\n          \"Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing\",\n          \"Machine Learning, Multimodal Computing and Interaction, Privacy and Security, Speech Processing, Spoken Interfaces and Dialogue Processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Research\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"The major theme of Professor Xing's research lies in the development of machine learning and statistical methodology; especially for building quantitative models and predictive understandings of the evolutionary mechanism, regulatory circuitry, and developmental processes of biological systems; and for building intelligent systems for a wide range of applications in vision, IR and NLP that involves computational learning and reasoning under uncertainty.Foundations of Statistical Learning\\n, including theory and algorithms for: 1) Time/space varying-coefficient models with evolving structures; 2) Sparse structured input/output models in high-dimensional problems; 3) Nonparametric Bayesian techniques for infinite-dimensional models; 4) RKHS embedding, nonparametric inference, and spectral methods for graphical models; 5) Distributed and online algorithms for optimization, approximate inference, and sampling on massive data.Large-scale Information & Intelligent System:\\n1) Development of scalable parallel architecture, protocol, programming interface, generic algorithms and models, for Big Learning; 2) Multi-view latent space models, topics models, and sparse coding for image/text/relational data mining; 3) Evolving structure, stable metrics, and prediction for dynamic social networks, goal-driven network design and optimization; 4) Web-scale image understanding, search, prediction, and storyline synthesis; 5) Information visualization, indexing and storage, web/mobile app development.Computational Biology:\\n1) Understanding genome-microenvironment interactions in cancer and embryogenesis via joint analysis of genomic, proteomic, and pathway signaling data; 2) Genetic analysis of population variation, demography and evolution; 3) Statistical inference of genome-transcriptome-phenome association in complex diseases; 4) Personalized diagnosis and treatment of spectrum diseases via next generation sequencing and computational \\\"omic\\\" analysis; 5) Biological image and text mining.\",\n          \"My research focuses on better understanding the social and pragmatic nature of conversation, and using this understanding to build computational systems that improve the efficacy of conversation both between people, and between people and computers. In order to pursue these goals, I invoke approaches from computational discourse analysis and text mining, conversational agents, and computer-supported collaborative learning. I ground my research in the fields of language technologies and human-computer interaction, and am fortunate to work closely with students and post-docs from the LTI\\u00a0and the\\nHuman-Computer Interaction Institute\\n, as well as to direct my own lab,\\nTELEDIA\\n. My group\\u2019s highly interdisciplinary work, published in 160 peer-reviewed publications, is represented in the top venues in five fields: language technologies, learning sciences, cognitive science, educational technology and human-computer interaction.An exciting direction of my group's work is spearheading a satellite working group to support social interaction for learning in MOOCs with EdX called\\nDANCE\\n. My research toward this end has birthed and substantially contributed to the growth of two thriving interrelated research areas: automated analysis of collaborative learning processes and dynamic support for collaborative learning. Both areas use intelligent conversational agents to support collaborative learning in a context-sensitive way.All of my work draws insight from rich theoretical models from sociolinguistics and discourse analysis, and pares them down to precise operationalizations that capture the most important essence for achieving impact. I always start by investigating how conversation works and formalizing this understanding in models that are precise enough to be reproducible and that demonstrate explanatory power in connection with outcomes with real-world value. The next step is to adapt, extend and apply machine learning and text mining technologies that leverage this deep understanding to build computational models capable of automatically applying these constructs to naturally occurring language interactions. Finally, with the technology to automatically monitor naturalistic language communication in place, we can build interventions with real-world benefits.This approach leads to three aspects included in each project:Basic research on discourse analysis\\nto identify conversational constructs that predict important group outcomes such as learning, knowledge transfer or motivation.\\nBasic research on text classification technology\\nfor automated analysis of conversational constructs identified under research on discourse analysis, as well as tools to enable other researchers to do the same.\\nBasic research on conversational agent technology and summarization\\nthat eases development of interventions triggered by automatic analyses from basic research on text classification that either enables human facilitators to offer support, directly provide feedback to groups or influence group participation in positive ways.\",\n          \"My work broadly falls into: 1. Uncovering the latent structures of natural language, 2. Modeling the semantics of the physical world, and 3. Connecting language to perception and control.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Projects\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Lemur: The Lemur Project develops open-source search engines, toolbars, text analysis tools, search services and datasets that support international research and development. The project is best known for its Indri and Galago search engines, and large-scale ClueWeb datasets. Our software and datasets are widely used in scientific and research applications, and some commercial applications. Lemur's software development philosophy emphasizes state-of-the-art accuracy, flexibility and efficiency.Search Engines With Knowledge Resources: This project develops new methods for using knowledge graphs and ontologies to improve search engine accuracy, especially for vague, ambiguous or poorly specified queries. Knowledge graphs and ontologies are less structured than typical relational databases and semantic web resources, but more structured than text stored in full-text search engines. The weak semantics in these semi-structured information resources can support interesting applications, but can also accommodate contradictions, inconsistencies and mistakes \\u2014 making them easier to scale for large amounts of information. A search engine can use these resources to identify the probable meanings of query terms, and use this knowledge to identify documents that match those meanings.Retrieval of Scientific Data: Numerical data continues to expand as the results of scholarly research in data-rich sciences (e.g., non-textual data) continue to grow. This project extends search engine architectures to support large, centralized, universal repositories of affordable and easily used scientific data. Our goal is to access tabular, numeric and other non-textual information as easily and readily as documents without laborious additional work.Selective and Federated Search: I have a long-term interest in environments that contain numerous search engines. Much of my prior research focused on integrating many independent search engines \\u2014 perhaps operated by different organizations with different interests\\u2014 into a single integrated federated search system. My recent work investigates a related problem: decomposing a massive text collection into hundreds or thousands of small search engines designed to have skewed utility distributions that enable most index partitions to be ignored for most queries. This\\nselective search\\narchitecture is as effective as conventional search engine architectures, but has far lower computational costs and reveals new challenges and opportunities in large-scale search. The decomposition process creates text collections, thus inviting research on the characteristics desired or to be avoided in a text collection to enable accurate search. We've developed new resource selection algorithms to address efficiency problems in existing algorithms and dynamically adjust search costs based on query difficulty. Our goal is an easily customizable and extensible off-the-shelf method that provides an order of magnitude reduction in search costs over the current state-of-the-art, especially on corpora of more than a billion documents.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bio\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Dr. Alexander Waibel is a Professor of Computer Science at Carnegie Mellon University, Pittsburgh and at the Karlsruhe Institute of Technology, Germany. He is the director of the International Center for Advanced Communication Technologies (interACT). The Center works in a network with eight of the world\\u2019s top research institutions. The Center\\u2019s mission is to develop advanced machine learning algorithms to improve human-human and human-machine communication technologies. \\u00a0Prof. Waibel and his team developed many statistical and neural network learning \\u00a0algorithms that made such communication breakthroughs possible. Most notably, the \\u201cTime-Delay Neural Network\\u201d (1987) (the first \\u201cconvolutional\\u201d neural network) now is at the heart of many of today\\u2019s AI technologies. System breakthroughs at Waibel\\u2019s lab included early multimodal interfaces, speech and language interfaces, the first speech translation system in Europe & USA (1990/1991), the first simultaneous lecture translation system (2005), and Jibbigo, the first commercial speech translator on a phone (2009).Dr. Waibel founded and served as chairmen of C-STAR, the Consortium for Speech Translation Advanced Research in 1991. He directed many research programs in speech, translation, multimodal interfaces and machine learning in the US, Europe and Asia. He served as director of EU-Bridge (2012-2015) and CHIL (2004-2007), two large European multi-site Integrated Project initiatives on intelligent assistants and speech translation services. He also was co-director of IMMI, a joint venture between KIT, CNRS & RWTH.Dr. Waibel is an IEEE Fellow and received many awards for his work on multilingual and multimodal communication \\u00a0and translation. He published extensively (>750 publications, >25,000 citations, h-index 80) in the field and received/filed numerous patents. Waibel was elected to the National Academy of Sciences of Germany in 2017, and was named Honorary Senator of the Hochschulrektorenkonferenz (Representation of German Universities).During his career, Dr. Waibel founded and built ten successful companies. Following the acquisition of Jibbigo by Facebook, Waibel served as founding director of the Language Technology Group at FB. He also deployed speech translation technologies in humanitarian and disaster relief missions. His team recently deployed the first simultaneous interpretation service for lectures at Universities and interpretation tools at the European Parliament.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Education\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Dr. Waibel received his BS, MS and PhD degrees at Massachusetts Institute of Technology and CMU, respectively.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faculty_info\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"The faculty name: Eric P. Xing with title: Professor (On Leave), Language Technologies Institute has research area\\t: nan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 113
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions # for chroma db embed\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding # for llama index embed\n",
    "import os\n",
    "\n",
    "embed_name = \"BAAI/bge-large-en-v1.5\" # https://huggingface.co/models\n",
    "path = os.path.join('/content/drive/MyDrive/ANLP/ANLP-HW2', 'chroma_meta')\n",
    "\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=\"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\", # huggingface api\n",
    "    model_name=embed_name\n",
    ")\n",
    "\n",
    "# same embedding function as chroma index\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_name)\n",
    "\n",
    "# get the Chroma Client\n",
    "client = chromadb.PersistentClient(path=path)\n",
    "\n",
    "# create/get a collection, create_collection if first time\n",
    "collection = client.create_collection(name=\"faculty_baai\", embedding_function=huggingface_ef)\n",
    "# client.delete_collection(name=\"paper_baai\")"
   ],
   "metadata": {
    "id": "5xeCmyjMtdhs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.columns"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3s8svCs4u7eg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710382368220,
     "user_tz": 240,
     "elapsed": 2,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "91ccc8a2-e34f-4f8f-de4a-c7cc11a6c465"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Name', 'Title', 'Office', 'Email', 'Phone', 'Research Area',\n",
       "       'Research', 'Projects', 'Bio', 'Education', 'faculty_info'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# store embedding\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "\n",
    "for idx, _ in df.iterrows():\n",
    "    Name, Title, Office, Email, Phone, Research_Area, Research, \\\n",
    "        Projects, Bio, Education, faculty_info = df.iloc[idx]\n",
    "\n",
    "    node = TextNode(\n",
    "        text=(faculty_info)\n",
    "    )\n",
    "    metadata = {\n",
    "        \"Faculty Name\": Name,\n",
    "        \"Faculty Title\": Title,\n",
    "        \"Faculty Office\": Office,\n",
    "        \"Faculty Email\": Email,\n",
    "        \"Faculty Phone\": Phone,\n",
    "        \"Faculty's Research Area\": Research_Area,\n",
    "        \"Faculty's Research\": Research,\n",
    "        \"Faculty's Projects\": Projects,\n",
    "        \"Faculty's Bio\": Bio,\n",
    "        \"Faculty's Education\": Education,\n",
    "    }\n",
    "    node.metadata = metadata\n",
    "    nodes.append(node)\n",
    "\n",
    "\n",
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding\n",
    "\n",
    "\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "vector_store.add(nodes)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embed_model)"
   ],
   "metadata": {
    "id": "_KrTZctAux9f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "retriever = ChromadbRetriever(\n",
    "    collection, embed_model, query_mode=\"default\", similarity_top_k=5\n",
    ")\n",
    "\n",
    "HF_TOKEN = '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'\n",
    "\n",
    "query_str = \"What is David Garlan's two word title\"\n",
    "response = retriever._retrieve(query_str)"
   ],
   "metadata": {
    "id": "xQu7eWtAvY0y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response[0].get_content()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "5LihoiP1wDoT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710382721960,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "118ab0f0-be64-47a2-ee3e-2eb069558b87"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The faculty name: David Mortensen with title: Assistant Research Professor, Language Technologies Institute has research area\\t: Corpus Annotation and Resources, Natural Language Processing and Computational Linguistics'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 119
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## other"
   ],
   "metadata": {
    "id": "vx3pCiIdLMpz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df_all = pd.read_csv(\"aggregated_documents.csv\")\n",
    "df_category = pd.read_excel(\"categorized_doc.xlsx\")\n",
    "df_categorized = pd.merge(df_all, df_category[[\"title\", \"category\", \"description\"]], on=\"title\", how=\"left\")\n",
    "df_categorized['title'] = df_categorized['title'].str.replace('|', '/').str.replace('.txt', '')\n",
    "\n",
    "df_categorized = df_categorized[((df_categorized['category'] == 'course') | (df_categorized['category'] == 'paper') | (df_categorized['category'] == 'people')) == False]\n",
    "df_categorized = df_categorized.reset_index(drop=True)\n",
    "df_categorized['category'].value_counts()\n",
    "df = df_categorized\n",
    "df = df[df['title'].str.contains('people') == False].reset_index(drop=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YE8-hRJhLQUs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710387022987,
     "user_tz": 240,
     "elapsed": 2,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "5d358fc2-67d3-48f1-df6d-7b7f74f5ca9f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-200-96f631861551>:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_categorized['title'] = df_categorized['title'].str.replace('|', '/').str.replace('.txt', '')\n",
      "<ipython-input-200-96f631861551>:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_categorized['title'] = df_categorized['title'].str.replace('|', '/').str.replace('.txt', '')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df.isna().sum()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOYc1_M8AwIS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710387063427,
     "user_tz": 240,
     "elapsed": 337,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "10634190-2133-42bc-e1bf-087f2b236d3b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "title          0\n",
       "text           0\n",
       "category       1\n",
       "description    1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 202
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# df['category'].fillna(value='athletics', inplace=True)\n",
    "# df['category'].fillna(value='athletics', inplace=True)\n",
    "df['category'] = 'The Kiltie Band'\n",
    "df['description'] = \"The Kiltie Band began in 1908 with a group of just seven students dedicated to supporting Carnegie Tech football, and today's Kiltie Band continues a tradition of excellence originated over a century ago\""
   ],
   "metadata": {
    "id": "Qu348Svb6FkG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions # for chroma db embed\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding # for llama index embed\n",
    "import os\n",
    "\n",
    "embed_name = \"BAAI/bge-large-en-v1.5\" # https://huggingface.co/models\n",
    "path = os.path.join('/content/drive/MyDrive/ANLP/ANLP-HW2', 'chroma_meta')\n",
    "\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=\"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\", # huggingface api\n",
    "    model_name=embed_name\n",
    ")\n",
    "\n",
    "# same embedding function as chroma index\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_name)\n",
    "\n",
    "# get the Chroma Client\n",
    "client = chromadb.PersistentClient(path=path)\n",
    "\n",
    "# create/get a collection, create_collection if first time\n",
    "collection = client.create_collection(name=\"other_baai\", embedding_function=huggingface_ef)"
   ],
   "metadata": {
    "id": "0X40tG032Zxb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter, TokenTextSplitter\n",
    "\n",
    "# text_parser = SentenceSplitter(chunk_size=1024, paragraph_separator='\\n', chunk_overlap=400)\n",
    "text_parser = SentenceSplitter(chunk_size=512, paragraph_separator='\\n')\n",
    "\n",
    "text_chunks = []\n",
    "# maintain relationship with source doc index, to help inject doc metadata\n",
    "doc_idxs = []\n",
    "for doc_idx, doc in enumerate(df['text']):\n",
    "    cur_text_chunks = text_parser.split_text(doc)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ],
   "metadata": {
    "id": "23sTbqqf2e3f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    title, text, category, description = df.iloc[doc_idxs[idx]]\n",
    "    node = TextNode(\n",
    "        text=f'{description}: {text_chunk}',\n",
    "    )\n",
    "    metadata = {\n",
    "        \"source\": title,\n",
    "        \"category\": category\n",
    "    }\n",
    "    node.metadata = metadata\n",
    "    nodes.append(node)"
   ],
   "metadata": {
    "id": "cJIkgdVt3Agl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ],
   "metadata": {
    "id": "LCNz4Zts3JWH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "vector_store.add(nodes)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embed_model)"
   ],
   "metadata": {
    "id": "VyqfDk6m3MWR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "retriever = ChromadbRetriever(\n",
    "    collection, embed_model, query_mode=\"default\", similarity_top_k=5\n",
    ")\n",
    "\n",
    "HF_TOKEN = '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'\n",
    "\n",
    "query_str = \"What sort of credentials are required to print something from an LTI printer?\"\n",
    "response = retriever._retrieve(query_str)"
   ],
   "metadata": {
    "id": "JuYZLtFe3Qz3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response[0].get_content()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "3YpFOhRz4MK_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710386795514,
     "user_tz": 240,
     "elapsed": 339,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "3de62adf-0cc0-4497-cb03-a792d7cd40ad"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Detailed information of LTI program: LTIâ€™s printers are located in GHC 5404 and GHC 6604. The School of Computer Science provides a number of black-and-white and color printers for use by students. The SCS Computer Facilities publishes a list of printers online at http://www.cs.cmu.edu/~help/printing/. 2.3 Office Space for MS Students To help them create a sense of community, full time students in the LTIâ€™s MLT program have access to a shared office space. 2.4 Computers for MS Students Students are expected to provide their own laptop computers that can be used to access university resources and complete course assignments. Laptops running Windows, MacOS, and Linux software are all acceptable. MS students will be given a CS user ID. A CS user ID is required to use the LTI computer cluster, department printers, and other SCS services. The School of Computer Science has a Help Center located at 4203 GHC. They can be contacted at help@cs.cmu.edu, extension 8-4231 from a campus phone, or 412-268-4231 from an outside line. MS students will be given access to the LTIâ€™s computer cluster on an as-needed basis, to be used for course assignments, directed study projects, and/or capstone projects. The LTI cluster provides storage and computation for projects involving large datasets and/or lengthy computation. 3 Masterâ€™s Degree Completion and Certification 3.1 Standard Degree Requirements and Degree Certification 3.1.1 Graduate Students Carnegie Mellon graduate students are expected to complete their degree requirements within the standard length of time for their program of study as outlined in the relevant Graduate Student Handbook. Standard program lengths for graduate students vary significantly-- ranging from two semesters for some full-time masterâ€™s programs to several or more years for doctoral programs. Upon completion of the graduate program degree requirements, the degree will be certified by the studentâ€™s academic program in the semester in which the student completes the requirements. 3.1.2 Early Completion Graduate students who consider the completion of all degree requirements in less than the standard length of time for their program of study may consult with their degree-granting program or department to determine if early degree certification is allowed and under what circumstances.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 197
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Combine Retriever"
   ],
   "metadata": {
    "id": "ROgMPNl8hEy_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## implement\n",
    "\n",
    "refer to: https://docs.llamaindex.ai/en/stable/examples/query_engine/RetrieverRouterQueryEngine.html#define-retrieval-augmented-router-query-engine"
   ],
   "metadata": {
    "id": "wNHljYvVO3MP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions # for chroma db embed\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding # for llama index embed\n",
    "import os\n",
    "\n",
    "\n",
    "embed_name = \"BAAI/bge-large-en-v1.5\"\n",
    "path = os.path.join('/content/drive/MyDrive/ANLP/ANLP-HW2', 'chroma_meta')\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=HF_TOKEN,\n",
    "    model_name=embed_name\n",
    ")\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_name)\n",
    "client = chromadb.PersistentClient(path=path)\n",
    "\n",
    "collection_course = client.get_collection(name=\"course_baai\", embedding_function=huggingface_ef)\n",
    "collection_paper = client.get_collection(name=\"paper_baai\", embedding_function=huggingface_ef)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "483b1b7255ba4f1c9e976e22b2e3f680",
      "5474f11102434ec1a2d7e46280ae50e9",
      "3223b0db02734179a3bb24a84b26d73c",
      "a219418a91d445869ce6fe75e60c96a4",
      "581fcb667b71464ab8e2f944d2de002f",
      "26d419e2bcef46d5b2592008288efc58",
      "167cd9ed120c4589abc67a607b7a7b6d",
      "9dcf54f037f645afb40b742f32eb28a1",
      "4c57b4dfa6994c0996c60c4661f00b30",
      "49844922eba641c1956f3d2e0e1e7eb8",
      "655f0a330f984e39a948d9fdd4b399bd",
      "967b40ff68454376a3763fd5088cecc4",
      "8e4bbdd360474f9fab274c46a7bcac42",
      "11ab117bb9d9400ab8df2b96d6b008bb",
      "5ad75cfcd05b49ebb675fb9fedb9957e",
      "fd7b3537b611429489989bcc6a4d8a31",
      "2778c331af164857bdf8774cb48a8cb2",
      "f38f79a999f74e3e9c6521dd22bc7267",
      "f692b59c305249cca3c6c00c156996dc",
      "1741aeee64d94c899add9debbfaaa00f",
      "54b4d1d327bd466f9b6cc6054a55d541",
      "737fb400aaa14ffd85f8a2bf74492f58",
      "e67b60c069314c01b2391dcdeecff38d",
      "2f02498a91a845d0b9d7361520f1b51d",
      "cc1295239b5d4dfaac67daf8aa8bb46e",
      "86a3e8565a184390987af80ad835a263",
      "3693a06c553241ada1d8f2c81889ceea",
      "02c892a9638e4ca9bc0e02a1a1ca1117",
      "e57a9396ad4e4a2b99d991d5a6d35cc4",
      "b6a7197f83264b95a6d3c70d723065a9",
      "1c8733a8f39048388a5b54f48be11627",
      "565d130c3f214dcba9b70d9fcf65c929",
      "64d578ef9a36451d9cef71142c1a14ac",
      "abf807cce2a14716a23cd731af44cf7b",
      "81e8c1e3cd034835be8d57b801127eff",
      "bb526b1264664e7f8e634ac2dc5f3a1e",
      "08c4f078dd5445d2a92290ca2a622358",
      "40e83953126b4997a3d160cb7f03d674",
      "bc1c142b1c204037bed41ec353bda637",
      "1c349c3ffb0a4b74bd7e966690c018b0",
      "7f893adfb0bf4d6f8c24251e098d792f",
      "a59e321ec7d94643b61efc29dfddaebf",
      "46193f461b184f16af29f824fd80d8bf",
      "2bd6b0ed918646cea59882817803bcd4",
      "888e11f2f4004245abb414431418f358",
      "cffb0413a6694bbc8108304fb40f697e",
      "769cd53406604ae5999cf18d4db4ab46",
      "ec017e6105da451b9b1be3d5504e5272",
      "33c6104a1a6643c2abaf55d26a627ed1",
      "2a86cb3149b44d90acb6db55efbb94b4",
      "e70123adbe004c4cbf609b615c6a82ff",
      "04b676aa16344285a03dc4d506607b03",
      "2b664d1a39544ab19dca89d1e6f4d4e2",
      "f57051560d7f4c07aa598a2d0426d80d",
      "bded7af6ad4e4c3fa155c123ea24795d",
      "d18d1f17651141beadf4a165665c18b9",
      "64f304226c65401982bde8630c430fa5",
      "356c3dcd6a8e4cc4b2f0bd8ed11a99e1",
      "6522d8c823ea4664829f0179dbdb00bc",
      "1bcdf28c281c461a96d05cb8ed7d7aec",
      "ff55ab3ae1fd4a6e9648f48f7b36848d",
      "183ea577aa024c07abfbf4561eb1a46e",
      "c50063260fd74820a843083795637e9d",
      "ef7da0e59e6a4c8598dfd0d60c9e1370",
      "65b29195258a4804a45cb7d2f331bfda",
      "bb07dd12f8404357a96ee046321829bf"
     ]
    },
    "id": "nTXxTuNuxalh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710375233626,
     "user_tz": 240,
     "elapsed": 14276,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "861fa235-5a27-465a-f052-b42803efb2a5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "483b1b7255ba4f1c9e976e22b2e3f680"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "967b40ff68454376a3763fd5088cecc4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e67b60c069314c01b2391dcdeecff38d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abf807cce2a14716a23cd731af44cf7b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "888e11f2f4004245abb414431418f358"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d18d1f17651141beadf4a165665c18b9"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "choices = [\n",
    "    \"Useful for questions related to apples\",\n",
    "    \"Useful for questions related to oranges\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_choice_str(choices):\n",
    "    choices_str = \"\\n\\n\".join(\n",
    "        [f\"{idx+1}. {c}\" for idx, c in enumerate(choices)]\n",
    "    )\n",
    "    return choices_str\n",
    "\n",
    "\n",
    "choices_str = get_choice_str(choices)\n",
    "\n",
    "router_prompt0 = PromptTemplate(\n",
    "    \"Some choices are given below. It is provided in a numbered list (1 to\"\n",
    "    \" {num_choices}), where each item in the list corresponds to a\"\n",
    "    \" summary.\\n---------------------\\n{context_list}\\n---------------------\\nUsing\"\n",
    "    \" only the choices above and not prior knowledge, return the top choices\"\n",
    "    \" (no more than {max_outputs}, but only select what is needed) that are\"\n",
    "    \" most relevant to the question: '{query_str}'\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "llm = gemma_7b\n",
    "\n",
    "def get_formatted_prompt(query_str):\n",
    "    fmt_prompt = router_prompt0.format(\n",
    "        num_choices=len(choices),\n",
    "        max_outputs=2,\n",
    "        context_list=choices_str,\n",
    "        query_str=query_str,\n",
    "    )\n",
    "    return fmt_prompt\n",
    "\n",
    "query_str = \"Can you tell me more about the amount of Vitamin C in apples\"\n",
    "fmt_prompt = get_formatted_prompt(query_str)\n",
    "response = llm.complete(fmt_prompt)"
   ],
   "metadata": {
    "id": "pNt4oyMiUdmK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core import Settings\n",
    "Settings.llm = gemma_7b\n",
    "Settings.embed_model = embed_model"
   ],
   "metadata": {
    "id": "wMpU9-793fOt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core import TreeIndex, VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "# define sub-indices\n",
    "retriever_course = ChromadbRetriever(\n",
    "    collection_course, embed_model, query_mode=\"default\", similarity_top_k=3\n",
    ")\n",
    "retriever_paper = ChromadbRetriever(\n",
    "    collection_paper, embed_model, query_mode=\"default\", similarity_top_k=3\n",
    ")\n",
    "index_course = retriever_course._index\n",
    "index_paper = retriever_paper._index\n",
    "\n",
    "gemma_7b = HuggingFaceInferenceAPI(\n",
    "    model_name=\"google/gemma-7b\", token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# define query engines and tools\n",
    "tool_course = QueryEngineTool.from_defaults(\n",
    "    query_engine=index_course.as_query_engine(llm=gemma_7b),\n",
    "    description=(\"Use this query engine for course related questions \"\\\n",
    "                 \"like course number (5 digits like 11711), course title, \"\\\n",
    "                  \"course schedule time (in 2023/2024 Winter/Summer/Fall), \"\\\n",
    "                  \"faculty of the course\"),\n",
    ")\n",
    "tool_paper = QueryEngineTool.from_defaults(\n",
    "    query_engine=index_paper.as_query_engine(llm=gemma_7b),\n",
    "    description=(\"Use this query engine for paper related questions \"\\\n",
    "                 \"Sometimes query will have key word paper in it and sometimes \"\\\n",
    "                 \"just abbreviation like SenteCon\"),\n",
    ")"
   ],
   "metadata": {
    "id": "PK8ieIrIxarZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(llm=llm),\n",
    "    query_engine_tools=[tool_course, tool_paper],\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "   \"What's the course number for large language models methods and application?\"\n",
    ")"
   ],
   "metadata": {
    "id": "X_oAI816xatG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response.source_nodes[0].get_content()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KQpwUvvxavF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710379916686,
     "user_tz": 240,
     "elapsed": 4,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "c78e84ef-4056-430b-b4f6-b5a0f92d3919"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The course 11667: Large Language Models Methods and Application is taught by: Ippolito, Xiong'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 95
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## use"
   ],
   "metadata": {
    "id": "sAXgd61JO52d"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from retriever.embedding_retriever import EmbeddingRetriever"
   ],
   "metadata": {
    "id": "GrzNqq64O6iF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710540881720,
     "user_tz": 240,
     "elapsed": 5409,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lod_Dskx9wCm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710540883667,
     "user_tz": 240,
     "elapsed": 302,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    },
    "outputId": "0568c0f3-24de-42a7-ab1a-b2f3e6b18e5f"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "aggregated_documents.csv\t chroma_meta\t\t   faculty_info\t\tparser\n",
      "ANLP_hw2_pretrained_model.ipynb  data\t\t\t   faculty_info.csv\trag_embed.ipynb\n",
      "categorized_doc.xlsx\t\t eval_course_dataset.pkl   knowledge_source_pd\tREADME.md\n",
      "chroma_database\t\t\t eval_faculty_dataset.pkl  language_model\trequirements.txt\n",
      "chroma_db\t\t\t eval_paper_dataset.pkl    LICENSE\t\tretriever\n",
      "chroma.log\t\t\t evaluation_metric\t   openai_key.txt\tsetup.sh\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# retriever = EmbeddingRetriever(3)\n",
    "# query = 'What is the contact number of the Director of Sports Medicine?'\n",
    "# retriever.retrieve(query, top_n=3)"
   ],
   "metadata": {
    "id": "P2ReYpBzPKAp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710541573646,
     "user_tz": 240,
     "elapsed": 1,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    }
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# [course_retriever, paper_retriever, faculty_retriever, other_retriever]\n",
    "# retriever = EmbeddingRetriever(3)\n",
    "# question = query\n",
    "# retriever = retriever.slave_retrievers[3]\n",
    "# doc_nodes = retriever._retrieve(question)\n",
    "# docs = [node.get_content() for node in doc_nodes]\n",
    "# docs"
   ],
   "metadata": {
    "id": "RWMfGAV2PKDq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710541562977,
     "user_tz": 240,
     "elapsed": 4,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import requests\n",
    "# url = 'https://raw.githubusercontent.com/neubig/nlp-from-scratch-assignment-spring2024/main/data/questions.txt'\n",
    "# response = requests.get(url)\n",
    "\n",
    "# for i in [i for i in response.text.split('\\n') if 'course' in i.lower()]:\n",
    "#     # response = retriever._retrieve(i)\n",
    "#     response = retriever.retrieve(i, top_n=2)\n",
    "#     print(f'\\nquestions: {i}')\n",
    "#     print(f'response:')\n",
    "#     for i in range(len(response)):\n",
    "#         print(response[i])"
   ],
   "metadata": {
    "id": "3MND9X7mPKCK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710463701707,
     "user_tz": 240,
     "elapsed": 820,
     "user": {
      "displayName": "Justin Li",
      "userId": "06702573907710427304"
     }
    }
   },
   "execution_count": 29,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "collapsed_sections": [
    "cvdxTeX0cIBe",
    "zdF3O2NEHovH",
    "xLFMwElVekMo",
    "UkXb_JbscK0z",
    "HMkztPsyezTl",
    "axRbloieK4Ut",
    "6LokBc_8K76k",
    "8-7JjNMOsg6n",
    "vx3pCiIdLMpz",
    "wNHljYvVO3MP",
    "TBavfbcVX7wd",
    "gP8O4FyfZPyF",
    "2nhCg70DZSlM"
   ],
   "authorship_tag": "ABX9TyMyIgUZkKNnVJLHythLzGw5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4253c3ac876646969800ae4e1ec976fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11caf8eded9349388f1a69769bd528ce",
       "IPY_MODEL_f8f1bb2c719f475d8eef088526a66f44",
       "IPY_MODEL_36724d06fd07404b96f13ed432ab3464",
       "IPY_MODEL_39c2ee7d7d7d414c9146f61b861418c5"
      ],
      "layout": "IPY_MODEL_cc145f9181214a249d3e16382156ee0f"
     }
    },
    "e3532100425d461fb75c3fbedc339036": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd731680eca541b3a49bd70a140ae1d6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d1d6491dfc49465099601dadc41f3d7d",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "2956f89e27ff4ba6b5d18c9a3febb560": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "PasswordModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_e4165bda3ac64f898a9f6974e43d0543",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fde4e55f78f84f9aa0e6b114e5d57328",
      "value": ""
     }
    },
    "102ce3c501dc4cec910009de2c16243d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "CheckboxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_297ef908769c41a1929cc53ac7afc1da",
      "style": "IPY_MODEL_e9b78e4b781f480c9422dbd980b3963c",
      "value": true
     }
    },
    "9f2a82adc23f45e5955abada54fa7cfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_b5a6d213f6ca42848a1cf6120084d7e0",
      "style": "IPY_MODEL_7541fb3f914a4437861a3e50f9e3af5b",
      "tooltip": ""
     }
    },
    "96fe94c5eb224d10966840a12ade0e71": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63f3f9004f0c4c569872df4f831efa38",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2f766405722045e49baaa6661709f6ba",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "cc145f9181214a249d3e16382156ee0f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "bd731680eca541b3a49bd70a140ae1d6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1d6491dfc49465099601dadc41f3d7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4165bda3ac64f898a9f6974e43d0543": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fde4e55f78f84f9aa0e6b114e5d57328": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "297ef908769c41a1929cc53ac7afc1da": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9b78e4b781f480c9422dbd980b3963c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5a6d213f6ca42848a1cf6120084d7e0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7541fb3f914a4437861a3e50f9e3af5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "63f3f9004f0c4c569872df4f831efa38": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f766405722045e49baaa6661709f6ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0bb24fbf7a324a769d518804dc2be14c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91a52f22895b48b5a88cc3929dd1a0dc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1a0c6b30d6404d95bb8638f1987ddd1d",
      "value": "Connecting..."
     }
    },
    "91a52f22895b48b5a88cc3929dd1a0dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a0c6b30d6404d95bb8638f1987ddd1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "11caf8eded9349388f1a69769bd528ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c91d4f24c0b24d6d8ca5130b37644a0c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4ee462bd55634cec9f59a3bb5d386c40",
      "value": "Token is valid (permission: write)."
     }
    },
    "f8f1bb2c719f475d8eef088526a66f44": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b39d440b40be4a70986cb530be78cb27",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d729b69953414075a75d22f83fe5b5f1",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "36724d06fd07404b96f13ed432ab3464": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e5dde11fe9a4900b86df0823a390452",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7253b89f5c1840b0b0ef781072e8e45e",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "39c2ee7d7d7d414c9146f61b861418c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c13f228adaad44c08c0a5e6ab9dd31cf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_11ff42a12fa04f52a46d768385cc81d3",
      "value": "Login successful"
     }
    },
    "c91d4f24c0b24d6d8ca5130b37644a0c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ee462bd55634cec9f59a3bb5d386c40": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b39d440b40be4a70986cb530be78cb27": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d729b69953414075a75d22f83fe5b5f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e5dde11fe9a4900b86df0823a390452": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7253b89f5c1840b0b0ef781072e8e45e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c13f228adaad44c08c0a5e6ab9dd31cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11ff42a12fa04f52a46d768385cc81d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ae345b97fac4a59a3cb86871d2e9118": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d1c15eb837c6458790e85f1f86abbfe4",
       "IPY_MODEL_9ef70a69e8004acc95cb81082f51b05d",
       "IPY_MODEL_c1838c1031994a36aee4544b0fd4774e"
      ],
      "layout": "IPY_MODEL_ff04e10d3a38437e85bd6a64ad6590e6"
     }
    },
    "d1c15eb837c6458790e85f1f86abbfe4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf9e5b4c26bc4875b6529adeccf31673",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6376d6f79c85426689afe8b34e4a26c0",
      "value": "config.json:â€‡100%"
     }
    },
    "9ef70a69e8004acc95cb81082f51b05d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d5118b800524938a742d1177729aa53",
      "max": 779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7601e941a691454baa2bdee7e8b1c49a",
      "value": 779
     }
    },
    "c1838c1031994a36aee4544b0fd4774e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_161f1802750d49158360944652d1abbd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3de23fb3b40c4abcbf83ab6cb1e86635",
      "value": "â€‡779/779â€‡[00:00&lt;00:00,â€‡66.3kB/s]"
     }
    },
    "ff04e10d3a38437e85bd6a64ad6590e6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf9e5b4c26bc4875b6529adeccf31673": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6376d6f79c85426689afe8b34e4a26c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d5118b800524938a742d1177729aa53": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7601e941a691454baa2bdee7e8b1c49a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "161f1802750d49158360944652d1abbd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3de23fb3b40c4abcbf83ab6cb1e86635": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "33e6f16da28740c198559f547af5a67b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8b542f17579f43c5a8d9ce2c6d7af9b9",
       "IPY_MODEL_11d0ccc33c254581b553ad1dfdaed29a",
       "IPY_MODEL_60c58d8bb620423eb3b34e14d21ab211"
      ],
      "layout": "IPY_MODEL_7f5d7b30d285448ba8012b8b1ae7e659"
     }
    },
    "8b542f17579f43c5a8d9ce2c6d7af9b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5f9f8765d454ca79fe19ba55db386f3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c7e9e84b374945d095cb0144463c8404",
      "value": "model.safetensors:â€‡100%"
     }
    },
    "11d0ccc33c254581b553ad1dfdaed29a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1cc292914fc469da88e5b0b4de362f8",
      "max": 1340616616,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f4bd5425554045f79173851d4cd72018",
      "value": 1340616616
     }
    },
    "60c58d8bb620423eb3b34e14d21ab211": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e3ba5f5932b46a1bffa7dd93f0c863f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_56030eb9d7854104a44078718bc3413d",
      "value": "â€‡1.34G/1.34Gâ€‡[00:02&lt;00:00,â€‡454MB/s]"
     }
    },
    "7f5d7b30d285448ba8012b8b1ae7e659": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5f9f8765d454ca79fe19ba55db386f3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7e9e84b374945d095cb0144463c8404": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1cc292914fc469da88e5b0b4de362f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4bd5425554045f79173851d4cd72018": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9e3ba5f5932b46a1bffa7dd93f0c863f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56030eb9d7854104a44078718bc3413d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6093f1b0f0994f46b5fe49703c9979e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_16482e5e03cf4ed4b1d26c117e96f074",
       "IPY_MODEL_5f2da951b177425aaa76a4c9ec53ed2b",
       "IPY_MODEL_600102a0043e44ef8ae47ba89d0f8152"
      ],
      "layout": "IPY_MODEL_1bc975eea6654ab2949dc2a45d2247d2"
     }
    },
    "16482e5e03cf4ed4b1d26c117e96f074": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e721b3083bb4f02a35222d02c25a721",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7880b641ed1b4e5fb3486f9dcdb55b66",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "5f2da951b177425aaa76a4c9ec53ed2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_363b205d380d4b20aa98f8f5f89f518d",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4b32bac43d8c4f1eb5929a42a234a6c2",
      "value": 366
     }
    },
    "600102a0043e44ef8ae47ba89d0f8152": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_139936304b014a26a84a64bc54ff3985",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fb096773ac704323a990979418d197b0",
      "value": "â€‡366/366â€‡[00:00&lt;00:00,â€‡33.6kB/s]"
     }
    },
    "1bc975eea6654ab2949dc2a45d2247d2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e721b3083bb4f02a35222d02c25a721": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7880b641ed1b4e5fb3486f9dcdb55b66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "363b205d380d4b20aa98f8f5f89f518d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b32bac43d8c4f1eb5929a42a234a6c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "139936304b014a26a84a64bc54ff3985": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb096773ac704323a990979418d197b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52c67c6c7a484ad889316af03a81d626": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03c76c1417bd4be1a3977535a5cc1fc8",
       "IPY_MODEL_b94e9ef0c18b4cf386739b83a1a63bc1",
       "IPY_MODEL_1b82622cac5c4d519c41bdc78bf9fdde"
      ],
      "layout": "IPY_MODEL_6c048c06ee6e4dbeb624625a1c46aac4"
     }
    },
    "03c76c1417bd4be1a3977535a5cc1fc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62a50d3eb8a3473db443102ae6265447",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c29b7fb1652948059fa7c264ade3eb00",
      "value": "vocab.txt:â€‡100%"
     }
    },
    "b94e9ef0c18b4cf386739b83a1a63bc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4d16028ea124968a49b6271619e9489",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a34643700ca545a5b36c114e0488cd1a",
      "value": 231508
     }
    },
    "1b82622cac5c4d519c41bdc78bf9fdde": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e62443fd7a7744fbb2a048b7e15d73c1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_10b6d187125d4b9988ba659e762f259a",
      "value": "â€‡232k/232kâ€‡[00:00&lt;00:00,â€‡11.9MB/s]"
     }
    },
    "6c048c06ee6e4dbeb624625a1c46aac4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62a50d3eb8a3473db443102ae6265447": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c29b7fb1652948059fa7c264ade3eb00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d4d16028ea124968a49b6271619e9489": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a34643700ca545a5b36c114e0488cd1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e62443fd7a7744fbb2a048b7e15d73c1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10b6d187125d4b9988ba659e762f259a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc935fa78b3b4aaca03335c501d2f26d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_228a41f8c0a2484dbac57abd7056fc27",
       "IPY_MODEL_2ec0ee9afe014c41b9890b436943f47f",
       "IPY_MODEL_7a33e0c28c6f4087836ed44c7162381b"
      ],
      "layout": "IPY_MODEL_f37e6a41268d4af08c25b4482c6ebdd0"
     }
    },
    "228a41f8c0a2484dbac57abd7056fc27": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42e0f16e2d744ba6ae2dc550364d9f3f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3056660ead814f7a916809b4f6f3a3a7",
      "value": "tokenizer.json:â€‡100%"
     }
    },
    "2ec0ee9afe014c41b9890b436943f47f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_587fb732821c41439b0c5005de06944e",
      "max": 711396,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9793af0d854e400896239e2cfd26ceb4",
      "value": 711396
     }
    },
    "7a33e0c28c6f4087836ed44c7162381b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_973158f2600348d7b1d86b4afa4bf9ac",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_117e3e3599114e9c91a52dc64dc3fb0e",
      "value": "â€‡711k/711kâ€‡[00:00&lt;00:00,â€‡2.87MB/s]"
     }
    },
    "f37e6a41268d4af08c25b4482c6ebdd0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42e0f16e2d744ba6ae2dc550364d9f3f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3056660ead814f7a916809b4f6f3a3a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "587fb732821c41439b0c5005de06944e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9793af0d854e400896239e2cfd26ceb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "973158f2600348d7b1d86b4afa4bf9ac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "117e3e3599114e9c91a52dc64dc3fb0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5332f0a75fa4622b281030e4fc77649": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_27e5cf64e4794199b1eef32c9a7b183d",
       "IPY_MODEL_271de0eac4b3474798073e42586e0802",
       "IPY_MODEL_528dc6bd0fa7462c829f0d6f68499bfb"
      ],
      "layout": "IPY_MODEL_2212315d4e0e41259c5c2617ed5fa1b7"
     }
    },
    "27e5cf64e4794199b1eef32c9a7b183d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee21d5970b5144daad472f6551f5917a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_409b15fb0afe45088375096c7ffa6047",
      "value": "special_tokens_map.json:â€‡100%"
     }
    },
    "271de0eac4b3474798073e42586e0802": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8b25eb44e6e4a6a8557097a6917ff7b",
      "max": 125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_627f1e57724a423788a249ceb39532e2",
      "value": 125
     }
    },
    "528dc6bd0fa7462c829f0d6f68499bfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9dcf5feb21a847de814eee7a5e6df277",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4c6d6cd43e0b46baaa559f23df6f776e",
      "value": "â€‡125/125â€‡[00:00&lt;00:00,â€‡11.0kB/s]"
     }
    },
    "2212315d4e0e41259c5c2617ed5fa1b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee21d5970b5144daad472f6551f5917a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "409b15fb0afe45088375096c7ffa6047": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8b25eb44e6e4a6a8557097a6917ff7b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "627f1e57724a423788a249ceb39532e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9dcf5feb21a847de814eee7a5e6df277": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c6d6cd43e0b46baaa559f23df6f776e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "483b1b7255ba4f1c9e976e22b2e3f680": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5474f11102434ec1a2d7e46280ae50e9",
       "IPY_MODEL_3223b0db02734179a3bb24a84b26d73c",
       "IPY_MODEL_a219418a91d445869ce6fe75e60c96a4"
      ],
      "layout": "IPY_MODEL_581fcb667b71464ab8e2f944d2de002f"
     }
    },
    "5474f11102434ec1a2d7e46280ae50e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26d419e2bcef46d5b2592008288efc58",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_167cd9ed120c4589abc67a607b7a7b6d",
      "value": "config.json:â€‡100%"
     }
    },
    "3223b0db02734179a3bb24a84b26d73c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9dcf54f037f645afb40b742f32eb28a1",
      "max": 779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4c57b4dfa6994c0996c60c4661f00b30",
      "value": 779
     }
    },
    "a219418a91d445869ce6fe75e60c96a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49844922eba641c1956f3d2e0e1e7eb8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_655f0a330f984e39a948d9fdd4b399bd",
      "value": "â€‡779/779â€‡[00:00&lt;00:00,â€‡63.5kB/s]"
     }
    },
    "581fcb667b71464ab8e2f944d2de002f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26d419e2bcef46d5b2592008288efc58": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "167cd9ed120c4589abc67a607b7a7b6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9dcf54f037f645afb40b742f32eb28a1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c57b4dfa6994c0996c60c4661f00b30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49844922eba641c1956f3d2e0e1e7eb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "655f0a330f984e39a948d9fdd4b399bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "967b40ff68454376a3763fd5088cecc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e4bbdd360474f9fab274c46a7bcac42",
       "IPY_MODEL_11ab117bb9d9400ab8df2b96d6b008bb",
       "IPY_MODEL_5ad75cfcd05b49ebb675fb9fedb9957e"
      ],
      "layout": "IPY_MODEL_fd7b3537b611429489989bcc6a4d8a31"
     }
    },
    "8e4bbdd360474f9fab274c46a7bcac42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2778c331af164857bdf8774cb48a8cb2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f38f79a999f74e3e9c6521dd22bc7267",
      "value": "model.safetensors:â€‡100%"
     }
    },
    "11ab117bb9d9400ab8df2b96d6b008bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f692b59c305249cca3c6c00c156996dc",
      "max": 1340616616,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1741aeee64d94c899add9debbfaaa00f",
      "value": 1340616616
     }
    },
    "5ad75cfcd05b49ebb675fb9fedb9957e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54b4d1d327bd466f9b6cc6054a55d541",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_737fb400aaa14ffd85f8a2bf74492f58",
      "value": "â€‡1.34G/1.34Gâ€‡[00:05&lt;00:00,â€‡193MB/s]"
     }
    },
    "fd7b3537b611429489989bcc6a4d8a31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2778c331af164857bdf8774cb48a8cb2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f38f79a999f74e3e9c6521dd22bc7267": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f692b59c305249cca3c6c00c156996dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1741aeee64d94c899add9debbfaaa00f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54b4d1d327bd466f9b6cc6054a55d541": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "737fb400aaa14ffd85f8a2bf74492f58": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e67b60c069314c01b2391dcdeecff38d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f02498a91a845d0b9d7361520f1b51d",
       "IPY_MODEL_cc1295239b5d4dfaac67daf8aa8bb46e",
       "IPY_MODEL_86a3e8565a184390987af80ad835a263"
      ],
      "layout": "IPY_MODEL_3693a06c553241ada1d8f2c81889ceea"
     }
    },
    "2f02498a91a845d0b9d7361520f1b51d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02c892a9638e4ca9bc0e02a1a1ca1117",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e57a9396ad4e4a2b99d991d5a6d35cc4",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "cc1295239b5d4dfaac67daf8aa8bb46e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6a7197f83264b95a6d3c70d723065a9",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1c8733a8f39048388a5b54f48be11627",
      "value": 366
     }
    },
    "86a3e8565a184390987af80ad835a263": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_565d130c3f214dcba9b70d9fcf65c929",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_64d578ef9a36451d9cef71142c1a14ac",
      "value": "â€‡366/366â€‡[00:00&lt;00:00,â€‡31.0kB/s]"
     }
    },
    "3693a06c553241ada1d8f2c81889ceea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02c892a9638e4ca9bc0e02a1a1ca1117": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e57a9396ad4e4a2b99d991d5a6d35cc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6a7197f83264b95a6d3c70d723065a9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c8733a8f39048388a5b54f48be11627": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "565d130c3f214dcba9b70d9fcf65c929": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64d578ef9a36451d9cef71142c1a14ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "abf807cce2a14716a23cd731af44cf7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81e8c1e3cd034835be8d57b801127eff",
       "IPY_MODEL_bb526b1264664e7f8e634ac2dc5f3a1e",
       "IPY_MODEL_08c4f078dd5445d2a92290ca2a622358"
      ],
      "layout": "IPY_MODEL_40e83953126b4997a3d160cb7f03d674"
     }
    },
    "81e8c1e3cd034835be8d57b801127eff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc1c142b1c204037bed41ec353bda637",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1c349c3ffb0a4b74bd7e966690c018b0",
      "value": "vocab.txt:â€‡100%"
     }
    },
    "bb526b1264664e7f8e634ac2dc5f3a1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f893adfb0bf4d6f8c24251e098d792f",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a59e321ec7d94643b61efc29dfddaebf",
      "value": 231508
     }
    },
    "08c4f078dd5445d2a92290ca2a622358": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46193f461b184f16af29f824fd80d8bf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2bd6b0ed918646cea59882817803bcd4",
      "value": "â€‡232k/232kâ€‡[00:00&lt;00:00,â€‡13.3MB/s]"
     }
    },
    "40e83953126b4997a3d160cb7f03d674": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc1c142b1c204037bed41ec353bda637": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c349c3ffb0a4b74bd7e966690c018b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f893adfb0bf4d6f8c24251e098d792f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a59e321ec7d94643b61efc29dfddaebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "46193f461b184f16af29f824fd80d8bf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bd6b0ed918646cea59882817803bcd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "888e11f2f4004245abb414431418f358": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cffb0413a6694bbc8108304fb40f697e",
       "IPY_MODEL_769cd53406604ae5999cf18d4db4ab46",
       "IPY_MODEL_ec017e6105da451b9b1be3d5504e5272"
      ],
      "layout": "IPY_MODEL_33c6104a1a6643c2abaf55d26a627ed1"
     }
    },
    "cffb0413a6694bbc8108304fb40f697e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a86cb3149b44d90acb6db55efbb94b4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e70123adbe004c4cbf609b615c6a82ff",
      "value": "tokenizer.json:â€‡100%"
     }
    },
    "769cd53406604ae5999cf18d4db4ab46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04b676aa16344285a03dc4d506607b03",
      "max": 711396,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b664d1a39544ab19dca89d1e6f4d4e2",
      "value": 711396
     }
    },
    "ec017e6105da451b9b1be3d5504e5272": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f57051560d7f4c07aa598a2d0426d80d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bded7af6ad4e4c3fa155c123ea24795d",
      "value": "â€‡711k/711kâ€‡[00:00&lt;00:00,â€‡1.45MB/s]"
     }
    },
    "33c6104a1a6643c2abaf55d26a627ed1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a86cb3149b44d90acb6db55efbb94b4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e70123adbe004c4cbf609b615c6a82ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04b676aa16344285a03dc4d506607b03": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b664d1a39544ab19dca89d1e6f4d4e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f57051560d7f4c07aa598a2d0426d80d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bded7af6ad4e4c3fa155c123ea24795d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d18d1f17651141beadf4a165665c18b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_64f304226c65401982bde8630c430fa5",
       "IPY_MODEL_356c3dcd6a8e4cc4b2f0bd8ed11a99e1",
       "IPY_MODEL_6522d8c823ea4664829f0179dbdb00bc"
      ],
      "layout": "IPY_MODEL_1bcdf28c281c461a96d05cb8ed7d7aec"
     }
    },
    "64f304226c65401982bde8630c430fa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff55ab3ae1fd4a6e9648f48f7b36848d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_183ea577aa024c07abfbf4561eb1a46e",
      "value": "special_tokens_map.json:â€‡100%"
     }
    },
    "356c3dcd6a8e4cc4b2f0bd8ed11a99e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c50063260fd74820a843083795637e9d",
      "max": 125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ef7da0e59e6a4c8598dfd0d60c9e1370",
      "value": 125
     }
    },
    "6522d8c823ea4664829f0179dbdb00bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65b29195258a4804a45cb7d2f331bfda",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bb07dd12f8404357a96ee046321829bf",
      "value": "â€‡125/125â€‡[00:00&lt;00:00,â€‡11.9kB/s]"
     }
    },
    "1bcdf28c281c461a96d05cb8ed7d7aec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff55ab3ae1fd4a6e9648f48f7b36848d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "183ea577aa024c07abfbf4561eb1a46e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c50063260fd74820a843083795637e9d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef7da0e59e6a4c8598dfd0d60c9e1370": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "65b29195258a4804a45cb7d2f331bfda": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb07dd12f8404357a96ee046321829bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
